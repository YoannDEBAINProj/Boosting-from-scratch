{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "932ef425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yoann\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import time\n",
    "from scipy.optimize import minimize_scalar\n",
    "from collections import defaultdict\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d333b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1000000\n",
    "min_obs=int(np.ceil(np.sqrt(n)))\n",
    "# min_obs=10\n",
    "x=np.random.uniform(-10,10,(n,))\n",
    "# y=x**2+np.random.normal(0,2,(n,))\n",
    "y=np.sin(x)+np.random.normal(0,0.1,(n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a3619",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(beta)\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d162522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1000\n",
    "min_obs=int(np.ceil(np.sqrt(n)))\n",
    "# min_obs=10\n",
    "x=np.random.uniform(-10,10,(n,))\n",
    "y=np.sin(x)+np.random.normal(0,1,(n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0013fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(k, x, y):\n",
    "    z=np.maximum(x-k, 0)\n",
    "    num=np.dot(y, z)**2\n",
    "    denom=np.sum(z**2)\n",
    "    return -num/denom\n",
    "\n",
    "k_vals=np.linspace(-10,10,n)\n",
    "f_vals=np.array([f(k, x, y) for k in k_vals])\n",
    "\n",
    "plt.scatter(k_vals, f_vals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d15053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_k(x, y):\n",
    "    \n",
    "    idx_sorted = np.argsort(x)\n",
    "    x_sorted= x[idx_sorted]\n",
    "    \n",
    "    x_min=x_sorted[min_obs-1]\n",
    "    x_max=x_sorted[n-min_obs]\n",
    "    \n",
    "    def objective_right(k):\n",
    "        z=np.maximum(x-k, 0)\n",
    "        beta=np.dot(y,z)/np.sum(z**2)\n",
    "        r = np.sum((y-beta*z)**2)\n",
    "        return r\n",
    "    \n",
    "    def objective_left(k):\n",
    "        z=np.maximum(k-x, 0)\n",
    "        beta=np.dot(y,z)/np.sum(z**2)\n",
    "        r = np.sum((y-beta*z)**2)\n",
    "        return r\n",
    "    \n",
    "    result_left = minimize_scalar(objective_left, bounds=(x_min, x_max), method='bounded')\n",
    "    result_right = minimize_scalar(objective_right, bounds=(x_min, x_max), method='bounded')\n",
    "    if result_left.fun < result_right.fun:\n",
    "        return (result_left.x, \"left\", result_left.fun)\n",
    "    else : \n",
    "        return (result_right.x, \"right\", result_right.fun)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "o=optimize_k(x, y)\n",
    "end = time.time()\n",
    "\n",
    "print(o)\n",
    "print(\"time : \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c0a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "beta_min, k_min, side, erreur_min, y_pred = best_relu(x,y,min_obs)\n",
    "end = time.time()\n",
    "print(beta_min)\n",
    "print(k_min)\n",
    "print(side)\n",
    "print(erreur_min)\n",
    "print(\"time : \", end-start)\n",
    "# y_prim=y-y_pred\n",
    "# beta_min, k_min, side, erreur_min, y_pred = best_relu(x,y_prim,min_obs)\n",
    "# for i in range(1000):\n",
    "#     y_prim=y_prim-y_pred\n",
    "#     beta_min, k_min, side, erreur_min, y_pred = best_relu(x,y_prim,min_obs)\n",
    "# y_prim=y_prim-y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62b505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y_prim)\n",
    "xmin, xmax = ax.get_xbound()\n",
    "if side==\"left\":\n",
    "    x_line = [xmin, k_min]\n",
    "    y_line = [(k_min-xmin)*beta_min , 0]\n",
    "else : \n",
    "    x_line = [k_min, xmax]\n",
    "    y_line = [0, (xmax-k_min)*beta_min]\n",
    "\n",
    "# Create and add the line\n",
    "line = mlines.Line2D(x_line, y_line, color='red', linewidth=2)\n",
    "ax.add_line(line)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f4dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_abs_slope(x, y):\n",
    "    def objective(a):\n",
    "        r = np.abs(y-a*x).sum()\n",
    "        return r\n",
    "    \n",
    "    result = minimize_scalar(objective, method='brent')\n",
    "    return result.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11e86c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weighted_median_interpolated(x, y):\n",
    "\n",
    "    y_w=y/x\n",
    "    w=np.abs(x)\n",
    "    \n",
    "    # Sort by values\n",
    "    sorted_indices = np.argsort(y_w)\n",
    "    y_w_sorted = y_w[sorted_indices]\n",
    "    weights_sorted = w[sorted_indices]\n",
    "\n",
    "    total_weight = np.sum(weights_sorted)\n",
    "    cum_weights = np.cumsum(weights_sorted)\n",
    "\n",
    "    # Find where cumulative weight crosses 50%\n",
    "    cutoff = 0.5 * total_weight\n",
    "    idx = np.searchsorted(cum_weights, cutoff)\n",
    "\n",
    "    if idx == 0:\n",
    "        return y_w_sorted[0]\n",
    "    elif cum_weights[idx] == cutoff or weights_sorted[idx] == 0:\n",
    "        return y_w_sorted[idx]\n",
    "    else:\n",
    "        # Linear interpolation between previous and current\n",
    "        w1 = cum_weights[idx - 1]\n",
    "        w2 = cum_weights[idx]\n",
    "        v1 = y_w_sorted[idx - 1]\n",
    "        v2 = y_w_sorted[idx]\n",
    "        return v1 + (cutoff - w1) / (w2 - w1) * (v2 - v1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "49aea5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajouter poids\n",
    "#voir pour optimiser et ne pas recalculer \n",
    "\n",
    "def best_relu_brentv1(x, y, min_obs, w=None):\n",
    "    x_sorted= np.sort(x)\n",
    "    x_min=x_sorted[min_obs-1]\n",
    "    x_max=x_sorted[n-min_obs]\n",
    "    \n",
    "#     beta_z_store = {'left': None, 'right': None}\n",
    "    \n",
    "    def objective_right(k):\n",
    "        z=np.where(x<=k, 0, x-k)\n",
    "        beta=np.dot(y,z)/np.dot(z,z)\n",
    "#         beta_z_store['right'] = (beta, z)\n",
    "        r = np.sum((y-beta*z)**2)\n",
    "        return r\n",
    "    \n",
    "    def objective_left(k):\n",
    "        z=np.where(x>=k, 0, k-x)\n",
    "        beta=np.dot(y,z)/np.dot(z,z)\n",
    "#         beta_z_store['left'] = (beta, z)\n",
    "        r = np.sum((y-beta*z)**2)\n",
    "        return r\n",
    "    \n",
    "    result_left = minimize_scalar(objective_left, bounds=(x_min, x_max), method='bounded')\n",
    "    result_right = minimize_scalar(objective_right, bounds=(x_min, x_max), method='bounded')\n",
    "    \n",
    "    \n",
    "    if result_left.fun < result_right.fun:\n",
    "#         z=np.maximum(result_left.x-x, 0)\n",
    "        z=np.where(x>=result_left.x, 0, result_left.x-x)\n",
    "        beta_min=np.dot(y,z)/np.dot(z,z)\n",
    "#         beta_min, z = beta_z_store['left']\n",
    "        y_pred=beta_min*z\n",
    "        return (beta_min, result_left.x, \"left\", result_left.fun, y_pred)\n",
    "    else : \n",
    "#         z=np.maximum(x-result_right.x, 0)\n",
    "        z=np.where(x<=result_right.x, 0, x-result_right.x)\n",
    "        beta_min=np.dot(y,z)/np.dot(z,z)\n",
    "        \n",
    "#         beta_min, z = beta_z_store['right']\n",
    "        y_pred=beta_min*z\n",
    "        return (beta_min, result_right.x, \"right\", result_right.fun, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "2e026322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajouter poids\n",
    "#voir pour optimiser et ne pas recalculer \n",
    "\n",
    "def best_relu_brentv2(x, y, min_obs, w=None):\n",
    "    x_sorted= np.sort(x)\n",
    "    x_min=x_sorted[min_obs-1]\n",
    "    x_max=x_sorted[n-min_obs]\n",
    "    \n",
    "    beta_z_store = {'left': None, 'right': None}\n",
    "    \n",
    "    def objective_right(k):\n",
    "#         z=np.maximum(x-k, 0)\n",
    "        z=np.where(x<=k, 0, x-k)\n",
    "        beta=np.dot(y,z)/np.sum(z**2)\n",
    "        beta_z_store['right'] = (beta, z)\n",
    "        r = np.sum((y-beta*z)**2)\n",
    "        return r\n",
    "    \n",
    "    def objective_left(k):\n",
    "#         z=np.maximum(k-x, 0)\n",
    "        z=np.where(x>=k, 0, k-x)\n",
    "        beta=np.dot(y,z)/np.sum(z**2)\n",
    "        beta_z_store['left'] = (beta, z)\n",
    "        r = np.sum((y-beta*z)**2)\n",
    "        return r\n",
    "    \n",
    "    result_left = minimize_scalar(objective_left, bounds=(x_min, x_max), method='bounded')\n",
    "    result_right = minimize_scalar(objective_right, bounds=(x_min, x_max), method='bounded')\n",
    "    \n",
    "    \n",
    "    if result_left.fun < result_right.fun:\n",
    "#         z=np.maximum(result_left.x-x, 0)\n",
    "#         z=np.where(x>=result_left.x, 0, result_left.x-x)\n",
    "#         beta_min=np.dot(y,z)/np.sum(z**2)\n",
    "        beta_min, z = beta_z_store['left']\n",
    "        y_pred=beta_min*z\n",
    "        return (beta_min, result_left.x, \"left\", result_left.fun, y_pred)\n",
    "    else : \n",
    "#         z=np.maximum(x-result_right.x, 0)\n",
    "#         z=np.where(x<=result_right.x, 0, x-result_right.x)\n",
    "#         beta_min=np.dot(y,z)/np.sum(z**2)\n",
    "        \n",
    "        beta_min, z = beta_z_store['right']\n",
    "        y_pred=beta_min*z\n",
    "        return (beta_min, result_right.x, \"right\", result_right.fun, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "8d213979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.optimize import minimize_scalar\n",
    "\n",
    "n=1000\n",
    "x=np.random.uniform(-10,10,(n,))\n",
    "# y=x**2+np.random.normal(0,1,(n,))\n",
    "y=np.sin(x)+np.random.normal(0,2,(n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "549ad681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.030721071704365985,\n",
       " -0.8996029656474083,\n",
       " 'right',\n",
       " 4221.279050447156,\n",
       " array([5.96956930e-02, 0.00000000e+00, 1.03354885e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.48675744e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.40002088e-01, 2.87755994e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.85726507e-01, 2.15944917e-01, 2.27378498e-01,\n",
       "        0.00000000e+00, 1.08535209e-01, 0.00000000e+00, 1.99006338e-01,\n",
       "        3.25600989e-01, 4.40815767e-02, 4.33964334e-02, 0.00000000e+00,\n",
       "        2.09707566e-01, 2.70003733e-01, 0.00000000e+00, 1.92614618e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.81395373e-01,\n",
       "        2.75599392e-01, 9.88093949e-02, 0.00000000e+00, 1.40783107e-01,\n",
       "        2.71528304e-01, 2.36155463e-01, 1.23359873e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.31374975e-01, 9.06857873e-02,\n",
       "        1.28731995e-01, 1.34182661e-01, 1.94184266e-01, 1.33856148e-01,\n",
       "        1.78731209e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.83756647e-01, 1.10250803e-01, 2.35122347e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.53096328e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.63164542e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.06635563e-01, 0.00000000e+00, 1.02228680e-01, 2.41057430e-01,\n",
       "        0.00000000e+00, 1.46685346e-02, 1.60381071e-01, 1.30446841e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.86980776e-01, 2.08397879e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.48654083e-01,\n",
       "        3.14093381e-01, 1.48018909e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.08650044e-02, 0.00000000e+00, 0.00000000e+00, 3.23030861e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.45604881e-01, 0.00000000e+00,\n",
       "        1.82611011e-01, 0.00000000e+00, 7.55182425e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.98067577e-01, 0.00000000e+00, 1.54874887e-01,\n",
       "        0.00000000e+00, 1.70352118e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.95657393e-01, 2.84284111e-01, 1.12404978e-01, 0.00000000e+00,\n",
       "        2.45649715e-01, 3.22393926e-01, 0.00000000e+00, 1.36917324e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.20694915e-01,\n",
       "        1.04743365e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.83846352e-02, 1.88261933e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.81142029e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.00738478e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.15641858e-02, 8.54164375e-02,\n",
       "        2.66743713e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.63754845e-01, 0.00000000e+00, 1.76539897e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.55320174e-01, 0.00000000e+00, 8.25508035e-02,\n",
       "        0.00000000e+00, 2.31059786e-01, 2.77191172e-01, 2.47858983e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.87247149e-02, 1.91770382e-02, 0.00000000e+00, 1.04228596e-01,\n",
       "        1.16396933e-01, 1.86528658e-02, 3.02325684e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 9.12284839e-02, 0.00000000e+00, 1.07313644e-01,\n",
       "        6.08400903e-02, 0.00000000e+00, 1.25792780e-01, 2.61152466e-01,\n",
       "        0.00000000e+00, 2.03287066e-01, 1.78581700e-01, 1.63582008e-01,\n",
       "        2.51521171e-01, 2.41798765e-01, 0.00000000e+00, 8.26011873e-02,\n",
       "        0.00000000e+00, 2.27328291e-01, 3.24124851e-01, 1.66836364e-01,\n",
       "        1.48730037e-01, 0.00000000e+00, 7.78481299e-02, 0.00000000e+00,\n",
       "        1.89776219e-01, 6.95041202e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.64843897e-01, 3.32990589e-01, 1.79396600e-02, 0.00000000e+00,\n",
       "        2.15403590e-01, 2.32432777e-01, 1.47150272e-01, 2.50915274e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.02989085e-01, 0.00000000e+00,\n",
       "        3.28671156e-01, 1.17520204e-01, 0.00000000e+00, 1.97035623e-01,\n",
       "        2.39084000e-01, 0.00000000e+00, 1.15238785e-02, 8.06844769e-03,\n",
       "        1.95408136e-01, 2.57612701e-01, 4.53223334e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.70651521e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.26401838e-01, 2.06263137e-01, 1.66934568e-01,\n",
       "        2.49558016e-02, 2.66456297e-03, 0.00000000e+00, 5.34753243e-02,\n",
       "        3.20922066e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.05708999e-01, 3.13083850e-01, 1.50000888e-01,\n",
       "        2.00017924e-01, 0.00000000e+00, 3.31894487e-01, 1.09233191e-01,\n",
       "        2.61754942e-01, 0.00000000e+00, 2.52386301e-01, 0.00000000e+00,\n",
       "        1.49768393e-01, 0.00000000e+00, 2.43909448e-01, 2.26452809e-01,\n",
       "        1.79273268e-01, 1.49366311e-01, 1.46422729e-01, 1.62139498e-08,\n",
       "        0.00000000e+00, 3.04066319e-01, 2.61612941e-01, 0.00000000e+00,\n",
       "        2.73900078e-01, 0.00000000e+00, 5.91218932e-02, 2.26523167e-02,\n",
       "        2.94063664e-01, 3.72908589e-02, 0.00000000e+00, 2.99228699e-01,\n",
       "        0.00000000e+00, 8.78967880e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.37883650e-02, 0.00000000e+00, 3.24415528e-01, 0.00000000e+00,\n",
       "        1.02188141e-01, 2.99581416e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.25331503e-01, 0.00000000e+00,\n",
       "        1.53144247e-01, 0.00000000e+00, 5.49217932e-02, 2.43882618e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.33955170e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.21540000e-01, 5.97411224e-02, 7.90668426e-02, 1.19427291e-01,\n",
       "        0.00000000e+00, 1.57469347e-01, 0.00000000e+00, 1.07935199e-01,\n",
       "        0.00000000e+00, 2.07274109e-01, 0.00000000e+00, 5.05244861e-02,\n",
       "        3.07493321e-01, 2.01284270e-01, 1.04052465e-01, 1.75420103e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.71799036e-01, 3.25646153e-01,\n",
       "        1.46635178e-01, 0.00000000e+00, 0.00000000e+00, 2.44565843e-01,\n",
       "        6.47278060e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.47851710e-02, 2.18977926e-01, 2.74087634e-01, 0.00000000e+00,\n",
       "        1.53235211e-01, 6.32703709e-02, 8.72879730e-02, 0.00000000e+00,\n",
       "        4.97894730e-02, 2.66879325e-01, 2.77987425e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.43137882e-02, 5.90031538e-02,\n",
       "        7.85714907e-03, 1.58229580e-01, 8.43556856e-02, 9.14549708e-02,\n",
       "        0.00000000e+00, 3.13263599e-01, 2.96870972e-01, 3.19092468e-01,\n",
       "        1.35068967e-01, 1.78930755e-01, 3.07118663e-01, 2.57266746e-01,\n",
       "        0.00000000e+00, 6.21042820e-02, 9.62921020e-03, 2.92592650e-01,\n",
       "        2.05413996e-01, 2.30885109e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.32813906e-01, 0.00000000e+00, 1.05200326e-04, 2.44129475e-01,\n",
       "        2.16657612e-01, 1.96571790e-01, 7.81249129e-02, 3.11709458e-02,\n",
       "        1.52012171e-01, 2.46119437e-01, 3.07270031e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.95289867e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.74789842e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.02633010e-02, 0.00000000e+00, 2.67311947e-01, 2.20740919e-01,\n",
       "        0.00000000e+00, 1.02959526e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.19987367e-01, 0.00000000e+00, 1.57147288e-01, 0.00000000e+00,\n",
       "        8.70170047e-03, 2.70765611e-01, 1.08956342e-01, 2.12648603e-01,\n",
       "        0.00000000e+00, 2.21505642e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.15605436e-02, 1.63896469e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.89309465e-01, 1.18414298e-01, 2.02472878e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.74548027e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.73757295e-01, 7.56999786e-02, 7.58794251e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.54108580e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.80397390e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.11273322e-02, 0.00000000e+00, 3.24633915e-01, 2.31809168e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.22593349e-01,\n",
       "        0.00000000e+00, 2.63287796e-01, 1.82816935e-01, 4.53912898e-03,\n",
       "        8.19097839e-03, 1.02184024e-01, 4.07006923e-02, 8.57463545e-03,\n",
       "        0.00000000e+00, 2.88727828e-01, 1.43854447e-01, 0.00000000e+00,\n",
       "        1.42589009e-01, 0.00000000e+00, 1.38325572e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.74951627e-01, 0.00000000e+00, 2.69921173e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.46137601e-01, 3.16146616e-01,\n",
       "        1.28632009e-01, 0.00000000e+00, 3.15432682e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.42344990e-02,\n",
       "        2.86454469e-01, 2.48822703e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.01088950e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.09302928e-01, 0.00000000e+00, 1.58702751e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.80326661e-01, 2.14113244e-01, 0.00000000e+00,\n",
       "        1.97536082e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.59003417e-01, 1.99148366e-01, 0.00000000e+00,\n",
       "        3.01870318e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.80304041e-01, 0.00000000e+00, 1.64874898e-01, 3.13347819e-01,\n",
       "        2.86672863e-01, 0.00000000e+00, 3.32257700e-01, 1.73056149e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.09167065e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.08078578e-01, 0.00000000e+00, 1.39785199e-01,\n",
       "        0.00000000e+00, 3.25865156e-01, 2.45726121e-01, 1.47521672e-01,\n",
       "        0.00000000e+00, 6.00730020e-02, 3.51849639e-02, 2.45621133e-01,\n",
       "        0.00000000e+00, 7.66680515e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.67723001e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.44188275e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.64407187e-02, 0.00000000e+00, 2.95774259e-01, 0.00000000e+00,\n",
       "        3.05315915e-01, 9.69819308e-03, 1.47433289e-01, 0.00000000e+00,\n",
       "        1.92323760e-01, 2.07031375e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.46554958e-01, 0.00000000e+00, 3.74106930e-02, 0.00000000e+00,\n",
       "        1.04649537e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.29511779e-01, 2.87852088e-01, 1.64820531e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 8.99371360e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.27826183e-01, 1.28837168e-01, 1.67492620e-01,\n",
       "        1.33623392e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.70505170e-01, 0.00000000e+00, 3.13323364e-01, 2.47486303e-01,\n",
       "        0.00000000e+00, 8.49261027e-02, 1.29319267e-01, 0.00000000e+00,\n",
       "        2.87507192e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.25875483e-01, 3.94578621e-02,\n",
       "        4.54644097e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.51456775e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.72384360e-01, 2.83738787e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.07932729e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.17909718e-01, 6.04007469e-02, 9.47315692e-02, 3.30241854e-01,\n",
       "        9.80769102e-02, 0.00000000e+00, 7.24212175e-02, 9.22097290e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.11532479e-01,\n",
       "        5.17110113e-02, 4.90032268e-02, 2.11092465e-01, 3.05903998e-01,\n",
       "        1.20146401e-02, 0.00000000e+00, 0.00000000e+00, 1.94212512e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.78586512e-01,\n",
       "        8.59089046e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.19288043e-01, 2.32485785e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        9.82286433e-03, 2.37555767e-01, 0.00000000e+00, 9.97498667e-02,\n",
       "        0.00000000e+00, 2.54710215e-01, 6.27660617e-02, 1.27560659e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.99098212e-01, 1.30348704e-01, 3.21166829e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 9.30190197e-03, 1.46547776e-01,\n",
       "        0.00000000e+00, 1.96869434e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.93694846e-02, 2.74257000e-01, 2.28270071e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.27071987e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 9.67281125e-02, 1.37361718e-01, 0.00000000e+00,\n",
       "        1.88772506e-01, 0.00000000e+00, 0.00000000e+00, 2.03369863e-01,\n",
       "        3.27627668e-01, 0.00000000e+00, 2.87485801e-01, 1.62556098e-01,\n",
       "        0.00000000e+00, 2.45079656e-01, 0.00000000e+00, 1.09908211e-02,\n",
       "        2.89270638e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.88368809e-01, 0.00000000e+00, 2.41240799e-01, 2.34444904e-01,\n",
       "        0.00000000e+00, 1.87906263e-01, 2.10623572e-01, 9.62494355e-02,\n",
       "        8.57256924e-02, 3.09026163e-01, 0.00000000e+00, 1.41000952e-03,\n",
       "        1.93153700e-01, 2.81544509e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.18151827e-01, 2.37075380e-02,\n",
       "        1.82243984e-01, 0.00000000e+00, 0.00000000e+00, 1.06318188e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.66214832e-02, 2.27846925e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.52656470e-01,\n",
       "        1.79513656e-01, 5.95101417e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.46297119e-02, 1.76110916e-01, 1.24610893e-01,\n",
       "        2.21032476e-01, 0.00000000e+00, 2.26294916e-01, 1.42008941e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.84163181e-01, 1.10638684e-01, 3.08467406e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.00413579e-01, 0.00000000e+00,\n",
       "        1.01309837e-01, 0.00000000e+00, 0.00000000e+00, 3.12733595e-01,\n",
       "        8.79569834e-02, 1.93269852e-01, 3.24728546e-01, 2.94816836e-01,\n",
       "        1.73220458e-01, 0.00000000e+00, 1.51377579e-01, 0.00000000e+00,\n",
       "        2.39302882e-01, 2.89834168e-01, 9.70348900e-02, 1.50401401e-01,\n",
       "        1.24321230e-01, 2.88305314e-02, 8.91271151e-02, 7.75094305e-02,\n",
       "        1.54922368e-01, 1.71224561e-02, 0.00000000e+00, 1.62307879e-01,\n",
       "        2.22317770e-01, 0.00000000e+00, 1.97767721e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.42450186e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.33392606e-01, 1.41049018e-01,\n",
       "        0.00000000e+00, 3.28914640e-01, 2.48146964e-01, 0.00000000e+00,\n",
       "        4.71147726e-02, 2.49896125e-01, 1.80778493e-02, 1.00044590e-01,\n",
       "        3.07295220e-02, 5.13347885e-02, 0.00000000e+00, 6.53099155e-02,\n",
       "        3.27241996e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.98698615e-01, 0.00000000e+00, 1.84903601e-01, 0.00000000e+00,\n",
       "        2.36820322e-01, 1.86758055e-01, 9.11841958e-02, 0.00000000e+00,\n",
       "        1.87505709e-01, 2.94014118e-01, 0.00000000e+00, 2.46965711e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.57304174e-01,\n",
       "        0.00000000e+00, 1.85431917e-01, 4.18825309e-02, 2.37772740e-01,\n",
       "        0.00000000e+00, 1.19081941e-01, 1.22861967e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.15086285e-01, 2.99683745e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.15259437e-02, 0.00000000e+00, 1.43248089e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.42567064e-01, 3.25160269e-01, 1.10180743e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.12614807e-01, 2.52136430e-01,\n",
       "        3.25467594e-01, 4.89709302e-02, 2.19355042e-01, 0.00000000e+00,\n",
       "        3.09740804e-01, 3.05555427e-01, 0.00000000e+00, 1.13259403e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.51008811e-02, 1.07906005e-01, 0.00000000e+00,\n",
       "        1.69185815e-01, 3.27627649e-01, 2.05412810e-01, 3.16782688e-01,\n",
       "        0.00000000e+00, 2.69490568e-01, 0.00000000e+00, 9.77539714e-04,\n",
       "        0.00000000e+00, 1.06783353e-01, 2.09078212e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.12783840e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.24941782e-01, 1.46388469e-01, 0.00000000e+00,\n",
       "        2.10721428e-01, 0.00000000e+00, 1.90658573e-01, 1.32247567e-01,\n",
       "        2.84912691e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.16385244e-01, 0.00000000e+00, 1.29683702e-01, 2.06452060e-01,\n",
       "        2.62100579e-01, 0.00000000e+00, 0.00000000e+00, 1.98405248e-01,\n",
       "        2.37802466e-01, 1.68110197e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.33876749e-01, 2.88865004e-01, 1.30803344e-02, 1.92997491e-01,\n",
       "        2.57714740e-01, 9.03805840e-02, 2.52739645e-01, 0.00000000e+00,\n",
       "        3.22317892e-01, 0.00000000e+00, 0.00000000e+00, 3.61492559e-02,\n",
       "        0.00000000e+00, 1.43398117e-01, 1.75135092e-01, 1.13566492e-01,\n",
       "        0.00000000e+00, 1.23865886e-01, 1.49303987e-01, 0.00000000e+00,\n",
       "        2.49314155e-02, 0.00000000e+00, 2.61183668e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.97659138e-02,\n",
       "        3.45823405e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.02465841e-01,\n",
       "        2.19096313e-01, 1.08211886e-01, 2.63623922e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.28560074e-02, 0.00000000e+00, 4.79315255e-02,\n",
       "        0.00000000e+00, 1.14540641e-01, 1.16528497e-01, 9.33464858e-02,\n",
       "        1.58645804e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.07999582e-02, 1.14947658e-02, 3.21675369e-03,\n",
       "        1.76187387e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.76360772e-02, 0.00000000e+00,\n",
       "        1.56755762e-01, 9.99663990e-02, 1.07642791e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.49338145e-03, 1.85246149e-02, 5.18357545e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.32381153e-01, 5.76738406e-02,\n",
       "        0.00000000e+00, 3.09179949e-02, 0.00000000e+00, 1.64434788e-01]))"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_relu_brentv1(x, y, min_obs=int(np.ceil(np.sqrt(n))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "57d9cf7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0307210573836833,\n",
       " -0.8996029656474083,\n",
       " 'right',\n",
       " 4221.279050447156,\n",
       " array([5.96957680e-02, 0.00000000e+00, 1.03354940e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.48675778e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.40002126e-01, 2.87755962e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.85726477e-01, 2.15944919e-01, 2.27378495e-01,\n",
       "        0.00000000e+00, 1.08535261e-01, 0.00000000e+00, 1.99006348e-01,\n",
       "        3.25600940e-01, 4.40816590e-02, 4.33965160e-02, 0.00000000e+00,\n",
       "        2.09707571e-01, 2.70003710e-01, 0.00000000e+00, 1.92615556e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.81395345e-01,\n",
       "        2.75599366e-01, 9.88094516e-02, 0.00000000e+00, 1.40783144e-01,\n",
       "        2.71528280e-01, 2.36155456e-01, 1.23359919e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.31374970e-01, 9.06858478e-02,\n",
       "        1.28732038e-01, 1.34182702e-01, 1.94184278e-01, 1.33856189e-01,\n",
       "        1.78731228e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.83756618e-01, 1.10250854e-01, 2.35122340e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.53096360e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.63164523e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.06635616e-01, 0.00000000e+00, 1.02228736e-01, 2.41057421e-01,\n",
       "        0.00000000e+00, 1.46686306e-02, 1.60381099e-01, 1.30446883e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.86980791e-01, 2.08397885e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.48654117e-01,\n",
       "        3.14093338e-01, 1.48018943e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.08650974e-02, 0.00000000e+00, 0.00000000e+00, 3.23030814e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.45604869e-01, 0.00000000e+00,\n",
       "        1.82611028e-01, 0.00000000e+00, 7.55192354e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.98067587e-01, 0.00000000e+00, 1.54874917e-01,\n",
       "        0.00000000e+00, 1.70352142e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.95657405e-01, 2.84284081e-01, 1.12405028e-01, 0.00000000e+00,\n",
       "        2.45649704e-01, 3.22393879e-01, 0.00000000e+00, 1.36917363e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.20694961e-01,\n",
       "        1.04743419e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.83847248e-02, 1.88261948e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.81142000e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.00738487e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.15642739e-02, 8.54165005e-02,\n",
       "        2.66743691e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.63754871e-01, 0.00000000e+00, 1.76539918e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.55320157e-01, 0.00000000e+00, 8.25508678e-02,\n",
       "        0.00000000e+00, 2.31059781e-01, 2.77191145e-01, 2.47859896e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.87248089e-02, 1.91771321e-02, 0.00000000e+00, 1.04228650e-01,\n",
       "        1.16396982e-01, 1.86529599e-02, 3.02325646e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 9.12285442e-02, 0.00000000e+00, 1.07313697e-01,\n",
       "        6.08401648e-02, 0.00000000e+00, 1.25792824e-01, 2.61152447e-01,\n",
       "        0.00000000e+00, 2.03287074e-01, 1.78581720e-01, 1.63582034e-01,\n",
       "        2.51521156e-01, 2.41798755e-01, 0.00000000e+00, 8.26012516e-02,\n",
       "        0.00000000e+00, 2.27328288e-01, 3.24124803e-01, 1.66836389e-01,\n",
       "        1.48730070e-01, 0.00000000e+00, 7.78481964e-02, 0.00000000e+00,\n",
       "        1.89776233e-01, 6.95041906e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.64843876e-01, 3.32990537e-01, 1.79397544e-02, 0.00000000e+00,\n",
       "        2.15403593e-01, 2.32432771e-01, 1.47150306e-01, 2.50916185e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.02989140e-01, 0.00000000e+00,\n",
       "        3.28671105e-01, 1.17520252e-01, 0.00000000e+00, 1.97035634e-01,\n",
       "        2.39083992e-01, 0.00000000e+00, 1.15239760e-02, 8.06854674e-03,\n",
       "        1.95408148e-01, 2.57612684e-01, 4.53224151e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.70651497e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.26401788e-01, 2.06263144e-01, 1.66934593e-01,\n",
       "        2.49558928e-02, 2.66466454e-03, 0.00000000e+00, 5.34754022e-02,\n",
       "        3.20922019e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.05708959e-01, 3.13083807e-01, 1.50000921e-01,\n",
       "        2.00017934e-01, 0.00000000e+00, 3.31894435e-01, 1.09233243e-01,\n",
       "        2.61754923e-01, 0.00000000e+00, 2.52386286e-01, 0.00000000e+00,\n",
       "        1.49768426e-01, 0.00000000e+00, 2.43909437e-01, 2.26452806e-01,\n",
       "        1.79273287e-01, 1.49366344e-01, 1.46422764e-01, 1.19027386e-07,\n",
       "        0.00000000e+00, 3.04066280e-01, 2.61612922e-01, 0.00000000e+00,\n",
       "        2.73900053e-01, 0.00000000e+00, 5.91219685e-02, 2.26524089e-02,\n",
       "        2.94063630e-01, 3.72909443e-02, 0.00000000e+00, 2.99228663e-01,\n",
       "        0.00000000e+00, 8.78968498e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.37884381e-02, 0.00000000e+00, 3.24415479e-01, 0.00000000e+00,\n",
       "        1.02188196e-01, 2.99581379e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.25331548e-01, 0.00000000e+00,\n",
       "        1.53144278e-01, 0.00000000e+00, 5.49218704e-02, 2.43882608e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.33955210e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.21539953e-01, 5.97411974e-02, 7.90669086e-02, 1.19427338e-01,\n",
       "        0.00000000e+00, 1.57469376e-01, 0.00000000e+00, 1.07935251e-01,\n",
       "        0.00000000e+00, 2.07274116e-01, 0.00000000e+00, 5.05245653e-02,\n",
       "        3.07493280e-01, 2.01284279e-01, 1.04052519e-01, 1.75420124e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.71799012e-01, 3.25646104e-01,\n",
       "        1.46635212e-01, 0.00000000e+00, 0.00000000e+00, 2.44565832e-01,\n",
       "        6.47278786e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.47852623e-02, 2.18977927e-01, 2.74087609e-01, 0.00000000e+00,\n",
       "        1.53235242e-01, 6.32704443e-02, 8.72880351e-02, 0.00000000e+00,\n",
       "        4.97895527e-02, 2.66879303e-01, 2.77987398e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.43138750e-02, 5.90032291e-02,\n",
       "        7.85724822e-03, 1.58229609e-01, 8.43557491e-02, 9.14550310e-02,\n",
       "        0.00000000e+00, 3.13263556e-01, 2.96870937e-01, 3.19092422e-01,\n",
       "        1.35069007e-01, 1.78930774e-01, 3.07118622e-01, 2.57266729e-01,\n",
       "        0.00000000e+00, 6.21043559e-02, 9.62930852e-03, 2.92592617e-01,\n",
       "        2.05414003e-01, 2.30885104e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.32813900e-01, 0.00000000e+00, 1.05303090e-04, 2.44129464e-01,\n",
       "        2.16657614e-01, 1.96571801e-01, 7.81249793e-02, 3.11710340e-02,\n",
       "        1.52012203e-01, 2.46119426e-01, 3.07269990e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.95289832e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.74789863e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.02633944e-02, 0.00000000e+00, 2.67311925e-01, 2.20740919e-01,\n",
       "        0.00000000e+00, 1.02959580e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.19987320e-01, 0.00000000e+00, 1.57147317e-01, 0.00000000e+00,\n",
       "        8.70179923e-03, 2.70765588e-01, 1.08956394e-01, 2.12648607e-01,\n",
       "        0.00000000e+00, 2.21505641e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.15606270e-02, 1.63896496e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.89309479e-01, 1.18414345e-01, 2.02472886e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.74548048e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.73757270e-01, 7.57000461e-02, 7.58794925e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.54108611e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.80398054e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.11274298e-02, 0.00000000e+00, 3.24633867e-01, 2.31809163e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.22593301e-01,\n",
       "        0.00000000e+00, 2.63287776e-01, 1.82816953e-01, 4.53922968e-03,\n",
       "        8.19107739e-03, 1.02184079e-01, 4.07007762e-02, 8.57473426e-03,\n",
       "        0.00000000e+00, 2.88727796e-01, 1.43854483e-01, 0.00000000e+00,\n",
       "        1.42589045e-01, 0.00000000e+00, 1.38325610e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.74951602e-01, 0.00000000e+00, 2.69921150e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.46137636e-01, 3.16146571e-01,\n",
       "        1.28632051e-01, 0.00000000e+00, 3.15442816e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.42345672e-02,\n",
       "        2.86454438e-01, 2.48822690e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.01088959e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.09302933e-01, 0.00000000e+00, 1.58703705e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.80326633e-01, 2.14113247e-01, 0.00000000e+00,\n",
       "        1.97537018e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.59003446e-01, 1.99148376e-01, 0.00000000e+00,\n",
       "        3.01870280e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.80304060e-01, 0.00000000e+00, 1.64874924e-01, 3.13347776e-01,\n",
       "        2.86672833e-01, 0.00000000e+00, 3.32257648e-01, 1.73056171e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.09167024e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.08078631e-01, 0.00000000e+00, 1.39785236e-01,\n",
       "        0.00000000e+00, 3.25865107e-01, 2.45726109e-01, 1.47521706e-01,\n",
       "        0.00000000e+00, 6.00730768e-02, 3.51850503e-02, 2.45621121e-01,\n",
       "        0.00000000e+00, 7.66681186e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.67722979e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.44188311e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.64408046e-02, 0.00000000e+00, 2.95774224e-01, 0.00000000e+00,\n",
       "        3.05315876e-01, 9.69829137e-03, 1.47433323e-01, 0.00000000e+00,\n",
       "        1.92323773e-01, 2.07031381e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.46554992e-01, 0.00000000e+00, 3.74107783e-02, 0.00000000e+00,\n",
       "        1.04649591e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.29511728e-01, 2.87852056e-01, 1.64820557e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 8.99371969e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.27826133e-01, 1.28837211e-01, 1.67492645e-01,\n",
       "        1.33624358e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.70505147e-01, 0.00000000e+00, 3.13323321e-01, 2.47486290e-01,\n",
       "        0.00000000e+00, 8.49261660e-02, 1.29319309e-01, 0.00000000e+00,\n",
       "        2.87507161e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.25875481e-01, 3.94579466e-02,\n",
       "        4.54644913e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.51466706e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.72384336e-01, 2.83738757e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.07932688e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.17909765e-01, 6.04008216e-02, 9.47316278e-02, 3.30241803e-01,\n",
       "        9.80769673e-02, 0.00000000e+00, 7.24212865e-02, 9.22097889e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.11532436e-01,\n",
       "        5.17110900e-02, 4.90033068e-02, 2.11092469e-01, 3.05903958e-01,\n",
       "        1.20147373e-02, 0.00000000e+00, 0.00000000e+00, 1.94213450e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.78586485e-01,\n",
       "        8.59089674e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.19287997e-01, 2.32485780e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        9.82296257e-03, 2.37555759e-01, 0.00000000e+00, 9.97499231e-02,\n",
       "        0.00000000e+00, 2.54710199e-01, 6.27661353e-02, 1.27560702e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.99098222e-01, 1.30348746e-01, 3.21166782e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 9.30200045e-03, 1.46547810e-01,\n",
       "        0.00000000e+00, 1.96869445e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.93695598e-02, 2.74256975e-01, 2.28270993e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.27071937e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 9.67281703e-02, 1.37361757e-01, 0.00000000e+00,\n",
       "        1.88772521e-01, 0.00000000e+00, 0.00000000e+00, 2.03369871e-01,\n",
       "        3.27627618e-01, 0.00000000e+00, 2.87485769e-01, 1.62556125e-01,\n",
       "        0.00000000e+00, 2.45079645e-01, 0.00000000e+00, 1.09909187e-02,\n",
       "        2.89270606e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.88368778e-01, 0.00000000e+00, 2.41240789e-01, 2.34444897e-01,\n",
       "        0.00000000e+00, 1.87906278e-01, 2.10623576e-01, 9.62494934e-02,\n",
       "        8.57257553e-02, 3.09026122e-01, 0.00000000e+00, 1.41011168e-03,\n",
       "        1.93153713e-01, 2.81544480e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.18151875e-01, 2.37076297e-02,\n",
       "        1.82244002e-01, 0.00000000e+00, 0.00000000e+00, 1.06318242e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.66215642e-02, 2.27846921e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.52656502e-01,\n",
       "        1.79513675e-01, 5.95102168e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.46297892e-02, 1.76110936e-01, 1.24610938e-01,\n",
       "        2.21032476e-01, 0.00000000e+00, 2.26294914e-01, 1.42008977e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.84163151e-01, 1.10638735e-01, 3.08467365e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.00413542e-01, 0.00000000e+00,\n",
       "        1.01309893e-01, 0.00000000e+00, 0.00000000e+00, 3.12733553e-01,\n",
       "        8.79570453e-02, 1.93269865e-01, 3.24728497e-01, 2.94816802e-01,\n",
       "        1.73220480e-01, 0.00000000e+00, 1.51377611e-01, 0.00000000e+00,\n",
       "        2.39302873e-01, 2.89834136e-01, 9.70349476e-02, 1.50401434e-01,\n",
       "        1.24321275e-01, 2.88306208e-02, 8.91271763e-02, 7.75094972e-02,\n",
       "        1.54922399e-01, 1.71225509e-02, 0.00000000e+00, 1.62307906e-01,\n",
       "        2.22317769e-01, 0.00000000e+00, 1.97767732e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.42450223e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.33392553e-01, 1.41049055e-01,\n",
       "        0.00000000e+00, 3.28914590e-01, 2.48146951e-01, 0.00000000e+00,\n",
       "        4.71148534e-02, 2.49896111e-01, 1.80779436e-02, 1.00044647e-01,\n",
       "        3.07296105e-02, 5.13348674e-02, 0.00000000e+00, 6.53099879e-02,\n",
       "        3.27241946e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.98698578e-01, 0.00000000e+00, 1.84903618e-01, 0.00000000e+00,\n",
       "        2.36820314e-01, 1.86758071e-01, 9.11842561e-02, 0.00000000e+00,\n",
       "        1.87505724e-01, 2.94014083e-01, 0.00000000e+00, 2.46965699e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.57304204e-01,\n",
       "        0.00000000e+00, 1.85431933e-01, 4.18826142e-02, 2.37772732e-01,\n",
       "        0.00000000e+00, 1.19081988e-01, 1.22862012e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.15086334e-01, 2.99683708e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.15260411e-02, 0.00000000e+00, 1.43248125e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.42567053e-01, 3.25160220e-01, 1.10180795e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.12614858e-01, 2.52136415e-01,\n",
       "        3.25467545e-01, 4.89710102e-02, 2.19355043e-01, 0.00000000e+00,\n",
       "        3.09740763e-01, 3.05555388e-01, 0.00000000e+00, 1.13259453e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.51009722e-02, 1.07906057e-01, 0.00000000e+00,\n",
       "        1.69185839e-01, 3.27627599e-01, 2.05412817e-01, 3.16782643e-01,\n",
       "        0.00000000e+00, 2.69490545e-01, 0.00000000e+00, 9.77642072e-04,\n",
       "        0.00000000e+00, 1.06783406e-01, 2.09079143e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.12783797e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.24941733e-01, 1.46388504e-01, 0.00000000e+00,\n",
       "        2.10721433e-01, 0.00000000e+00, 1.90658587e-01, 1.32247608e-01,\n",
       "        2.84913587e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.16385292e-01, 0.00000000e+00, 1.29683744e-01, 2.06452067e-01,\n",
       "        2.62100560e-01, 0.00000000e+00, 0.00000000e+00, 1.98405258e-01,\n",
       "        2.37802458e-01, 1.68110221e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.33876696e-01, 2.88864972e-01, 1.30804311e-02, 1.92997504e-01,\n",
       "        2.57714723e-01, 9.03806447e-02, 2.52739630e-01, 0.00000000e+00,\n",
       "        3.22317845e-01, 0.00000000e+00, 0.00000000e+00, 3.61493419e-02,\n",
       "        0.00000000e+00, 1.43398153e-01, 1.75135113e-01, 1.13566541e-01,\n",
       "        0.00000000e+00, 1.23865931e-01, 1.49304021e-01, 0.00000000e+00,\n",
       "        2.49315066e-02, 0.00000000e+00, 2.61183649e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.97659701e-02,\n",
       "        3.45824272e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.02465803e-01,\n",
       "        2.19096313e-01, 1.08211939e-01, 2.63623902e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.28561042e-02, 0.00000000e+00, 4.79316059e-02,\n",
       "        0.00000000e+00, 1.14540691e-01, 1.16528545e-01, 9.33465451e-02,\n",
       "        1.58645833e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.08000513e-02, 1.14948633e-02, 3.21685500e-03,\n",
       "        1.76187408e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.76361625e-02, 0.00000000e+00,\n",
       "        1.56755791e-01, 9.99664552e-02, 1.07642843e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.49348124e-03, 1.85247091e-02, 5.18367584e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.32381194e-01, 5.76739165e-02,\n",
       "        0.00000000e+00, 3.09180833e-02, 0.00000000e+00, 1.64434814e-01]))"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_relu_brentv2(x, y, min_obs=int(np.ceil(np.sqrt(n))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a0e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4217f26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "beta_min, k_min, side, erreur_min, y_pred = best_relu(x,y,min_obs)\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fcfbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "sol_opt=optimize_abs_slope(y_pred, y)\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247096ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sol_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7247042",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "sol_w=weighted_median_interpolated(y_pred, y)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c68939",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sol_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a642b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0cde04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d429f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75291544",
   "metadata": {},
   "source": [
    "# version classique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37ef93c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajouter poids \n",
    "\n",
    "def weighted_median_interpolated(x, y, w):\n",
    "\n",
    "    y_w=y/x\n",
    "    w=np.abs(x)\n",
    "    \n",
    "    # Sort by values\n",
    "    sorted_indices = np.argsort(y_w)\n",
    "    y_w_sorted = y_w[sorted_indices]\n",
    "    weights_sorted = w[sorted_indices]\n",
    "\n",
    "    total_weight = np.sum(weights_sorted)\n",
    "    cum_weights = np.cumsum(weights_sorted)\n",
    "\n",
    "    # Find where cumulative weight crosses 50%\n",
    "    cutoff = 0.5 * total_weight\n",
    "    idx = np.searchsorted(cum_weights, cutoff)\n",
    "\n",
    "    if idx == 0:\n",
    "        return y_w_sorted[0]\n",
    "    elif cum_weights[idx] == cutoff or weights_sorted[idx] == 0:\n",
    "        return y_w_sorted[idx]\n",
    "    else:\n",
    "        # Linear interpolation between previous and current\n",
    "        w1 = cum_weights[idx - 1]\n",
    "        w2 = cum_weights[idx]\n",
    "        v1 = y_w_sorted[idx - 1]\n",
    "        v2 = y_w_sorted[idx]\n",
    "        return v1 + (cutoff - w1) / (w2 - w1) * (v2 - v1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46e0fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_huber_slope(delta, y_pred, y_true,  w):\n",
    "    def objective(a):\n",
    "        diff = y_true - a*y_pred\n",
    "        abs_diff = np.abs(diff)\n",
    "        r=np.sum(np.where(abs_diff <= delta, 0.5 * diff ** 2, delta * (abs_diff - 0.5 * delta))) \n",
    "        return r\n",
    "    \n",
    "    result = minimize_scalar(objective, method='brent')\n",
    "    return result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8eae5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self, name=\"sse\", delta=(\"fixe\", 3)):\n",
    "        \n",
    "        \"\"\"\n",
    "        name : string indiquant le type de loss \n",
    "               - 'sse' : ....., \n",
    "               - 'sae' : ......, \n",
    "               - 'huber' : ......, \n",
    "        \n",
    "        delta : uniquement valable si name = 'huber'\n",
    "               - (\"fixe\", x) : tuple (string, float) indiquant que l'on utilise un delta fixe=x\n",
    "               - (\"z\", x) : tuple (string, float) indiquant que l'on utilise un delta = x*std\n",
    "               - (\"quantile\", x) : tuple (string, float) indiquant que l'on utilise un delta variable = au x quantile\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.name = name\n",
    "        self.delta_method = delta[0]\n",
    "        self.delta_val = delta[1]\n",
    "\n",
    "    def loss(self, y_true, y_pred, w):\n",
    "        diff = y_true - y_pred\n",
    "        if self.name == \"sse\":\n",
    "            if w is None : \n",
    "                return 0.5 * np.sum(diff ** 2)\n",
    "        elif self.name == \"sae\":\n",
    "            if w is None : \n",
    "                return np.sum(np.abs(diff))\n",
    "        elif self.name == \"huber\":\n",
    "            if w is None : \n",
    "                abs_diff = np.abs(diff)\n",
    "                if self.delta_method==\"fixe\" : \n",
    "                    delta = self.delta_val\n",
    "                elif self.delta_method==\"z\" : \n",
    "                    delta=self.delta_val*np.std(abs_diff)\n",
    "                else : \n",
    "                    delta = np.quantile(abs_diff, self.delta_val)\n",
    "                return np.sum(np.where(abs_diff <= delta,\n",
    "                                            0.5 * diff ** 2,\n",
    "                                            delta * (abs_diff - 0.5 * delta))) \n",
    "                \n",
    "        \n",
    "    def gradient(self, y_true, y_pred, w):\n",
    "        if self.name == \"sse\":\n",
    "            if w is None : \n",
    "                return y_true - y_pred\n",
    "        elif self.name == \"sae\":\n",
    "            if w is None : \n",
    "                return np.sign(y_true - y_pred)\n",
    "        elif self.name == \"huber\":\n",
    "            if w is None : \n",
    "                diff = y_true - y_pred\n",
    "                abs_diff = np.abs(diff)\n",
    "                if self.delta_method==\"fixe\" : \n",
    "                    delta = self.delta_val\n",
    "                elif self.delta_method==\"z\" : \n",
    "                    delta=self.delta_val*np.std(abs_diff)\n",
    "                else : \n",
    "                    delta = np.quantile(abs_diff, self.delta_val)\n",
    "                grad = np.where(abs_diff <= delta, diff, delta * np.sign(diff))\n",
    "                return grad\n",
    "        \n",
    "    def opti_slope(self, y_true, y_pred, w):\n",
    "        if self.name==\"sse\":\n",
    "            return 1\n",
    "        elif self.name==\"sae\":\n",
    "            return weighted_median_interpolated(y_pred, y_true, w)\n",
    "        else : \n",
    "            if w is None : \n",
    "                if self.delta_method==\"fixe\" : \n",
    "                    delta = self.delta_val\n",
    "                elif self.delta_method==\"z\" : \n",
    "                    delta=self.delta_val*np.std(np.abs(y_true))\n",
    "                else : \n",
    "                    delta = np.quantile(np.abs(y_true), self.delta_val)\n",
    "                return opt_huber_slope(delta, y_pred, y_true,  w)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eee240d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#?proposer fit residus avec autre chose que mse?\n",
    "\n",
    "#=> eviter recalcule\n",
    "    #stocker ordre variables \n",
    "    #stocker sommex et somme_x_left \n",
    "    \n",
    "#ajouter poids\n",
    "\n",
    "#optimiser recherche dans cas o x est discret => eliminer redondance dans x_sorted\n",
    "\n",
    "def best_relu_greedy(x, y, min_obs, w=None):\n",
    "    n=x.shape[0]\n",
    "    idx_sorted = np.argsort(x)\n",
    "    x_sorted, y_sorted = x[idx_sorted], y[idx_sorted]\n",
    "\n",
    "    somme_y=np.sum(y)\n",
    "    somme_x=np.sum(x)\n",
    "    somme_x2=np.sum(x**2)\n",
    "    somme_y2=np.sum(y**2)\n",
    "    somme_xy=np.dot(x, y)\n",
    "\n",
    "    k=(x_sorted[min_obs-1]+x_sorted[min_obs])/2\n",
    "    #h(k-x)1_{x<=k}\n",
    "    x_left=x_sorted[:min_obs]\n",
    "    y_left=y_sorted[:min_obs]\n",
    "    n_obs_left=min_obs\n",
    "    somme_x_left=np.sum(x_left)\n",
    "    somme_y_left=np.sum(y_left)\n",
    "    somme_x2_left=np.sum(x_left**2)\n",
    "    somme_xy_left=np.dot(x_left, y_left)\n",
    "\n",
    "    cov_left=(k*somme_y_left-somme_xy_left)\n",
    "    ecart_x2_left=((n_obs_left*(k**2))-(2*k*somme_x_left)+somme_x2_left)\n",
    "    beta_left=cov_left/ecart_x2_left\n",
    "    erreur_left=somme_y2-(2*beta_left*cov_left)+(beta_left**2)*ecart_x2_left\n",
    "    erreur_min=erreur_left\n",
    "    beta_min=beta_left\n",
    "    side=\"left\"\n",
    "    k_min=k\n",
    "\n",
    "\n",
    "\n",
    "    somme_y_right=somme_y-somme_y_left\n",
    "    somme_x_right=somme_x-somme_x_left\n",
    "    somme_x2_right=somme_x2-somme_x2_left\n",
    "    somme_xy_right=somme_xy-somme_xy_left\n",
    "    n_obs_right=n-n_obs_left\n",
    "    cov_right=(somme_xy_right-k*somme_y_right)\n",
    "    ecart_x2_right=((n_obs_right*(k**2))-(2*k*somme_x_right)+somme_x2_right)\n",
    "    beta_right=cov_right/ecart_x2_right\n",
    "    erreur_right=somme_y2-(2*beta_right*cov_right)+(beta_right**2)*ecart_x2_right\n",
    "    if erreur_right<erreur_min:\n",
    "        erreur_min=erreur_right\n",
    "        beta_min=beta_right\n",
    "        side=\"right\"\n",
    "        k_min=k\n",
    "\n",
    "\n",
    "    for i in range(min_obs+1, n-min_obs, 1):\n",
    "        k=(x_sorted[i-1]+x_sorted[i])/2\n",
    "        somme_x_left+=x_sorted[i-1]\n",
    "        somme_y_left+=y_sorted[i-1]\n",
    "        somme_x2_left+=x_sorted[i-1]**2\n",
    "        somme_xy_left+=y_sorted[i-1]*x_sorted[i-1]\n",
    "        n_obs_left+=1\n",
    "        cov_left=(k*somme_y_left-somme_xy_left)\n",
    "        ecart_x2_left=((n_obs_left*(k**2))-(2*k*somme_x_left)+somme_x2_left)\n",
    "        beta_left=cov_left/ecart_x2_left\n",
    "        erreur_left=somme_y2-(2*beta_left*cov_left)+(beta_left**2)*ecart_x2_left\n",
    "        if erreur_left<erreur_min:\n",
    "            erreur_min=erreur_left\n",
    "            beta_min=beta_left\n",
    "            side=\"left\"\n",
    "            k_min=k\n",
    "\n",
    "\n",
    "        somme_y_right=somme_y-somme_y_left\n",
    "        somme_x_right=somme_x-somme_x_left\n",
    "        somme_x2_right=somme_x2-somme_x2_left\n",
    "        somme_xy_right=somme_xy-somme_xy_left\n",
    "        n_obs_right=n-n_obs_left\n",
    "        cov_right=(somme_xy_right-k*somme_y_right)\n",
    "        ecart_x2_right=((n_obs_right*(k**2))-(2*k*somme_x_right)+somme_x2_right)\n",
    "        beta_right=cov_right/ecart_x2_right\n",
    "        erreur_right=somme_y2-(2*beta_right*cov_right)+(beta_right**2)*ecart_x2_right\n",
    "        if erreur_right<erreur_min:\n",
    "            erreur_min=erreur_right\n",
    "            beta_min=beta_right\n",
    "            side=\"right\"\n",
    "            k_min=k\n",
    "    \n",
    "    if side==\"left\":\n",
    "        y_pred=np.where(x<=k_min, beta_min*(k_min-x), 0)\n",
    "    else : \n",
    "        y_pred=np.where(x>=k_min, beta_min*(x-k_min), 0)\n",
    "        \n",
    "    return (beta_min, k_min, side, erreur_min, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef27ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajouter poids\n",
    "#voir pour optimiser et ne pas recalculer \n",
    "\n",
    "def best_relu_brent(x, y, min_obs, w=None):\n",
    "    x_sorted= np.sort(x)\n",
    "    x_min=x_sorted[min_obs-1]\n",
    "    x_max=x_sorted[n-min_obs]\n",
    "    \n",
    "    beta_z_store = {'left': None, 'right': None}\n",
    "    \n",
    "    def objective_right(k):\n",
    "#         z=np.maximum(x-k, 0)\n",
    "        z=np.where(x<=k, 0, x-k)\n",
    "        beta=np.dot(y,z)/np.sum(z**2)\n",
    "        beta_z_store['right'] = (beta, z)\n",
    "        r = np.sum((y-beta*z)**2)\n",
    "        return r\n",
    "    \n",
    "    def objective_left(k):\n",
    "#         z=np.maximum(k-x, 0)\n",
    "        z=np.where(x>=k, 0, k-x)\n",
    "        beta=np.dot(y,z)/np.sum(z**2)\n",
    "        beta_z_store['left'] = (beta, z)\n",
    "        r = np.sum((y-beta*z)**2)\n",
    "        return r\n",
    "    \n",
    "    result_left = minimize_scalar(objective_left, bounds=(x_min, x_max), method='bounded')\n",
    "    result_right = minimize_scalar(objective_right, bounds=(x_min, x_max), method='bounded')\n",
    "    \n",
    "    \n",
    "    if result_left.fun < result_right.fun:\n",
    "#         z=np.maximum(result_left.x-x, 0)\n",
    "#         z=np.where(x>=result_left.x, 0, result_left.x-x)\n",
    "#         beta_min=np.dot(y,z)/np.sum(z**2)\n",
    "        beta_min, z = beta_z_store['left']\n",
    "        y_pred=beta_min*z\n",
    "        return (beta_min, result_left.x, \"left\", result_left.fun, y_pred)\n",
    "    else : \n",
    "#         z=np.maximum(x-result_right.x, 0)\n",
    "#         z=np.where(x<=result_right.x, 0, x-result_right.x)\n",
    "#         beta_min=np.dot(y,z)/np.sum(z**2)\n",
    "        \n",
    "        beta_min, z = beta_z_store['right']\n",
    "        y_pred=beta_min*z\n",
    "        return (beta_min, result_right.x, \"right\", result_right.fun, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51c0337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComponentWiseBoostingRegressor:\n",
    "    \n",
    "    \"\"\"\n",
    "    component-wise ReLu boosting regression (avec feature quanti)\n",
    "\n",
    "    deg_inter : entier indiquant le degre d'interaction entre les variables. \n",
    "\n",
    "    form_inter : string indiquant le type d'interaction. uniquement valable si deg_inter>0\n",
    "                 - 'ind' : indicatrice \n",
    "                 - 'mult' : multiplication\n",
    "\n",
    "    step_bef_inter : entier indiquant le nombre d'tapes  considrer avant d'inclure des intractions dans le modle.\n",
    "                     uniquement valable si deg_inter>0\n",
    "\n",
    "    min_obs_bin : entier indiquant le nombre minimal d'observations pour effectuer une regression\n",
    "\n",
    "    strat_k : \n",
    "             - None indiquant que l'on va chercher tous les bins de taille d'au moins min_obs_bin observations\n",
    "             - entier indiquant .........\n",
    "             - 'sample' indiquant que l'on souhaite chercher le meilleur k parmis un sous echantillon \n",
    "             - 'brent' indiquant que l'on va chercher via l'algo de brent\n",
    "    \n",
    "    frac_ech : float (entre 0 et 1) indiquant la fraction d'echantillon a utiliser pour rechercher le meilleur k. \n",
    "               Uniquement utilisable si strat_k='sample'. \n",
    "    \n",
    "\n",
    "    w : \n",
    "        - None indiquant que les individus ont le meme poids\n",
    "        - array contenant les poids des individus \n",
    "\n",
    "    mode_learning_rate : string indiquant le type de pas \n",
    "                        - 'fixe' indiquant que l'utilise un pas fixe \n",
    "                        - 'steepest' indiquant que celui ci sera estime a chaque iteration\n",
    "                        - 'exp' indiquant que celui ci diminue exponentiellement avec le nombre d'tapes\n",
    "    \n",
    "    fix_learning_rate : float indiquant la taille du pas (de dpart si mode_learning_rate='exp' et constant si mode_learning_rate='fixe')\n",
    "                        Uniquement utilisable si mode_learning_rate!='steepest' \n",
    "    \n",
    "    dec_learning_rate : float indiquant le taux de decroissance du pas. Uniquemen utilisable si mode_learning_rate='exp'\n",
    "\n",
    "\n",
    "    loss : string indiquant la loss a utiliser. \n",
    "           - 'sse' : ...,\n",
    "           - 'sae' : ......, \n",
    "           - 'huber' : ....,\n",
    "           \n",
    "    delta_huber : uniquement valable si loss='huber'\n",
    "                  - (\"fixe\", x) : tuple (string, float) indiquant que l'on utilise un delta fixe=x\n",
    "                  - (\"z\", x) : tuple (string, float) indiquant que l'on utilise un delta = x*std\n",
    "                  - (\"quantile\", x) : tuple (string, float) indiquant que l'on utilise un delta variable = au x quantile\n",
    "\n",
    "\n",
    "    stop : tuple indiquant la regle d'arret.\n",
    "           - ('m', x) : tuple (string, int) indiquant que l'on arrete apres x etapes\n",
    "           - ('info', x) : tuple (string, float) indiquant que l'on arrete lorsque la critere d'info augmente. \n",
    "                           x represente la penalite du nombre de parametre. \n",
    "                           critere info de la forme : log(sum error)+(x*k)/n avec k le nombre de parametres\n",
    "           - ('grad', x) : tuple (string, float) indiquant que l'on arrete lorsque la gradient est inferieur a x\n",
    "           - ('dif_loss', x) : tuple (string, float) indiquant que l'on arrete lorsque la difference de loss est inferieur a x\n",
    "\n",
    "\n",
    "    moment : booleen indiquant si l'on utilise le boosting avec moment \n",
    "    \n",
    "    verbose : boolean indiquant si l'on affiche un resume de l'avance\n",
    "\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stop=('m', 50), mode_learning_rate=\"fixe\", fix_learning_rate=0.1, loss=\"sse\",\n",
    "                 min_obs_bin=50, deg_inter=0, moment=False, strat_k=None, form_inter=\"ind\", step_bef_inter=0,\n",
    "                 delta_huber=(\"z\", 3), w=None, verbose=True, dec_learning_rate=0, frac_ech):\n",
    "        \n",
    "        self.stop = stop\n",
    "        \n",
    "        self.mode_learning_rate=mode_learning_rate\n",
    "        \n",
    "        if mode_learning_rate!=\"steepest\":\n",
    "            self.learning_rate = fix_learning_rate\n",
    "            if mode_learning_rate==\"exp\":\n",
    "                self.dec_learning_rate=dec_learning_rate\n",
    "        else : \n",
    "            self.learning_rate=0\n",
    "\n",
    "        self.loss_func = Loss(loss, delta_huber)\n",
    "        self.min_obs_bin = min_obs_bin\n",
    "        self.form_inter=form_inter\n",
    "        self.step_bef_inter=step_bef_inter\n",
    "        self.deg_inter = deg_inter\n",
    "        self.moment = moment\n",
    "        \n",
    "        self.strat_k=strat_k\n",
    "        if self.strat_k==\"sample\":\n",
    "            self.frac_ech=frac_ech\n",
    "        \n",
    "        self.verbose=verbose\n",
    "        self.w = w\n",
    "        \n",
    "        self.intercept=0\n",
    "        #ajouter opti intercept lors de la recherche du meilleur modele\n",
    "        \n",
    "        self.models = dict()\n",
    "        #structure dict model : indice feature -> side -> k : beta\n",
    "        #structure dict model avec interactions : node (indice feature, side, k, beta, degree) -> node (indice feature, side, k, beta, degree)\n",
    "        \n",
    "        \n",
    "        #temporaire\n",
    "#         self.pred=None\n",
    "    \n",
    "        \n",
    "    def best_model(self, X, res, y, y_pred_actu):\n",
    "        best_score = float('inf')\n",
    "        best_beta, best_k, best_side, best_y_pred, best_feature = None, None, None, None, None\n",
    "        \n",
    "        #paralleliser\n",
    "        #ajouter intercept dans choix meilleur modele\n",
    "        #ajouter interaction dans choix meilleur modele\n",
    "        for j in range(0, X.shape[1], 1):\n",
    "            if self.strat_k is None : \n",
    "                temp_beta, temp_k, temp_side, temp_erreur, temp_y_pred = best_relu_greedy(X[:,j], res, self.min_obs_bin, self.w)\n",
    "            elif self.strat_k==\"brent\" : \n",
    "                temp_beta, temp_k, temp_side, temp_erreur, temp_y_pred = best_relu_brent(X[:,j], res, self.min_obs_bin, self.w)\n",
    "            else : \n",
    "                #ajouter methode bins\n",
    "                return None\n",
    "            if temp_erreur < best_score:\n",
    "                best_score = temp_erreur\n",
    "                best_beta, best_k, best_side, best_y_pred = temp_beta, temp_k, temp_side, temp_y_pred\n",
    "                best_feature = j\n",
    "                \n",
    "                \n",
    "        if self.mode_learning_rate==\"steepest\": \n",
    "            self.learning_rate=self.loss_func.opti_slope(y-y_pred_actu, best_y_pred, self.w)\n",
    "                 \n",
    "        return (best_beta, best_k, best_side, best_y_pred, best_feature)\n",
    "                 \n",
    "    def update_model_new_res(self, best_beta, best_k, best_side, best_y_pred, best_feature, y, y_pred_actu):\n",
    "        if self.models.get(best_feature):\n",
    "            if self.models[best_feature].get(best_side):\n",
    "                if self.models[best_feature][best_side].get(best_k):\n",
    "                    self.models[best_feature][best_side][best_k]+=(self.learning_rate*best_beta)\n",
    "                else : \n",
    "                    self.models[best_feature][best_side][best_k]=(self.learning_rate*best_beta)\n",
    "            else :\n",
    "                self.models[best_feature][best_side]={best_k : self.learning_rate*best_beta}\n",
    "        else : \n",
    "            self.models[best_feature]={best_side : {best_k : self.learning_rate*best_beta}}\n",
    "        return (y_pred_actu+self.learning_rate*best_y_pred,\n",
    "                self.loss_func.gradient(y-y_pred_actu, self.learning_rate*best_y_pred, self.w))\n",
    "        \n",
    "      \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.models = dict()\n",
    "          \n",
    "        y_pred_actu=np.zeros((X.shape[0],))\n",
    "        residuals = self.loss_func.gradient(y, y_pred_actu, self.w)\n",
    "        if self.stop[0]=='m':\n",
    "            for i in range(0,self.stop[1],1):\n",
    "                if self.verbose : \n",
    "                    print(10*'-', i, 10*'-')\n",
    "                best_beta, best_k, best_side, best_y_pred, best_feature = self.best_model(X, residuals, y, y_pred_actu)\n",
    "                y_pred_actu, residuals=self.update_model_new_res(best_beta, best_k, best_side, best_y_pred, best_feature, y, y_pred_actu)\n",
    "            \n",
    "#             self.pred=y_pred_actu\n",
    "            \n",
    "        elif self.stop[0]=='info':\n",
    "            nbr_param=0\n",
    "            prec_loss=self.loss_func.loss(y, y_pred_actu, self.w)\n",
    "            prec_info = ((self.stop[1]*nbr_param)/X.shape[0])+np.log(prec_loss)\n",
    "            best_beta, best_k, best_side, best_y_pred, best_feature = self.best_model(X, residuals, y, y_pred_actu)\n",
    "            nbr_param+=1\n",
    "            new_loss=self.loss_func.loss(y-y_pred_actu, self.learning_rate*best_y_pred, self.w)\n",
    "            new_info = ((self.stop[1]*nbr_param)/X.shape[0])+np.log(new_loss)\n",
    "            if self.verbose :\n",
    "                print(10*'-', nbr_param, 10*'-')\n",
    "                print(\"modele \", nbr_param-1, \" parametres / loss = \", prec_loss, \" / info = \", prec_info)\n",
    "                print(\"modele \", nbr_param, \" parametres / loss = \", new_loss, \" / info = \", new_info)\n",
    "                print('\\n')\n",
    "            while new_info<prec_info:\n",
    "                y_pred_actu, residuals=self.update_model_new_res(best_beta, best_k, best_side, best_y_pred, best_feature, y, y_pred_actu)\n",
    "                prec_loss=new_loss\n",
    "                prec_info=new_info\n",
    "                best_beta, best_k, best_side, best_y_pred, best_feature = self.best_model(X, residuals,  y, y_pred_actu)\n",
    "                nbr_param+=1\n",
    "                new_loss=self.loss_func.loss(y-y_pred_actu, self.learning_rate*best_y_pred, self.w)\n",
    "                new_info=((self.stop[1]*nbr_param)/X.shape[0])+np.log(new_loss)\n",
    "                if self.verbose: \n",
    "                    print(10*'-', nbr_param, 10*'-')\n",
    "                    print(\"modele \", nbr_param-1, \" parametres / loss = \", prec_loss, \" / info = \", prec_info)\n",
    "                    print(\"modele \", nbr_param, \" parametres / loss = \", new_loss, \" / info = \", new_info)\n",
    "                    print('\\n')\n",
    "                    \n",
    "#             self.pred=y_pred_actu\n",
    "                \n",
    "        elif self.stop[0]=='grad' : \n",
    "            best_beta, best_k, best_side, best_y_pred, best_feature = self.best_model(X, residuals, y, y_pred_actu)\n",
    "            grad=np.max(np.abs(self.learning_rate*best_y_pred))\n",
    "            if self.verbose : \n",
    "                nbr_etape=1\n",
    "                print(10*'-', nbr_etape, 10*'-')\n",
    "                print(grad)\n",
    "                print('\\n')\n",
    "                nbr_etape+=1\n",
    "            while grad>self.stop[1]:\n",
    "                y_pred_actu, residuals=self.update_model_new_res(best_beta, best_k, best_side, best_y_pred, best_feature, y, y_pred_actu)\n",
    "                best_beta, best_k, best_side, best_y_pred, best_feature = self.best_model(X, residuals, y, y_pred_actu)\n",
    "                grad=np.max(np.abs(self.learning_rate*best_y_pred))\n",
    "                if self.verbose : \n",
    "                    print(10*'-', nbr_etape, 10*'-')\n",
    "                    print(grad)\n",
    "                    print('\\n')\n",
    "                    nbr_etape+=1\n",
    "                    \n",
    "#             self.pred=y_pred_actu\n",
    "                \n",
    "        elif self.stop[0]=='dif_loss':\n",
    "            prec_loss = self.loss_func.loss(y, y_pred_actu, self.w)\n",
    "            best_beta, best_k, best_side, best_y_pred, best_feature = self.best_model(X, residuals, y, y_pred_actu)\n",
    "            new_loss = self.loss_func.loss(y-y_pred_actu, self.learning_rate*best_y_pred, self.w)\n",
    "            if self.verbose :\n",
    "                etape=1\n",
    "                print(10*'-', etape, 10*'-')\n",
    "                print(\"modele \", etape-1, \" loss = \", prec_loss)\n",
    "                print(\"modele \", etape, \" loss = \", new_loss)\n",
    "                print('\\n')\n",
    "                etape+=1\n",
    "            while (prec_loss-new_loss)>self.stop[1]:\n",
    "                y_pred_actu, residuals=self.update_model_new_res(best_beta, best_k, best_side, best_y_pred, best_feature, y, y_pred_actu)\n",
    "                prec_loss=new_loss\n",
    "                best_beta, best_k, best_side, best_y_pred, best_feature = self.best_model(X, residuals, y, y_pred_actu)\n",
    "                new_loss=self.loss_func.loss(y-y_pred_actu, self.learning_rate*best_y_pred, self.w)\n",
    "                if self.verbose :\n",
    "                    print(10*'-', etape, 10*'-')\n",
    "                    print(\"modele \", etape-1, \" loss = \", prec_loss)\n",
    "                    print(\"modele \", etape, \" loss = \", new_loss)\n",
    "                    print('\\n')\n",
    "                    etape+=1\n",
    "                    \n",
    "#             self.pred=y_pred_actu\n",
    "                    \n",
    "                    \n",
    "    def reecriture(self):\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6142a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96bedeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7e526fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10000\n",
    "min_obs=int(np.ceil(np.sqrt(n)))\n",
    "x=np.random.uniform(-10,10,n)\n",
    "y=x**2+np.random.normal(0,2,(n,))\n",
    "# y=(x[:,0]**2+np.sin(x[:,1]))*x[:,2]+np.random.normal(0,2,(n,))\n",
    "# y=np.sin(x)+np.random.normal(0,0.1,(n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7f63ae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=ComponentWiseBoostingRegressor(stop=('info', 1), mode_learning_rate=\"steepest\",loss=\"huber\",\n",
    "                 min_obs_bin=min_obs, deg_inter=0, moment=False, strat_k=\"brent\", form_inter=\"ind\", step_bef_inter=0,\n",
    "                 delta_huber=(\"quantile\", 0.95), w=None, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8b35ef8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 1 ----------\n",
      "modele  0  parametres / loss =  960488.9618570209  / info =  13.775197769044569\n",
      "modele  1  parametres / loss =  742601.9983455613  / info =  13.518015511733564\n",
      "\n",
      "\n",
      "---------- 2 ----------\n",
      "modele  1  parametres / loss =  742601.9983455613  / info =  13.518015511733564\n",
      "modele  2  parametres / loss =  421101.1506274226  / info =  12.950828346579016\n",
      "\n",
      "\n",
      "---------- 3 ----------\n",
      "modele  2  parametres / loss =  421101.1506274226  / info =  12.950828346579016\n",
      "modele  3  parametres / loss =  275447.432745759  / info =  12.526452082638729\n",
      "\n",
      "\n",
      "---------- 4 ----------\n",
      "modele  3  parametres / loss =  275447.432745759  / info =  12.526452082638729\n",
      "modele  4  parametres / loss =  205884.68419767765  / info =  12.235471505585183\n",
      "\n",
      "\n",
      "---------- 5 ----------\n",
      "modele  4  parametres / loss =  205884.68419767765  / info =  12.235471505585183\n",
      "modele  5  parametres / loss =  157814.35299432487  / info =  11.9696746401291\n",
      "\n",
      "\n",
      "---------- 6 ----------\n",
      "modele  5  parametres / loss =  157814.35299432487  / info =  11.9696746401291\n",
      "modele  6  parametres / loss =  136679.45881954493  / info =  11.825993746750692\n",
      "\n",
      "\n",
      "---------- 7 ----------\n",
      "modele  6  parametres / loss =  136679.45881954493  / info =  11.825993746750692\n",
      "modele  7  parametres / loss =  118553.09187275385  / info =  11.683816171902116\n",
      "\n",
      "\n",
      "---------- 8 ----------\n",
      "modele  7  parametres / loss =  118553.09187275385  / info =  11.683816171902116\n",
      "modele  8  parametres / loss =  107800.09837379094  / info =  11.588833850014975\n",
      "\n",
      "\n",
      "---------- 9 ----------\n",
      "modele  8  parametres / loss =  107800.09837379094  / info =  11.588833850014975\n",
      "modele  9  parametres / loss =  98833.35229897923  / info =  11.502090400647026\n",
      "\n",
      "\n",
      "---------- 10 ----------\n",
      "modele  9  parametres / loss =  98833.35229897923  / info =  11.502090400647026\n",
      "modele  10  parametres / loss =  92670.48298101839  / info =  11.437805286405057\n",
      "\n",
      "\n",
      "---------- 11 ----------\n",
      "modele  10  parametres / loss =  92670.48298101839  / info =  11.437805286405057\n",
      "modele  11  parametres / loss =  66817.41310294838  / info =  11.1108190007487\n",
      "\n",
      "\n",
      "---------- 12 ----------\n",
      "modele  11  parametres / loss =  66817.41310294838  / info =  11.1108190007487\n",
      "modele  12  parametres / loss =  54456.06085121578  / info =  10.906349432565385\n",
      "\n",
      "\n",
      "---------- 13 ----------\n",
      "modele  12  parametres / loss =  54456.06085121578  / info =  10.906349432565385\n",
      "modele  13  parametres / loss =  49033.38230788277  / info =  10.801556616722065\n",
      "\n",
      "\n",
      "---------- 14 ----------\n",
      "modele  13  parametres / loss =  49033.38230788277  / info =  10.801556616722065\n",
      "modele  14  parametres / loss =  43666.084808629465  / info =  10.685726988433183\n",
      "\n",
      "\n",
      "---------- 15 ----------\n",
      "modele  14  parametres / loss =  43666.084808629465  / info =  10.685726988433183\n",
      "modele  15  parametres / loss =  41156.35558294426  / info =  10.626633643339423\n",
      "\n",
      "\n",
      "---------- 16 ----------\n",
      "modele  15  parametres / loss =  41156.35558294426  / info =  10.626633643339423\n",
      "modele  16  parametres / loss =  40089.33073190153  / info =  10.600465511356559\n",
      "\n",
      "\n",
      "---------- 17 ----------\n",
      "modele  16  parametres / loss =  40089.33073190153  / info =  10.600465511356559\n",
      "modele  17  parametres / loss =  39677.22546580237  / info =  10.590232636217264\n",
      "\n",
      "\n",
      "---------- 18 ----------\n",
      "modele  17  parametres / loss =  39677.22546580237  / info =  10.590232636217264\n",
      "modele  18  parametres / loss =  39438.28578510235  / info =  10.584292343905416\n",
      "\n",
      "\n",
      "---------- 19 ----------\n",
      "modele  18  parametres / loss =  39438.28578510235  / info =  10.584292343905416\n",
      "modele  19  parametres / loss =  39295.48857640718  / info =  10.580764996774503\n",
      "\n",
      "\n",
      "---------- 20 ----------\n",
      "modele  19  parametres / loss =  39295.48857640718  / info =  10.580764996774503\n",
      "modele  20  parametres / loss =  39206.844483433146  / info =  10.578606614706233\n",
      "\n",
      "\n",
      "---------- 21 ----------\n",
      "modele  20  parametres / loss =  39206.844483433146  / info =  10.578606614706233\n",
      "modele  21  parametres / loss =  39116.415935182085  / info =  10.57639750275282\n",
      "\n",
      "\n",
      "---------- 22 ----------\n",
      "modele  21  parametres / loss =  39116.415935182085  / info =  10.57639750275282\n",
      "modele  22  parametres / loss =  38900.66602304811  / info =  10.570966650875086\n",
      "\n",
      "\n",
      "---------- 23 ----------\n",
      "modele  22  parametres / loss =  38900.66602304811  / info =  10.570966650875086\n",
      "modele  23  parametres / loss =  38737.38677981488  / info =  10.566860479337679\n",
      "\n",
      "\n",
      "---------- 24 ----------\n",
      "modele  23  parametres / loss =  38737.38677981488  / info =  10.566860479337679\n",
      "modele  24  parametres / loss =  38601.96319883656  / info =  10.563458414233079\n",
      "\n",
      "\n",
      "---------- 25 ----------\n",
      "modele  24  parametres / loss =  38601.96319883656  / info =  10.563458414233079\n",
      "modele  25  parametres / loss =  38488.738013794835  / info =  10.5606209583616\n",
      "\n",
      "\n",
      "---------- 26 ----------\n",
      "modele  25  parametres / loss =  38488.738013794835  / info =  10.5606209583616\n",
      "modele  26  parametres / loss =  37666.88610136124  / info =  10.539136634709253\n",
      "\n",
      "\n",
      "---------- 27 ----------\n",
      "modele  26  parametres / loss =  37666.88610136124  / info =  10.539136634709253\n",
      "modele  27  parametres / loss =  37540.72837964131  / info =  10.535881712713094\n",
      "\n",
      "\n",
      "---------- 28 ----------\n",
      "modele  27  parametres / loss =  37540.72837964131  / info =  10.535881712713094\n",
      "modele  28  parametres / loss =  37359.999413406986  / info =  10.531155876641682\n",
      "\n",
      "\n",
      "---------- 29 ----------\n",
      "modele  28  parametres / loss =  37359.999413406986  / info =  10.531155876641682\n",
      "modele  29  parametres / loss =  36981.37323282035  / info =  10.521069638724887\n",
      "\n",
      "\n",
      "---------- 30 ----------\n",
      "modele  29  parametres / loss =  36981.37323282035  / info =  10.521069638724887\n",
      "modele  30  parametres / loss =  36918.9261659638  / info =  10.519479602764866\n",
      "\n",
      "\n",
      "---------- 31 ----------\n",
      "modele  30  parametres / loss =  36918.9261659638  / info =  10.519479602764866\n",
      "modele  31  parametres / loss =  36850.32092384949  / info =  10.517719606503562\n",
      "\n",
      "\n",
      "---------- 32 ----------\n",
      "modele  31  parametres / loss =  36850.32092384949  / info =  10.517719606503562\n",
      "modele  32  parametres / loss =  36793.171226441336  / info =  10.516267542438923\n",
      "\n",
      "\n",
      "---------- 33 ----------\n",
      "modele  32  parametres / loss =  36793.171226441336  / info =  10.516267542438923\n",
      "modele  33  parametres / loss =  36656.46091187229  / info =  10.5126449785959\n",
      "\n",
      "\n",
      "---------- 34 ----------\n",
      "modele  33  parametres / loss =  36656.46091187229  / info =  10.5126449785959\n",
      "modele  34  parametres / loss =  36327.080937782725  / info =  10.503718773453762\n",
      "\n",
      "\n",
      "---------- 35 ----------\n",
      "modele  34  parametres / loss =  36327.080937782725  / info =  10.503718773453762\n",
      "modele  35  parametres / loss =  36267.78492637936  / info =  10.50218515862758\n",
      "\n",
      "\n",
      "---------- 36 ----------\n",
      "modele  35  parametres / loss =  36267.78492637936  / info =  10.50218515862758\n",
      "modele  36  parametres / loss =  36159.91768033415  / info =  10.49930653796409\n",
      "\n",
      "\n",
      "---------- 37 ----------\n",
      "modele  36  parametres / loss =  36159.91768033415  / info =  10.49930653796409\n",
      "modele  37  parametres / loss =  35867.70698768879  / info =  10.491292642854027\n",
      "\n",
      "\n",
      "---------- 38 ----------\n",
      "modele  37  parametres / loss =  35867.70698768879  / info =  10.491292642854027\n",
      "modele  38  parametres / loss =  35807.41362463117  / info =  10.489710235436663\n",
      "\n",
      "\n",
      "---------- 39 ----------\n",
      "modele  38  parametres / loss =  35807.41362463117  / info =  10.489710235436663\n",
      "modele  39  parametres / loss =  35719.57797325027  / info =  10.487354220062024\n",
      "\n",
      "\n",
      "---------- 40 ----------\n",
      "modele  39  parametres / loss =  35719.57797325027  / info =  10.487354220062024\n",
      "modele  40  parametres / loss =  35456.04763508284  / info =  10.480049113604553\n",
      "\n",
      "\n",
      "---------- 41 ----------\n",
      "modele  40  parametres / loss =  35456.04763508284  / info =  10.480049113604553\n",
      "modele  41  parametres / loss =  35395.758284026895  / info =  10.478447269457462\n",
      "\n",
      "\n",
      "---------- 42 ----------\n",
      "modele  41  parametres / loss =  35395.758284026895  / info =  10.478447269457462\n",
      "modele  42  parametres / loss =  35322.21605415216  / info =  10.476467394939727\n",
      "\n",
      "\n",
      "---------- 43 ----------\n",
      "modele  42  parametres / loss =  35322.21605415216  / info =  10.476467394939727\n",
      "modele  43  parametres / loss =  35081.98668079645  / info =  10.469743077744468\n",
      "\n",
      "\n",
      "---------- 44 ----------\n",
      "modele  43  parametres / loss =  35081.98668079645  / info =  10.469743077744468\n",
      "modele  44  parametres / loss =  35022.40533144003  / info =  10.46814328798831\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 45 ----------\n",
      "modele  44  parametres / loss =  35022.40533144003  / info =  10.46814328798831\n",
      "modele  45  parametres / loss =  34959.49447259339  / info =  10.46644536949883\n",
      "\n",
      "\n",
      "---------- 46 ----------\n",
      "modele  45  parametres / loss =  34959.49447259339  / info =  10.46644536949883\n",
      "modele  46  parametres / loss =  34738.53494863107  / info =  10.460204867018419\n",
      "\n",
      "\n",
      "---------- 47 ----------\n",
      "modele  46  parametres / loss =  34738.53494863107  / info =  10.460204867018419\n",
      "modele  47  parametres / loss =  34680.308775002704  / info =  10.458627334401632\n",
      "\n",
      "\n",
      "---------- 48 ----------\n",
      "modele  47  parametres / loss =  34680.308775002704  / info =  10.458627334401632\n",
      "modele  48  parametres / loss =  34625.35167509466  / info =  10.457141400119495\n",
      "\n",
      "\n",
      "---------- 49 ----------\n",
      "modele  48  parametres / loss =  34625.35167509466  / info =  10.457141400119495\n",
      "modele  49  parametres / loss =  34420.700167812116  / info =  10.451313411447424\n",
      "\n",
      "\n",
      "---------- 50 ----------\n",
      "modele  49  parametres / loss =  34420.700167812116  / info =  10.451313411447424\n",
      "modele  50  parametres / loss =  34364.102620728925  / info =  10.449767770039397\n",
      "\n",
      "\n",
      "---------- 51 ----------\n",
      "modele  50  parametres / loss =  34364.102620728925  / info =  10.449767770039397\n",
      "modele  51  parametres / loss =  34315.34442755499  / info =  10.448447892522582\n",
      "\n",
      "\n",
      "---------- 52 ----------\n",
      "modele  51  parametres / loss =  34315.34442755499  / info =  10.448447892522582\n",
      "modele  52  parametres / loss =  34124.83459951405  / info =  10.442980685574828\n",
      "\n",
      "\n",
      "---------- 53 ----------\n",
      "modele  52  parametres / loss =  34124.83459951405  / info =  10.442980685574828\n",
      "modele  53  parametres / loss =  34070.08809664253  / info =  10.44147509640578\n",
      "\n",
      "\n",
      "---------- 54 ----------\n",
      "modele  53  parametres / loss =  34070.08809664253  / info =  10.44147509640578\n",
      "modele  54  parametres / loss =  34022.80318598422  / info =  10.440186260732387\n",
      "\n",
      "\n",
      "---------- 55 ----------\n",
      "modele  54  parametres / loss =  34022.80318598422  / info =  10.440186260732387\n",
      "modele  55  parametres / loss =  33978.127317271515  / info =  10.438972282385034\n",
      "\n",
      "\n",
      "---------- 56 ----------\n",
      "modele  55  parametres / loss =  33978.127317271515  / info =  10.438972282385034\n",
      "modele  56  parametres / loss =  33934.35780749951  / info =  10.437783284765468\n",
      "\n",
      "\n",
      "---------- 57 ----------\n",
      "modele  56  parametres / loss =  33934.35780749951  / info =  10.437783284765468\n",
      "modele  57  parametres / loss =  33756.22240125747  / info =  10.432620046751753\n",
      "\n",
      "\n",
      "---------- 58 ----------\n",
      "modele  57  parametres / loss =  33756.22240125747  / info =  10.432620046751753\n",
      "modele  58  parametres / loss =  33702.658744827364  / info =  10.431132007733709\n",
      "\n",
      "\n",
      "---------- 59 ----------\n",
      "modele  58  parametres / loss =  33702.658744827364  / info =  10.431132007733709\n",
      "modele  59  parametres / loss =  33440.14237867097  / info =  10.42341232492495\n",
      "\n",
      "\n",
      "---------- 60 ----------\n",
      "modele  59  parametres / loss =  33440.14237867097  / info =  10.42341232492495\n",
      "modele  60  parametres / loss =  33375.75509473766  / info =  10.421585020008047\n",
      "\n",
      "\n",
      "---------- 61 ----------\n",
      "modele  60  parametres / loss =  33375.75509473766  / info =  10.421585020008047\n",
      "modele  61  parametres / loss =  33262.69689884967  / info =  10.418291834812946\n",
      "\n",
      "\n",
      "---------- 62 ----------\n",
      "modele  61  parametres / loss =  33262.69689884967  / info =  10.418291834812946\n",
      "modele  62  parametres / loss =  33022.966663704436  / info =  10.411158557887301\n",
      "\n",
      "\n",
      "---------- 63 ----------\n",
      "modele  62  parametres / loss =  33022.966663704436  / info =  10.411158557887301\n",
      "modele  63  parametres / loss =  32979.84493363421  / info =  10.40995189427216\n",
      "\n",
      "\n",
      "---------- 64 ----------\n",
      "modele  63  parametres / loss =  32979.84493363421  / info =  10.40995189427216\n",
      "modele  64  parametres / loss =  32939.67246327998  / info =  10.408833060248071\n",
      "\n",
      "\n",
      "---------- 65 ----------\n",
      "modele  64  parametres / loss =  32939.67246327998  / info =  10.408833060248071\n",
      "modele  65  parametres / loss =  32902.15082039669  / info =  10.407793309096789\n",
      "\n",
      "\n",
      "---------- 66 ----------\n",
      "modele  65  parametres / loss =  32902.15082039669  / info =  10.407793309096789\n",
      "modele  66  parametres / loss =  32867.086043645155  / info =  10.406827011789845\n",
      "\n",
      "\n",
      "---------- 67 ----------\n",
      "modele  66  parametres / loss =  32867.086043645155  / info =  10.406827011789845\n",
      "modele  67  parametres / loss =  32830.25233877019  / info =  10.405805696691205\n",
      "\n",
      "\n",
      "---------- 68 ----------\n",
      "modele  67  parametres / loss =  32830.25233877019  / info =  10.405805696691205\n",
      "modele  68  parametres / loss =  32745.104900436127  / info =  10.403308761202867\n",
      "\n",
      "\n",
      "---------- 69 ----------\n",
      "modele  68  parametres / loss =  32745.104900436127  / info =  10.403308761202867\n",
      "modele  69  parametres / loss =  32535.339829660097  / info =  10.396982156923254\n",
      "\n",
      "\n",
      "---------- 70 ----------\n",
      "modele  69  parametres / loss =  32535.339829660097  / info =  10.396982156923254\n",
      "modele  70  parametres / loss =  32497.188112304488  / info =  10.39590884495339\n",
      "\n",
      "\n",
      "---------- 71 ----------\n",
      "modele  70  parametres / loss =  32497.188112304488  / info =  10.39590884495339\n",
      "modele  71  parametres / loss =  32429.42396682615  / info =  10.393921436788574\n",
      "\n",
      "\n",
      "---------- 72 ----------\n",
      "modele  71  parametres / loss =  32429.42396682615  / info =  10.393921436788574\n",
      "modele  72  parametres / loss =  32242.685519288058  / info =  10.388246490896167\n",
      "\n",
      "\n",
      "---------- 73 ----------\n",
      "modele  72  parametres / loss =  32242.685519288058  / info =  10.388246490896167\n",
      "modele  73  parametres / loss =  32203.94251309134  / info =  10.387144162332307\n",
      "\n",
      "\n",
      "---------- 74 ----------\n",
      "modele  73  parametres / loss =  32203.94251309134  / info =  10.387144162332307\n",
      "modele  74  parametres / loss =  32148.558319013046  / info =  10.385522886327909\n",
      "\n",
      "\n",
      "---------- 75 ----------\n",
      "modele  74  parametres / loss =  32148.558319013046  / info =  10.385522886327909\n",
      "modele  75  parametres / loss =  31980.250836078172  / info =  10.380373829886834\n",
      "\n",
      "\n",
      "---------- 76 ----------\n",
      "modele  75  parametres / loss =  31980.250836078172  / info =  10.380373829886834\n",
      "modele  76  parametres / loss =  31941.458229365126  / info =  10.379260075998127\n",
      "\n",
      "\n",
      "---------- 77 ----------\n",
      "modele  76  parametres / loss =  31941.458229365126  / info =  10.379260075998127\n",
      "modele  77  parametres / loss =  31895.183577274267  / info =  10.377910292335123\n",
      "\n",
      "\n",
      "---------- 78 ----------\n",
      "modele  77  parametres / loss =  31895.183577274267  / info =  10.377910292335123\n",
      "modele  78  parametres / loss =  31741.765932399983  / info =  10.373188629974408\n",
      "\n",
      "\n",
      "---------- 79 ----------\n",
      "modele  78  parametres / loss =  31741.765932399983  / info =  10.373188629974408\n",
      "modele  79  parametres / loss =  31706.76251067344  / info =  10.372185265526513\n",
      "\n",
      "\n",
      "---------- 80 ----------\n",
      "modele  79  parametres / loss =  31706.76251067344  / info =  10.372185265526513\n",
      "modele  80  parametres / loss =  31667.769082283136  / info =  10.371054694433335\n",
      "\n",
      "\n",
      "---------- 81 ----------\n",
      "modele  80  parametres / loss =  31667.769082283136  / info =  10.371054694433335\n",
      "modele  81  parametres / loss =  31628.648356998994  / info =  10.369918582443177\n",
      "\n",
      "\n",
      "---------- 82 ----------\n",
      "modele  81  parametres / loss =  31628.648356998994  / info =  10.369918582443177\n",
      "modele  82  parametres / loss =  31487.386533419496  / info =  10.365542317110728\n",
      "\n",
      "\n",
      "---------- 83 ----------\n",
      "modele  82  parametres / loss =  31487.386533419496  / info =  10.365542317110728\n",
      "modele  83  parametres / loss =  31449.194032403364  / info =  10.364428634849602\n",
      "\n",
      "\n",
      "---------- 84 ----------\n",
      "modele  83  parametres / loss =  31449.194032403364  / info =  10.364428634849602\n",
      "modele  84  parametres / loss =  31312.24074015138  / info =  10.360164378037299\n",
      "\n",
      "\n",
      "---------- 85 ----------\n",
      "modele  84  parametres / loss =  31312.24074015138  / info =  10.360164378037299\n",
      "modele  85  parametres / loss =  31268.887181532395  / info =  10.358878862403621\n",
      "\n",
      "\n",
      "---------- 86 ----------\n",
      "modele  85  parametres / loss =  31268.887181532395  / info =  10.358878862403621\n",
      "modele  86  parametres / loss =  31181.81423550772  / info =  10.356190326791637\n",
      "\n",
      "\n",
      "---------- 87 ----------\n",
      "modele  86  parametres / loss =  31181.81423550772  / info =  10.356190326791637\n",
      "modele  87  parametres / loss =  30999.32672522535  / info =  10.350420764690314\n",
      "\n",
      "\n",
      "---------- 88 ----------\n",
      "modele  87  parametres / loss =  30999.32672522535  / info =  10.350420764690314\n",
      "modele  88  parametres / loss =  30966.62577089575  / info =  10.349465315494491\n",
      "\n",
      "\n",
      "---------- 89 ----------\n",
      "modele  88  parametres / loss =  30966.62577089575  / info =  10.349465315494491\n",
      "modele  89  parametres / loss =  30934.36072661237  / info =  10.348522842699435\n",
      "\n",
      "\n",
      "---------- 90 ----------\n",
      "modele  89  parametres / loss =  30934.36072661237  / info =  10.348522842699435\n",
      "modele  90  parametres / loss =  30904.171895822386  / info =  10.347646466582214\n",
      "\n",
      "\n",
      "---------- 91 ----------\n",
      "modele  90  parametres / loss =  30904.171895822386  / info =  10.347646466582214\n",
      "modele  91  parametres / loss =  30875.78877982466  / info =  10.34682762112863\n",
      "\n",
      "\n",
      "---------- 92 ----------\n",
      "modele  91  parametres / loss =  30875.78877982466  / info =  10.34682762112863\n",
      "modele  92  parametres / loss =  30849.10048127657  / info =  10.3460628710892\n",
      "\n",
      "\n",
      "---------- 93 ----------\n",
      "modele  92  parametres / loss =  30849.10048127657  / info =  10.3460628710892\n",
      "modele  93  parametres / loss =  30821.524052036104  / info =  10.345268557750002\n",
      "\n",
      "\n",
      "---------- 94 ----------\n",
      "modele  93  parametres / loss =  30821.524052036104  / info =  10.345268557750002\n",
      "modele  94  parametres / loss =  30756.27491429139  / info =  10.343249314670173\n",
      "\n",
      "\n",
      "---------- 95 ----------\n",
      "modele  94  parametres / loss =  30756.27491429139  / info =  10.343249314670173\n",
      "modele  95  parametres / loss =  30597.17632065819  / info =  10.338163006580004\n",
      "\n",
      "\n",
      "---------- 96 ----------\n",
      "modele  95  parametres / loss =  30597.17632065819  / info =  10.338163006580004\n",
      "modele  96  parametres / loss =  30568.482379325644  / info =  10.337324769534993\n",
      "\n",
      "\n",
      "---------- 97 ----------\n",
      "modele  96  parametres / loss =  30568.482379325644  / info =  10.337324769534993\n",
      "modele  97  parametres / loss =  30517.044417363817  / info =  10.335740639862484\n",
      "\n",
      "\n",
      "---------- 98 ----------\n",
      "modele  97  parametres / loss =  30517.044417363817  / info =  10.335740639862484\n",
      "modele  98  parametres / loss =  30375.779477218206  / info =  10.331200842115013\n",
      "\n",
      "\n",
      "---------- 99 ----------\n",
      "modele  98  parametres / loss =  30375.779477218206  / info =  10.331200842115013\n",
      "modele  99  parametres / loss =  30346.560535585384  / info =  10.330338463396348\n",
      "\n",
      "\n",
      "---------- 100 ----------\n",
      "modele  99  parametres / loss =  30346.560535585384  / info =  10.330338463396348\n",
      "modele  100  parametres / loss =  30304.70236215977  / info =  10.32905817292682\n",
      "\n",
      "\n",
      "---------- 101 ----------\n",
      "modele  100  parametres / loss =  30304.70236215977  / info =  10.32905817292682\n",
      "modele  101  parametres / loss =  30177.443006271154  / info =  10.324950003956852\n",
      "\n",
      "\n",
      "---------- 102 ----------\n",
      "modele  101  parametres / loss =  30177.443006271154  / info =  10.324950003956852\n",
      "modele  102  parametres / loss =  30148.17026400955  / info =  10.324079512552878\n",
      "\n",
      "\n",
      "---------- 103 ----------\n",
      "modele  102  parametres / loss =  30148.17026400955  / info =  10.324079512552878\n",
      "modele  103  parametres / loss =  30113.27505079468  / info =  10.323021385098986\n",
      "\n",
      "\n",
      "---------- 104 ----------\n",
      "modele  103  parametres / loss =  30113.27505079468  / info =  10.323021385098986\n",
      "modele  104  parametres / loss =  29997.478321609266  / info =  10.319268601165037\n",
      "\n",
      "\n",
      "---------- 105 ----------\n",
      "modele  104  parametres / loss =  29997.478321609266  / info =  10.319268601165037\n",
      "modele  105  parametres / loss =  29968.474207713716  / info =  10.318401248361189\n",
      "\n",
      "\n",
      "---------- 106 ----------\n",
      "modele  105  parametres / loss =  29968.474207713716  / info =  10.318401248361189\n",
      "modele  106  parametres / loss =  29876.161139226155  / info =  10.315416155063502\n",
      "\n",
      "\n",
      "---------- 107 ----------\n",
      "modele  106  parametres / loss =  29876.161139226155  / info =  10.315416155063502\n",
      "modele  107  parametres / loss =  29844.018180202587  / info =  10.314439702765679\n",
      "\n",
      "\n",
      "---------- 108 ----------\n",
      "modele  107  parametres / loss =  29844.018180202587  / info =  10.314439702765679\n",
      "modele  108  parametres / loss =  29773.488517372043  / info =  10.312173629454623\n",
      "\n",
      "\n",
      "---------- 109 ----------\n",
      "modele  108  parametres / loss =  29773.488517372043  / info =  10.312173629454623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modele  109  parametres / loss =  29628.041387526984  / info =  10.307376536038069\n",
      "\n",
      "\n",
      "---------- 110 ----------\n",
      "modele  109  parametres / loss =  29628.041387526984  / info =  10.307376536038069\n",
      "modele  110  parametres / loss =  29600.562041050554  / info =  10.306548628005213\n",
      "\n",
      "\n",
      "---------- 111 ----------\n",
      "modele  110  parametres / loss =  29600.562041050554  / info =  10.306548628005213\n",
      "modele  111  parametres / loss =  29574.72376335473  / info =  10.305775348595182\n",
      "\n",
      "\n",
      "---------- 112 ----------\n",
      "modele  111  parametres / loss =  29574.72376335473  / info =  10.305775348595182\n",
      "modele  112  parametres / loss =  29550.405306303008  / info =  10.305052738689522\n",
      "\n",
      "\n",
      "---------- 113 ----------\n",
      "modele  112  parametres / loss =  29550.405306303008  / info =  10.305052738689522\n",
      "modele  113  parametres / loss =  29527.439987941223  / info =  10.304375279071483\n",
      "\n",
      "\n",
      "---------- 114 ----------\n",
      "modele  113  parametres / loss =  29527.439987941223  / info =  10.304375279071483\n",
      "modele  114  parametres / loss =  29505.766941423717  / info =  10.303741012762108\n",
      "\n",
      "\n",
      "---------- 115 ----------\n",
      "modele  114  parametres / loss =  29505.766941423717  / info =  10.303741012762108\n",
      "modele  115  parametres / loss =  29485.252043838158  / info =  10.303145486603535\n",
      "\n",
      "\n",
      "---------- 116 ----------\n",
      "modele  115  parametres / loss =  29485.252043838158  / info =  10.303145486603535\n",
      "modele  116  parametres / loss =  29463.35147075219  / info =  10.302502446979595\n",
      "\n",
      "\n",
      "---------- 117 ----------\n",
      "modele  116  parametres / loss =  29463.35147075219  / info =  10.302502446979595\n",
      "modele  117  parametres / loss =  29411.264952301142  / info =  10.300833041581992\n",
      "\n",
      "\n",
      "---------- 118 ----------\n",
      "modele  117  parametres / loss =  29411.264952301142  / info =  10.300833041581992\n",
      "modele  118  parametres / loss =  29284.664690721933  / info =  10.296619268593961\n",
      "\n",
      "\n",
      "---------- 119 ----------\n",
      "modele  118  parametres / loss =  29284.664690721933  / info =  10.296619268593961\n",
      "modele  119  parametres / loss =  29261.84160226396  / info =  10.295939611836996\n",
      "\n",
      "\n",
      "---------- 120 ----------\n",
      "modele  119  parametres / loss =  29261.84160226396  / info =  10.295939611836996\n",
      "modele  120  parametres / loss =  29220.949569987875  / info =  10.2946411820384\n",
      "\n",
      "\n",
      "---------- 121 ----------\n",
      "modele  120  parametres / loss =  29220.949569987875  / info =  10.2946411820384\n",
      "modele  121  parametres / loss =  29108.852555748832  / info =  10.290897618434396\n",
      "\n",
      "\n",
      "---------- 122 ----------\n",
      "modele  121  parametres / loss =  29108.852555748832  / info =  10.290897618434396\n",
      "modele  122  parametres / loss =  29085.586831703855  / info =  10.290198032579479\n",
      "\n",
      "\n",
      "---------- 123 ----------\n",
      "modele  122  parametres / loss =  29085.586831703855  / info =  10.290198032579479\n",
      "modele  123  parametres / loss =  29052.523092260937  / info =  10.289160611942627\n",
      "\n",
      "\n",
      "---------- 124 ----------\n",
      "modele  123  parametres / loss =  29052.523092260937  / info =  10.289160611942627\n",
      "modele  124  parametres / loss =  28951.75644411459  / info =  10.285786152805409\n",
      "\n",
      "\n",
      "---------- 125 ----------\n",
      "modele  124  parametres / loss =  28951.75644411459  / info =  10.285786152805409\n",
      "modele  125  parametres / loss =  28928.413088606078  / info =  10.285079542978153\n",
      "\n",
      "\n",
      "---------- 126 ----------\n",
      "modele  125  parametres / loss =  28928.413088606078  / info =  10.285079542978153\n",
      "modele  126  parametres / loss =  28863.665265999425  / info =  10.282938825857725\n",
      "\n",
      "\n",
      "---------- 127 ----------\n",
      "modele  126  parametres / loss =  28863.665265999425  / info =  10.282938825857725\n",
      "modele  127  parametres / loss =  28839.21092849069  / info =  10.28219123070873\n",
      "\n",
      "\n",
      "---------- 128 ----------\n",
      "modele  127  parametres / loss =  28839.21092849069  / info =  10.28219123070873\n",
      "modele  128  parametres / loss =  28778.759422476087  / info =  10.280192873969414\n",
      "\n",
      "\n",
      "---------- 129 ----------\n",
      "modele  128  parametres / loss =  28778.759422476087  / info =  10.280192873969414\n",
      "modele  129  parametres / loss =  28655.880009528046  / info =  10.276013937019533\n",
      "\n",
      "\n",
      "---------- 130 ----------\n",
      "modele  129  parametres / loss =  28655.880009528046  / info =  10.276013937019533\n",
      "modele  130  parametres / loss =  28635.07081146187  / info =  10.275387497723866\n",
      "\n",
      "\n",
      "---------- 131 ----------\n",
      "modele  130  parametres / loss =  28635.07081146187  / info =  10.275387497723866\n",
      "modele  131  parametres / loss =  28615.393231855225  / info =  10.27480007690692\n",
      "\n",
      "\n",
      "---------- 132 ----------\n",
      "modele  131  parametres / loss =  28615.393231855225  / info =  10.27480007690692\n",
      "modele  132  parametres / loss =  28596.798308848985  / info =  10.274250043298341\n",
      "\n",
      "\n",
      "---------- 133 ----------\n",
      "modele  132  parametres / loss =  28596.798308848985  / info =  10.274250043298341\n",
      "modele  133  parametres / loss =  28579.176132669716  / info =  10.273733624359812\n",
      "\n",
      "\n",
      "---------- 134 ----------\n",
      "modele  133  parametres / loss =  28579.176132669716  / info =  10.273733624359812\n",
      "modele  134  parametres / loss =  28561.202165828465  / info =  10.273204508208703\n",
      "\n",
      "\n",
      "---------- 135 ----------\n",
      "modele  134  parametres / loss =  28561.202165828465  / info =  10.273204508208703\n",
      "modele  135  parametres / loss =  28516.582349582415  / info =  10.271741033882778\n",
      "\n",
      "\n",
      "---------- 136 ----------\n",
      "modele  135  parametres / loss =  28516.582349582415  / info =  10.271741033882778\n",
      "modele  136  parametres / loss =  28410.30534838814  / info =  10.268107222709805\n",
      "\n",
      "\n",
      "---------- 137 ----------\n",
      "modele  136  parametres / loss =  28410.30534838814  / info =  10.268107222709805\n",
      "modele  137  parametres / loss =  28391.487827417874  / info =  10.267544654836271\n",
      "\n",
      "\n",
      "---------- 138 ----------\n",
      "modele  137  parametres / loss =  28391.487827417874  / info =  10.267544654836271\n",
      "modele  138  parametres / loss =  28356.759805963004  / info =  10.266420721860774\n",
      "\n",
      "\n",
      "---------- 139 ----------\n",
      "modele  138  parametres / loss =  28356.759805963004  / info =  10.266420721860774\n",
      "modele  139  parametres / loss =  28262.874319131945  / info =  10.263204360872935\n",
      "\n",
      "\n",
      "---------- 140 ----------\n",
      "modele  139  parametres / loss =  28262.874319131945  / info =  10.263204360872935\n",
      "modele  140  parametres / loss =  28243.57357233692  / info =  10.262621226531074\n",
      "\n",
      "\n",
      "---------- 141 ----------\n",
      "modele  140  parametres / loss =  28243.57357233692  / info =  10.262621226531074\n",
      "modele  141  parametres / loss =  28215.736635559646  / info =  10.261735138003587\n",
      "\n",
      "\n",
      "---------- 142 ----------\n",
      "modele  141  parametres / loss =  28215.736635559646  / info =  10.261735138003587\n",
      "modele  142  parametres / loss =  28131.5346091245  / info =  10.25884645417734\n",
      "\n",
      "\n",
      "---------- 143 ----------\n",
      "modele  142  parametres / loss =  28131.5346091245  / info =  10.25884645417734\n",
      "modele  143  parametres / loss =  28112.120047722336  / info =  10.258256080755066\n",
      "\n",
      "\n",
      "---------- 144 ----------\n",
      "modele  143  parametres / loss =  28112.120047722336  / info =  10.258256080755066\n",
      "modele  144  parametres / loss =  28058.014320948278  / info =  10.256429585683758\n",
      "\n",
      "\n",
      "---------- 145 ----------\n",
      "modele  144  parametres / loss =  28058.014320948278  / info =  10.256429585683758\n",
      "modele  145  parametres / loss =  28036.777735476542  / info =  10.255772417835985\n",
      "\n",
      "\n",
      "---------- 146 ----------\n",
      "modele  145  parametres / loss =  28036.777735476542  / info =  10.255772417835985\n",
      "modele  146  parametres / loss =  27985.68338822834  / info =  10.25404835083178\n",
      "\n",
      "\n",
      "---------- 147 ----------\n",
      "modele  146  parametres / loss =  27985.68338822834  / info =  10.25404835083178\n",
      "modele  147  parametres / loss =  27882.587541184963  / info =  10.250457670509956\n",
      "\n",
      "\n",
      "---------- 148 ----------\n",
      "modele  147  parametres / loss =  27882.587541184963  / info =  10.250457670509956\n",
      "modele  148  parametres / loss =  27864.734336598936  / info =  10.24991716601234\n",
      "\n",
      "\n",
      "---------- 149 ----------\n",
      "modele  148  parametres / loss =  27864.734336598936  / info =  10.24991716601234\n",
      "modele  149  parametres / loss =  27847.816112858938  / info =  10.249409826225397\n",
      "\n",
      "\n",
      "---------- 150 ----------\n",
      "modele  149  parametres / loss =  27847.816112858938  / info =  10.249409826225397\n",
      "modele  150  parametres / loss =  27831.80739712489  / info =  10.24893479661255\n",
      "\n",
      "\n",
      "---------- 151 ----------\n",
      "modele  150  parametres / loss =  27831.80739712489  / info =  10.24893479661255\n",
      "modele  151  parametres / loss =  27816.602755863427  / info =  10.248488342846917\n",
      "\n",
      "\n",
      "---------- 152 ----------\n",
      "modele  151  parametres / loss =  27816.602755863427  / info =  10.248488342846917\n",
      "modele  152  parametres / loss =  27802.189476971973  / info =  10.248070054741927\n",
      "\n",
      "\n",
      "---------- 153 ----------\n",
      "modele  152  parametres / loss =  27802.189476971973  / info =  10.248070054741927\n",
      "modele  153  parametres / loss =  27787.054464184574  / info =  10.247625524466343\n",
      "\n",
      "\n",
      "---------- 154 ----------\n",
      "modele  153  parametres / loss =  27787.054464184574  / info =  10.247625524466343\n",
      "modele  154  parametres / loss =  27749.772798504368  / info =  10.24638293169978\n",
      "\n",
      "\n",
      "---------- 155 ----------\n",
      "modele  154  parametres / loss =  27749.772798504368  / info =  10.24638293169978\n",
      "modele  155  parametres / loss =  27660.70674709724  / info =  10.24326815612505\n",
      "\n",
      "\n",
      "---------- 156 ----------\n",
      "modele  155  parametres / loss =  27660.70674709724  / info =  10.24326815612505\n",
      "modele  156  parametres / loss =  27644.8324830193  / info =  10.242794099177594\n",
      "\n",
      "\n",
      "---------- 157 ----------\n",
      "modele  156  parametres / loss =  27644.8324830193  / info =  10.242794099177594\n",
      "modele  157  parametres / loss =  27615.881824864817  / info =  10.241846314646914\n",
      "\n",
      "\n",
      "---------- 158 ----------\n",
      "modele  157  parametres / loss =  27615.881824864817  / info =  10.241846314646914\n",
      "modele  158  parametres / loss =  27537.29472974324  / info =  10.239096536874673\n",
      "\n",
      "\n",
      "---------- 159 ----------\n",
      "modele  158  parametres / loss =  27537.29472974324  / info =  10.239096536874673\n",
      "modele  159  parametres / loss =  27521.031611375358  / info =  10.238605777223167\n",
      "\n",
      "\n",
      "---------- 160 ----------\n",
      "modele  159  parametres / loss =  27521.031611375358  / info =  10.238605777223167\n",
      "modele  160  parametres / loss =  27482.5732226338  / info =  10.237307381786186\n",
      "\n",
      "\n",
      "---------- 161 ----------\n",
      "modele  160  parametres / loss =  27482.5732226338  / info =  10.237307381786186\n",
      "modele  161  parametres / loss =  27466.90188467688  / info =  10.236836990955291\n",
      "\n",
      "\n",
      "---------- 162 ----------\n",
      "modele  161  parametres / loss =  27466.90188467688  / info =  10.236836990955291\n",
      "modele  162  parametres / loss =  27420.336183013547  / info =  10.235240213544921\n",
      "\n",
      "\n",
      "---------- 163 ----------\n",
      "modele  162  parametres / loss =  27420.336183013547  / info =  10.235240213544921\n",
      "modele  163  parametres / loss =  27327.966797427936  / info =  10.231965881622244\n",
      "\n",
      "\n",
      "---------- 164 ----------\n",
      "modele  163  parametres / loss =  27327.966797427936  / info =  10.231965881622244\n",
      "modele  164  parametres / loss =  27314.25430086994  / info =  10.23156398047071\n",
      "\n",
      "\n",
      "---------- 165 ----------\n",
      "modele  164  parametres / loss =  27314.25430086994  / info =  10.23156398047071\n",
      "modele  165  parametres / loss =  27300.53331237905  / info =  10.231161516234222\n",
      "\n",
      "\n",
      "---------- 166 ----------\n",
      "modele  165  parametres / loss =  27300.53331237905  / info =  10.231161516234222\n",
      "modele  166  parametres / loss =  27287.477233313486  / info =  10.230783166531273\n",
      "\n",
      "\n",
      "---------- 167 ----------\n",
      "modele  166  parametres / loss =  27287.477233313486  / info =  10.230783166531273\n",
      "modele  167  parametres / loss =  27274.56850008988  / info =  10.23040999023112\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 168 ----------\n",
      "modele  167  parametres / loss =  27274.56850008988  / info =  10.23040999023112\n",
      "modele  168  parametres / loss =  27240.35720673945  / info =  10.229254873493897\n",
      "\n",
      "\n",
      "---------- 169 ----------\n",
      "modele  168  parametres / loss =  27240.35720673945  / info =  10.229254873493897\n",
      "modele  169  parametres / loss =  27161.064309933976  / info =  10.226439767574\n",
      "\n",
      "\n",
      "---------- 170 ----------\n",
      "modele  169  parametres / loss =  27161.064309933976  / info =  10.226439767574\n",
      "modele  170  parametres / loss =  27147.459112135693  / info =  10.22603873395255\n",
      "\n",
      "\n",
      "---------- 171 ----------\n",
      "modele  170  parametres / loss =  27147.459112135693  / info =  10.22603873395255\n",
      "modele  171  parametres / loss =  27121.10062418131  / info =  10.225167324718354\n",
      "\n",
      "\n",
      "---------- 172 ----------\n",
      "modele  171  parametres / loss =  27121.10062418131  / info =  10.225167324718354\n",
      "modele  172  parametres / loss =  27051.39733415336  / info =  10.222693940395265\n",
      "\n",
      "\n",
      "---------- 173 ----------\n",
      "modele  172  parametres / loss =  27051.39733415336  / info =  10.222693940395265\n",
      "modele  173  parametres / loss =  27037.389064638137  / info =  10.222275967234427\n",
      "\n",
      "\n",
      "---------- 174 ----------\n",
      "modele  173  parametres / loss =  27037.389064638137  / info =  10.222275967234427\n",
      "modele  174  parametres / loss =  27005.079217361017  / info =  10.221180246455795\n",
      "\n",
      "\n",
      "---------- 175 ----------\n",
      "modele  174  parametres / loss =  27005.079217361017  / info =  10.221180246455795\n",
      "modele  175  parametres / loss =  26991.7119862596  / info =  10.220785134391669\n",
      "\n",
      "\n",
      "---------- 176 ----------\n",
      "modele  175  parametres / loss =  26991.7119862596  / info =  10.220785134391669\n",
      "modele  176  parametres / loss =  26950.375325978013  / info =  10.219352502992598\n",
      "\n",
      "\n",
      "---------- 177 ----------\n",
      "modele  176  parametres / loss =  26950.375325978013  / info =  10.219352502992598\n",
      "modele  177  parametres / loss =  26869.121911675473  / info =  10.216433021941787\n",
      "\n",
      "\n",
      "---------- 178 ----------\n",
      "modele  177  parametres / loss =  26869.121911675473  / info =  10.216433021941787\n",
      "modele  178  parametres / loss =  26856.102138257014  / info =  10.216048341834684\n",
      "\n",
      "\n",
      "---------- 179 ----------\n",
      "modele  178  parametres / loss =  26856.102138257014  / info =  10.216048341834684\n",
      "modele  179  parametres / loss =  26843.701578475273  / info =  10.21568649433641\n",
      "\n",
      "\n",
      "---------- 180 ----------\n",
      "modele  179  parametres / loss =  26843.701578475273  / info =  10.21568649433641\n",
      "modele  180  parametres / loss =  26831.91677409165  / info =  10.21534738231741\n",
      "\n",
      "\n",
      "---------- 181 ----------\n",
      "modele  180  parametres / loss =  26831.91677409165  / info =  10.21534738231741\n",
      "modele  181  parametres / loss =  26820.67616003903  / info =  10.215028367555872\n",
      "\n",
      "\n",
      "---------- 182 ----------\n",
      "modele  181  parametres / loss =  26820.67616003903  / info =  10.215028367555872\n",
      "modele  182  parametres / loss =  26809.341357210036  / info =  10.214705663871536\n",
      "\n",
      "\n",
      "---------- 183 ----------\n",
      "modele  182  parametres / loss =  26809.341357210036  / info =  10.214705663871536\n",
      "modele  183  parametres / loss =  26779.268142172827  / info =  10.213683290349433\n",
      "\n",
      "\n",
      "---------- 184 ----------\n",
      "modele  183  parametres / loss =  26779.268142172827  / info =  10.213683290349433\n",
      "modele  184  parametres / loss =  26709.521436062092  / info =  10.211175388911\n",
      "\n",
      "\n",
      "---------- 185 ----------\n",
      "modele  184  parametres / loss =  26709.521436062092  / info =  10.211175388911\n",
      "modele  185  parametres / loss =  26697.555197547925  / info =  10.210827274561042\n",
      "\n",
      "\n",
      "---------- 186 ----------\n",
      "modele  185  parametres / loss =  26697.555197547925  / info =  10.210827274561042\n",
      "modele  186  parametres / loss =  26674.383295694046  / info =  10.210058956715974\n",
      "\n",
      "\n",
      "---------- 187 ----------\n",
      "modele  186  parametres / loss =  26674.383295694046  / info =  10.210058956715974\n",
      "modele  187  parametres / loss =  26613.004537032895  / info =  10.207855267670512\n",
      "\n",
      "\n",
      "---------- 188 ----------\n",
      "modele  187  parametres / loss =  26613.004537032895  / info =  10.207855267670512\n",
      "modele  188  parametres / loss =  26600.70155706544  / info =  10.207492868747764\n",
      "\n",
      "\n",
      "---------- 189 ----------\n",
      "modele  188  parametres / loss =  26600.70155706544  / info =  10.207492868747764\n",
      "modele  189  parametres / loss =  26572.27652522876  / info =  10.206523715370633\n",
      "\n",
      "\n",
      "---------- 190 ----------\n",
      "modele  189  parametres / loss =  26572.27652522876  / info =  10.206523715370633\n",
      "modele  190  parametres / loss =  26560.294131139985  / info =  10.206172677743659\n",
      "\n",
      "\n",
      "---------- 191 ----------\n",
      "modele  190  parametres / loss =  26560.294131139985  / info =  10.206172677743659\n",
      "modele  191  parametres / loss =  26523.94903356728  / info =  10.204903340965545\n",
      "\n",
      "\n",
      "---------- 192 ----------\n",
      "modele  191  parametres / loss =  26523.94903356728  / info =  10.204903340965545\n",
      "modele  192  parametres / loss =  26452.436336652805  / info =  10.202303544071055\n",
      "\n",
      "\n",
      "---------- 193 ----------\n",
      "modele  192  parametres / loss =  26452.436336652805  / info =  10.202303544071055\n",
      "modele  193  parametres / loss =  26441.20803495207  / info =  10.201978982594802\n",
      "\n",
      "\n",
      "---------- 194 ----------\n",
      "modele  193  parametres / loss =  26441.20803495207  / info =  10.201978982594802\n",
      "modele  194  parametres / loss =  26430.500457899703  / info =  10.201673942640394\n",
      "\n",
      "\n",
      "---------- 195 ----------\n",
      "modele  194  parametres / loss =  26430.500457899703  / info =  10.201673942640394\n",
      "modele  195  parametres / loss =  26420.306231697192  / info =  10.201388168913654\n",
      "\n",
      "\n",
      "---------- 196 ----------\n",
      "modele  195  parametres / loss =  26420.306231697192  / info =  10.201388168913654\n",
      "modele  196  parametres / loss =  26410.305781878873  / info =  10.201109583485373\n",
      "\n",
      "\n",
      "---------- 197 ----------\n",
      "modele  196  parametres / loss =  26410.305781878873  / info =  10.201109583485373\n",
      "modele  197  parametres / loss =  26383.91134712332  / info =  10.200209684693801\n",
      "\n",
      "\n",
      "---------- 198 ----------\n",
      "modele  197  parametres / loss =  26383.91134712332  / info =  10.200209684693801\n",
      "modele  198  parametres / loss =  26322.53516674971  / info =  10.197980701725832\n",
      "\n",
      "\n",
      "---------- 199 ----------\n",
      "modele  198  parametres / loss =  26322.53516674971  / info =  10.197980701725832\n",
      "modele  199  parametres / loss =  26311.981168169565  / info =  10.197679672156017\n",
      "\n",
      "\n",
      "---------- 200 ----------\n",
      "modele  199  parametres / loss =  26311.981168169565  / info =  10.197679672156017\n",
      "modele  200  parametres / loss =  26291.63296260472  / info =  10.197006029245127\n",
      "\n",
      "\n",
      "---------- 201 ----------\n",
      "modele  200  parametres / loss =  26291.63296260472  / info =  10.197006029245127\n",
      "modele  201  parametres / loss =  26237.67731910876  / info =  10.195051722337924\n",
      "\n",
      "\n",
      "---------- 202 ----------\n",
      "modele  201  parametres / loss =  26237.67731910876  / info =  10.195051722337924\n",
      "modele  202  parametres / loss =  26226.800901890903  / info =  10.194737102094193\n",
      "\n",
      "\n",
      "---------- 203 ----------\n",
      "modele  202  parametres / loss =  26226.800901890903  / info =  10.194737102094193\n",
      "modele  203  parametres / loss =  26201.341311595254  / info =  10.19386588353782\n",
      "\n",
      "\n",
      "---------- 204 ----------\n",
      "modele  203  parametres / loss =  26201.341311595254  / info =  10.19386588353782\n",
      "modele  204  parametres / loss =  26190.441796155137  / info =  10.193549806246656\n",
      "\n",
      "\n",
      "---------- 205 ----------\n",
      "modele  204  parametres / loss =  26190.441796155137  / info =  10.193549806246656\n",
      "modele  205  parametres / loss =  26158.41763663115  / info =  10.192426315845307\n",
      "\n",
      "\n",
      "---------- 206 ----------\n",
      "modele  205  parametres / loss =  26158.41763663115  / info =  10.192426315845307\n",
      "modele  206  parametres / loss =  26095.400175342464  / info =  10.19011433928633\n",
      "\n",
      "\n",
      "---------- 207 ----------\n",
      "modele  206  parametres / loss =  26095.400175342464  / info =  10.19011433928633\n",
      "modele  207  parametres / loss =  26085.666920653075  / info =  10.189841282338898\n",
      "\n",
      "\n",
      "---------- 208 ----------\n",
      "modele  207  parametres / loss =  26085.666920653075  / info =  10.189841282338898\n",
      "modele  208  parametres / loss =  26075.93769798726  / info =  10.18956824079427\n",
      "\n",
      "\n",
      "---------- 209 ----------\n",
      "modele  208  parametres / loss =  26075.93769798726  / info =  10.18956824079427\n",
      "modele  209  parametres / loss =  25847.98287807372  / info =  10.180887845095947\n",
      "\n",
      "\n",
      "---------- 210 ----------\n",
      "modele  209  parametres / loss =  25847.98287807372  / info =  10.180887845095947\n",
      "modele  210  parametres / loss =  25755.91537750239  / info =  10.177419603107655\n",
      "\n",
      "\n",
      "---------- 211 ----------\n",
      "modele  210  parametres / loss =  25755.91537750239  / info =  10.177419603107655\n",
      "modele  211  parametres / loss =  25742.962004760335  / info =  10.177016548529627\n",
      "\n",
      "\n",
      "---------- 212 ----------\n",
      "modele  211  parametres / loss =  25742.962004760335  / info =  10.177016548529627\n",
      "modele  212  parametres / loss =  25730.694338534868  / info =  10.176639890474643\n",
      "\n",
      "\n",
      "---------- 213 ----------\n",
      "modele  212  parametres / loss =  25730.694338534868  / info =  10.176639890474643\n",
      "modele  213  parametres / loss =  25719.03744240651  / info =  10.17628675317249\n",
      "\n",
      "\n",
      "---------- 214 ----------\n",
      "modele  213  parametres / loss =  25719.03744240651  / info =  10.17628675317249\n",
      "modele  214  parametres / loss =  25707.959272579785  / info =  10.175955922258545\n",
      "\n",
      "\n",
      "---------- 215 ----------\n",
      "modele  214  parametres / loss =  25707.959272579785  / info =  10.175955922258545\n",
      "modele  215  parametres / loss =  25697.413792803593  / info =  10.175645635189499\n",
      "\n",
      "\n",
      "---------- 216 ----------\n",
      "modele  215  parametres / loss =  25697.413792803593  / info =  10.175645635189499\n",
      "modele  216  parametres / loss =  25687.367210020242  / info =  10.175354601785626\n",
      "\n",
      "\n",
      "---------- 217 ----------\n",
      "modele  216  parametres / loss =  25687.367210020242  / info =  10.175354601785626\n",
      "modele  217  parametres / loss =  25677.79757674349  / info =  10.17508198998044\n",
      "\n",
      "\n",
      "---------- 218 ----------\n",
      "modele  217  parametres / loss =  25677.79757674349  / info =  10.17508198998044\n",
      "modele  218  parametres / loss =  25668.668318671625  / info =  10.17482639556868\n",
      "\n",
      "\n",
      "---------- 219 ----------\n",
      "modele  218  parametres / loss =  25668.668318671625  / info =  10.17482639556868\n",
      "modele  219  parametres / loss =  25659.460903925814  / info =  10.174567628748846\n",
      "\n",
      "\n",
      "---------- 220 ----------\n",
      "modele  219  parametres / loss =  25659.460903925814  / info =  10.174567628748846\n",
      "modele  220  parametres / loss =  25636.73214815634  / info =  10.173781451592042\n",
      "\n",
      "\n",
      "---------- 221 ----------\n",
      "modele  220  parametres / loss =  25636.73214815634  / info =  10.173781451592042\n",
      "modele  221  parametres / loss =  25582.558289317883  / info =  10.171766081442351\n",
      "\n",
      "\n",
      "---------- 222 ----------\n",
      "modele  221  parametres / loss =  25582.558289317883  / info =  10.171766081442351\n",
      "modele  222  parametres / loss =  25572.883095505913  / info =  10.171487814979766\n",
      "\n",
      "\n",
      "---------- 223 ----------\n",
      "modele  222  parametres / loss =  25572.883095505913  / info =  10.171487814979766\n",
      "modele  223  parametres / loss =  25555.621432499087  / info =  10.170912588359233\n",
      "\n",
      "\n",
      "---------- 224 ----------\n",
      "modele  223  parametres / loss =  25555.621432499087  / info =  10.170912588359233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modele  224  parametres / loss =  25546.71282970163  / info =  10.170663930983153\n",
      "\n",
      "\n",
      "---------- 225 ----------\n",
      "modele  224  parametres / loss =  25546.71282970163  / info =  10.170663930983153\n",
      "modele  225  parametres / loss =  25538.209683187488  / info =  10.170431028585563\n",
      "\n",
      "\n",
      "---------- 226 ----------\n",
      "modele  225  parametres / loss =  25538.209683187488  / info =  10.170431028585563\n",
      "modele  226  parametres / loss =  25530.09686213016  / info =  10.17021330427965\n",
      "\n",
      "\n",
      "---------- 227 ----------\n",
      "modele  226  parametres / loss =  25530.09686213016  / info =  10.17021330427965\n",
      "modele  227  parametres / loss =  25522.338883956887  / info =  10.17000938231708\n",
      "\n",
      "\n",
      "---------- 228 ----------\n",
      "modele  227  parametres / loss =  25522.338883956887  / info =  10.17000938231708\n",
      "modele  228  parametres / loss =  25514.650575568223  / info =  10.169808098541205\n",
      "\n",
      "\n",
      "---------- 229 ----------\n",
      "modele  228  parametres / loss =  25514.650575568223  / info =  10.169808098541205\n",
      "modele  229  parametres / loss =  25484.22388967409  / info =  10.168714868697476\n",
      "\n",
      "\n",
      "---------- 230 ----------\n",
      "modele  229  parametres / loss =  25484.22388967409  / info =  10.168714868697476\n",
      "modele  230  parametres / loss =  25424.268177956415  / info =  10.166459436941302\n",
      "\n",
      "\n",
      "---------- 231 ----------\n",
      "modele  230  parametres / loss =  25424.268177956415  / info =  10.166459436941302\n",
      "modele  231  parametres / loss =  25416.020465472197  / info =  10.166234981169055\n",
      "\n",
      "\n",
      "---------- 232 ----------\n",
      "modele  231  parametres / loss =  25416.020465472197  / info =  10.166234981169055\n",
      "modele  232  parametres / loss =  25393.800602211762  / info =  10.165460352437442\n",
      "\n",
      "\n",
      "---------- 233 ----------\n",
      "modele  232  parametres / loss =  25393.800602211762  / info =  10.165460352437442\n",
      "modele  233  parametres / loss =  25342.6365710904  / info =  10.163543496256157\n",
      "\n",
      "\n",
      "---------- 234 ----------\n",
      "modele  233  parametres / loss =  25342.6365710904  / info =  10.163543496256157\n",
      "modele  234  parametres / loss =  25333.894789422506  / info =  10.163298493095132\n",
      "\n",
      "\n",
      "---------- 235 ----------\n",
      "modele  234  parametres / loss =  25333.894789422506  / info =  10.163298493095132\n",
      "modele  235  parametres / loss =  25316.913854129423  / info =  10.162727983131557\n",
      "\n",
      "\n",
      "---------- 236 ----------\n",
      "modele  235  parametres / loss =  25316.913854129423  / info =  10.162727983131557\n",
      "modele  236  parametres / loss =  25272.089024742607  / info =  10.161055865138845\n",
      "\n",
      "\n",
      "---------- 237 ----------\n",
      "modele  236  parametres / loss =  25272.089024742607  / info =  10.161055865138845\n",
      "modele  237  parametres / loss =  25263.06905570762  / info =  10.160798887166543\n",
      "\n",
      "\n",
      "---------- 238 ----------\n",
      "modele  237  parametres / loss =  25263.06905570762  / info =  10.160798887166543\n",
      "modele  238  parametres / loss =  25243.609120669928  / info =  10.160128298535366\n",
      "\n",
      "\n",
      "---------- 239 ----------\n",
      "modele  238  parametres / loss =  25243.609120669928  / info =  10.160128298535366\n",
      "modele  239  parametres / loss =  25235.06025399407  / info =  10.15988958648675\n",
      "\n",
      "\n",
      "---------- 240 ----------\n",
      "modele  239  parametres / loss =  25235.06025399407  / info =  10.15988958648675\n",
      "modele  240  parametres / loss =  25209.0359005636  / info =  10.158957776712239\n",
      "\n",
      "\n",
      "---------- 241 ----------\n",
      "modele  240  parametres / loss =  25209.0359005636  / info =  10.158957776712239\n",
      "modele  241  parametres / loss =  25157.08144500017  / info =  10.156994704284616\n",
      "\n",
      "\n",
      "---------- 242 ----------\n",
      "modele  241  parametres / loss =  25157.08144500017  / info =  10.156994704284616\n",
      "modele  242  parametres / loss =  25149.243208575153  / info =  10.156783083964807\n",
      "\n",
      "\n",
      "---------- 243 ----------\n",
      "modele  242  parametres / loss =  25149.243208575153  / info =  10.156783083964807\n",
      "modele  243  parametres / loss =  25141.7571593272  / info =  10.156585374663004\n",
      "\n",
      "\n",
      "---------- 244 ----------\n",
      "modele  243  parametres / loss =  25141.7571593272  / info =  10.156585374663004\n",
      "modele  244  parametres / loss =  25134.60965886034  / info =  10.156401046221339\n",
      "\n",
      "\n",
      "---------- 245 ----------\n",
      "modele  244  parametres / loss =  25134.60965886034  / info =  10.156401046221339\n",
      "modele  245  parametres / loss =  25125.822817228473  / info =  10.156151393769559\n",
      "\n",
      "\n",
      "---------- 246 ----------\n",
      "modele  245  parametres / loss =  25125.822817228473  / info =  10.156151393769559\n",
      "modele  246  parametres / loss =  25096.85089192866  / info =  10.155097654775838\n",
      "\n",
      "\n",
      "---------- 247 ----------\n",
      "modele  246  parametres / loss =  25096.85089192866  / info =  10.155097654775838\n",
      "modele  247  parametres / loss =  25089.581347714557  / info =  10.154907953199515\n",
      "\n",
      "\n",
      "---------- 248 ----------\n",
      "modele  247  parametres / loss =  25089.581347714557  / info =  10.154907953199515\n",
      "modele  248  parametres / loss =  25082.264289662155  / info =  10.154716273353525\n",
      "\n",
      "\n",
      "---------- 249 ----------\n",
      "modele  248  parametres / loss =  25082.264289662155  / info =  10.154716273353525\n",
      "modele  249  parametres / loss =  25055.558405945947  / info =  10.153750974351455\n",
      "\n",
      "\n",
      "---------- 250 ----------\n",
      "modele  249  parametres / loss =  25055.558405945947  / info =  10.153750974351455\n",
      "modele  250  parametres / loss =  25048.25251235491  / info =  10.153559344093837\n",
      "\n",
      "\n",
      "---------- 251 ----------\n",
      "modele  250  parametres / loss =  25048.25251235491  / info =  10.153559344093837\n",
      "modele  251  parametres / loss =  25040.695887347625  / info =  10.153357615856132\n",
      "\n",
      "\n",
      "---------- 252 ----------\n",
      "modele  251  parametres / loss =  25040.695887347625  / info =  10.153357615856132\n",
      "modele  252  parametres / loss =  25022.023981680133  / info =  10.152711675300678\n",
      "\n",
      "\n",
      "---------- 253 ----------\n",
      "modele  252  parametres / loss =  25022.023981680133  / info =  10.152711675300678\n",
      "modele  253  parametres / loss =  24977.466970263256  / info =  10.151029376226688\n",
      "\n",
      "\n",
      "---------- 254 ----------\n",
      "modele  253  parametres / loss =  24977.466970263256  / info =  10.151029376226688\n",
      "modele  254  parametres / loss =  24969.51231432591  / info =  10.150810852219088\n",
      "\n",
      "\n",
      "---------- 255 ----------\n",
      "modele  254  parametres / loss =  24969.51231432591  / info =  10.150810852219088\n",
      "modele  255  parametres / loss =  24955.65391101516  / info =  10.150355685167499\n",
      "\n",
      "\n",
      "---------- 256 ----------\n",
      "modele  255  parametres / loss =  24955.65391101516  / info =  10.150355685167499\n",
      "modele  256  parametres / loss =  24948.66533671728  / info =  10.15017560623049\n",
      "\n",
      "\n",
      "---------- 257 ----------\n",
      "modele  256  parametres / loss =  24948.66533671728  / info =  10.15017560623049\n",
      "modele  257  parametres / loss =  24941.835297260735  / info =  10.150001805029364\n",
      "\n",
      "\n",
      "---------- 258 ----------\n",
      "modele  257  parametres / loss =  24941.835297260735  / info =  10.150001805029364\n",
      "modele  258  parametres / loss =  24934.53059770279  / info =  10.149808892766046\n",
      "\n",
      "\n",
      "---------- 259 ----------\n",
      "modele  258  parametres / loss =  24934.53059770279  / info =  10.149808892766046\n",
      "modele  259  parametres / loss =  24908.63854270379  / info =  10.148869951712555\n",
      "\n",
      "\n",
      "---------- 260 ----------\n",
      "modele  259  parametres / loss =  24908.63854270379  / info =  10.148869951712555\n",
      "modele  260  parametres / loss =  24901.754003003996  / info =  10.148693521859597\n",
      "\n",
      "\n",
      "---------- 261 ----------\n",
      "modele  260  parametres / loss =  24901.754003003996  / info =  10.148693521859597\n",
      "modele  261  parametres / loss =  24895.16604042796  / info =  10.148528928683604\n",
      "\n",
      "\n",
      "---------- 262 ----------\n",
      "modele  261  parametres / loss =  24895.16604042796  / info =  10.148528928683604\n",
      "modele  262  parametres / loss =  24888.89924399185  / info =  10.148377169554115\n",
      "\n",
      "\n",
      "---------- 263 ----------\n",
      "modele  262  parametres / loss =  24888.89924399185  / info =  10.148377169554115\n",
      "modele  263  parametres / loss =  24864.086547290564  / info =  10.14747973398864\n",
      "\n",
      "\n",
      "---------- 264 ----------\n",
      "modele  263  parametres / loss =  24864.086547290564  / info =  10.14747973398864\n",
      "modele  264  parametres / loss =  24814.818576926627  / info =  10.1455962769703\n",
      "\n",
      "\n",
      "---------- 265 ----------\n",
      "modele  264  parametres / loss =  24814.818576926627  / info =  10.1455962769703\n",
      "modele  265  parametres / loss =  24808.00776260263  / info =  10.145421773687469\n",
      "\n",
      "\n",
      "---------- 266 ----------\n",
      "modele  265  parametres / loss =  24808.00776260263  / info =  10.145421773687469\n",
      "modele  266  parametres / loss =  24789.88806938348  / info =  10.144791109867105\n",
      "\n",
      "\n",
      "---------- 267 ----------\n",
      "modele  266  parametres / loss =  24789.88806938348  / info =  10.144791109867105\n",
      "modele  267  parametres / loss =  24747.895036877853  / info =  10.143195715364927\n",
      "\n",
      "\n",
      "---------- 268 ----------\n",
      "modele  267  parametres / loss =  24747.895036877853  / info =  10.143195715364927\n",
      "modele  268  parametres / loss =  24740.688685746984  / info =  10.143004482493032\n",
      "\n",
      "\n",
      "---------- 269 ----------\n",
      "modele  268  parametres / loss =  24740.688685746984  / info =  10.143004482493032\n",
      "modele  269  parametres / loss =  24729.081316432268  / info =  10.14263521127842\n",
      "\n",
      "\n",
      "---------- 270 ----------\n",
      "modele  269  parametres / loss =  24729.081316432268  / info =  10.14263521127842\n",
      "modele  270  parametres / loss =  24722.580508770727  / info =  10.142472295636798\n",
      "\n",
      "\n",
      "---------- 271 ----------\n",
      "modele  270  parametres / loss =  24722.580508770727  / info =  10.142472295636798\n",
      "modele  271  parametres / loss =  24701.56132377111  / info =  10.141721732107017\n",
      "\n",
      "\n",
      "---------- 272 ----------\n",
      "modele  271  parametres / loss =  24701.56132377111  / info =  10.141721732107017\n",
      "modele  272  parametres / loss =  24675.080004485455  / info =  10.140749106626131\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 273 ----------\n",
      "modele  272  parametres / loss =  24675.080004485455  / info =  10.140749106626131\n",
      "modele  273  parametres / loss =  24667.923169229158  / info =  10.14055902151391\n",
      "\n",
      "\n",
      "---------- 274 ----------\n",
      "modele  273  parametres / loss =  24667.923169229158  / info =  10.14055902151391\n",
      "modele  274  parametres / loss =  24661.084263849163  / info =  10.140381744277667\n",
      "\n",
      "\n",
      "---------- 275 ----------\n",
      "modele  274  parametres / loss =  24661.084263849163  / info =  10.140381744277667\n",
      "modele  275  parametres / loss =  24654.546048674783  / info =  10.140216586348172\n",
      "\n",
      "\n",
      "---------- 276 ----------\n",
      "modele  275  parametres / loss =  24654.546048674783  / info =  10.140216586348172\n",
      "modele  276  parametres / loss =  24648.291020275137  / info =  10.140062847261131\n",
      "\n",
      "\n",
      "---------- 277 ----------\n",
      "modele  276  parametres / loss =  24648.291020275137  / info =  10.140062847261131\n",
      "modele  277  parametres / loss =  24642.298468484172  / info =  10.139919695297129\n",
      "\n",
      "\n",
      "---------- 278 ----------\n",
      "modele  277  parametres / loss =  24642.298468484172  / info =  10.139919695297129\n",
      "modele  278  parametres / loss =  24636.57986150698  / info =  10.139787603691287\n",
      "\n",
      "\n",
      "---------- 279 ----------\n",
      "modele  278  parametres / loss =  24636.57986150698  / info =  10.139787603691287\n",
      "modele  279  parametres / loss =  24614.929614308097  / info =  10.1390084327248\n",
      "\n",
      "\n",
      "---------- 280 ----------\n",
      "modele  279  parametres / loss =  24614.929614308097  / info =  10.1390084327248\n",
      "modele  280  parametres / loss =  24608.930372464623  / info =  10.13886467931437\n",
      "\n",
      "\n",
      "---------- 281 ----------\n",
      "modele  280  parametres / loss =  24608.930372464623  / info =  10.13886467931437\n",
      "modele  281  parametres / loss =  24603.1835693375  / info =  10.138731126934967\n",
      "\n",
      "\n",
      "---------- 282 ----------\n",
      "modele  281  parametres / loss =  24603.1835693375  / info =  10.138731126934967\n",
      "modele  282  parametres / loss =  24597.66398757048  / info =  10.138606757556623\n",
      "\n",
      "\n",
      "---------- 283 ----------\n",
      "modele  282  parametres / loss =  24597.66398757048  / info =  10.138606757556623\n",
      "modele  283  parametres / loss =  24574.806698414133  / info =  10.137777079202708\n",
      "\n",
      "\n",
      "---------- 284 ----------\n",
      "modele  283  parametres / loss =  24574.806698414133  / info =  10.137777079202708\n",
      "modele  284  parametres / loss =  24529.02188913218  / info =  10.136012262390366\n",
      "\n",
      "\n",
      "---------- 285 ----------\n",
      "modele  284  parametres / loss =  24529.02188913218  / info =  10.136012262390366\n",
      "modele  285  parametres / loss =  24522.5860199203  / info =  10.135849850228027\n",
      "\n",
      "\n",
      "---------- 286 ----------\n",
      "modele  285  parametres / loss =  24522.5860199203  / info =  10.135849850228027\n",
      "modele  286  parametres / loss =  24505.915687036184  / info =  10.135269824000584\n",
      "\n",
      "\n",
      "---------- 287 ----------\n",
      "modele  286  parametres / loss =  24505.915687036184  / info =  10.135269824000584\n",
      "modele  287  parametres / loss =  24466.884386146612  / info =  10.133775824511252\n",
      "\n",
      "\n",
      "---------- 288 ----------\n",
      "modele  287  parametres / loss =  24466.884386146612  / info =  10.133775824511252\n",
      "modele  288  parametres / loss =  24460.077314386435  / info =  10.133597570085982\n",
      "\n",
      "\n",
      "---------- 289 ----------\n",
      "modele  288  parametres / loss =  24460.077314386435  / info =  10.133597570085982\n",
      "modele  289  parametres / loss =  24449.541652525255  / info =  10.133266748409318\n",
      "\n",
      "\n",
      "---------- 290 ----------\n",
      "modele  289  parametres / loss =  24449.541652525255  / info =  10.133266748409318\n",
      "modele  290  parametres / loss =  24443.863228918322  / info =  10.133134470716806\n",
      "\n",
      "\n",
      "---------- 291 ----------\n",
      "modele  290  parametres / loss =  24443.863228918322  / info =  10.133134470716806\n",
      "modele  291  parametres / loss =  24438.077149203407  / info =  10.132997733807542\n",
      "\n",
      "\n",
      "---------- 292 ----------\n",
      "modele  291  parametres / loss =  24438.077149203407  / info =  10.132997733807542\n",
      "modele  292  parametres / loss =  24416.882229912917  / info =  10.132230066701089\n",
      "\n",
      "\n",
      "---------- 293 ----------\n",
      "modele  292  parametres / loss =  24416.882229912917  / info =  10.132230066701089\n",
      "modele  293  parametres / loss =  24411.17876641925  / info =  10.132096452529105\n",
      "\n",
      "\n",
      "---------- 294 ----------\n",
      "modele  293  parametres / loss =  24411.17876641925  / info =  10.132096452529105\n",
      "modele  294  parametres / loss =  24405.712855507034  / info =  10.131972517298761\n",
      "\n",
      "\n",
      "---------- 295 ----------\n",
      "modele  294  parametres / loss =  24405.712855507034  / info =  10.131972517298761\n",
      "modele  295  parametres / loss =  24400.8398734201  / info =  10.13187283173071\n",
      "\n",
      "\n",
      "---------- 296 ----------\n",
      "modele  295  parametres / loss =  24400.8398734201  / info =  10.13187283173071\n",
      "modele  296  parametres / loss =  24379.582370426975  / info =  10.131101272881445\n",
      "\n",
      "\n",
      "---------- 297 ----------\n",
      "modele  296  parametres / loss =  24379.582370426975  / info =  10.131101272881445\n",
      "modele  297  parametres / loss =  24337.201409865753  / info =  10.129461380878569\n",
      "\n",
      "\n",
      "---------- 298 ----------\n",
      "modele  297  parametres / loss =  24337.201409865753  / info =  10.129461380878569\n",
      "modele  298  parametres / loss =  24331.27285836759  / info =  10.129317750823727\n",
      "\n",
      "\n",
      "---------- 299 ----------\n",
      "modele  298  parametres / loss =  24331.27285836759  / info =  10.129317750823727\n",
      "modele  299  parametres / loss =  24315.819892822234  / info =  10.128782441900164\n",
      "\n",
      "\n",
      "---------- 300 ----------\n",
      "modele  299  parametres / loss =  24315.819892822234  / info =  10.128782441900164\n",
      "modele  300  parametres / loss =  24279.735370213748  / info =  10.127397346028937\n",
      "\n",
      "\n",
      "---------- 301 ----------\n",
      "modele  300  parametres / loss =  24279.735370213748  / info =  10.127397346028937\n",
      "modele  301  parametres / loss =  24273.41465350815  / info =  10.127236983241426\n",
      "\n",
      "\n",
      "---------- 302 ----------\n",
      "modele  301  parametres / loss =  24273.41465350815  / info =  10.127236983241426\n",
      "modele  302  parametres / loss =  24263.68166606871  / info =  10.126935929684132\n",
      "\n",
      "\n",
      "---------- 303 ----------\n",
      "modele  302  parametres / loss =  24263.68166606871  / info =  10.126935929684132\n",
      "modele  303  parametres / loss =  24258.285761956795  / info =  10.126813518911437\n",
      "\n",
      "\n",
      "---------- 304 ----------\n",
      "modele  303  parametres / loss =  24258.285761956795  / info =  10.126813518911437\n",
      "modele  304  parametres / loss =  24252.4871360033  / info =  10.12667445341589\n",
      "\n",
      "\n",
      "---------- 305 ----------\n",
      "modele  304  parametres / loss =  24252.4871360033  / info =  10.12667445341589\n",
      "modele  305  parametres / loss =  24231.790579235305  / info =  10.125920710346985\n",
      "\n",
      "\n",
      "---------- 306 ----------\n",
      "modele  305  parametres / loss =  24231.790579235305  / info =  10.125920710346985\n",
      "modele  306  parametres / loss =  24226.360798036912  / info =  10.125796608474884\n",
      "\n",
      "\n",
      "---------- 307 ----------\n",
      "modele  306  parametres / loss =  24226.360798036912  / info =  10.125796608474884\n",
      "modele  307  parametres / loss =  24221.30547226  / info =  10.125687916243267\n",
      "\n",
      "\n",
      "---------- 308 ----------\n",
      "modele  307  parametres / loss =  24221.30547226  / info =  10.125687916243267\n",
      "modele  308  parametres / loss =  24201.865311666148  / info =  10.124984988168812\n",
      "\n",
      "\n",
      "---------- 309 ----------\n",
      "modele  308  parametres / loss =  24201.865311666148  / info =  10.124984988168812\n",
      "modele  309  parametres / loss =  24196.445084866195  / info =  10.124861004034733\n",
      "\n",
      "\n",
      "---------- 310 ----------\n",
      "modele  309  parametres / loss =  24196.445084866195  / info =  10.124861004034733\n",
      "modele  310  parametres / loss =  24191.24561159064  / info =  10.124746095109725\n",
      "\n",
      "\n",
      "---------- 311 ----------\n",
      "modele  310  parametres / loss =  24191.24561159064  / info =  10.124746095109725\n",
      "modele  311  parametres / loss =  24186.69102415821  / info =  10.124657803183172\n",
      "\n",
      "\n",
      "---------- 312 ----------\n",
      "modele  311  parametres / loss =  24186.69102415821  / info =  10.124657803183172\n",
      "modele  312  parametres / loss =  24167.25785565526  / info =  10.123954014868074\n",
      "\n",
      "\n",
      "---------- 313 ----------\n",
      "modele  312  parametres / loss =  24167.25785565526  / info =  10.123954014868074\n",
      "modele  313  parametres / loss =  24128.186233119257  / info =  10.12243598932561\n",
      "\n",
      "\n",
      "---------- 314 ----------\n",
      "modele  313  parametres / loss =  24128.186233119257  / info =  10.12243598932561\n",
      "modele  314  parametres / loss =  24122.628152294405  / info =  10.122305606443154\n",
      "\n",
      "\n",
      "---------- 315 ----------\n",
      "modele  314  parametres / loss =  24122.628152294405  / info =  10.122305606443154\n",
      "modele  315  parametres / loss =  24108.502255428863  / info =  10.12181984794444\n",
      "\n",
      "\n",
      "---------- 316 ----------\n",
      "modele  315  parametres / loss =  24108.502255428863  / info =  10.12181984794444\n",
      "modele  316  parametres / loss =  24075.195558539763  / info =  10.1205373595218\n",
      "\n",
      "\n",
      "---------- 317 ----------\n",
      "modele  316  parametres / loss =  24075.195558539763  / info =  10.1205373595218\n",
      "modele  317  parametres / loss =  24069.2837410953  / info =  10.12039177300554\n",
      "\n",
      "\n",
      "---------- 318 ----------\n",
      "modele  317  parametres / loss =  24069.2837410953  / info =  10.12039177300554\n",
      "modele  318  parametres / loss =  24060.29876900579  / info =  10.120118407115985\n",
      "\n",
      "\n",
      "---------- 319 ----------\n",
      "modele  318  parametres / loss =  24060.29876900579  / info =  10.120118407115985\n",
      "modele  319  parametres / loss =  24055.17427990184  / info =  10.120005399166624\n",
      "\n",
      "\n",
      "---------- 320 ----------\n",
      "modele  319  parametres / loss =  24055.17427990184  / info =  10.120005399166624\n",
      "modele  320  parametres / loss =  24049.99042609322  / info =  10.119889877450387\n",
      "\n",
      "\n",
      "---------- 321 ----------\n",
      "modele  320  parametres / loss =  24049.99042609322  / info =  10.119889877450387\n",
      "modele  321  parametres / loss =  24030.87396092366  / info =  10.119194697645673\n",
      "\n",
      "\n",
      "---------- 322 ----------\n",
      "modele  321  parametres / loss =  24030.87396092366  / info =  10.119194697645673\n",
      "modele  322  parametres / loss =  24025.741875239473  / info =  10.119081112664109\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 323 ----------\n",
      "modele  322  parametres / loss =  24025.741875239473  / info =  10.119081112664109\n",
      "modele  323  parametres / loss =  24020.81393657303  / info =  10.118975980845585\n",
      "\n",
      "\n",
      "---------- 324 ----------\n",
      "modele  323  parametres / loss =  24020.81393657303  / info =  10.118975980845585\n",
      "modele  324  parametres / loss =  24016.641415842234  / info =  10.118902261371453\n",
      "\n",
      "\n",
      "---------- 325 ----------\n",
      "modele  324  parametres / loss =  24016.641415842234  / info =  10.118902261371453\n",
      "modele  325  parametres / loss =  23998.733597793238  / info =  10.118256341179254\n",
      "\n",
      "\n",
      "---------- 326 ----------\n",
      "modele  325  parametres / loss =  23998.733597793238  / info =  10.118256341179254\n",
      "modele  326  parametres / loss =  23962.67836867275  / info =  10.116852830984945\n",
      "\n",
      "\n",
      "---------- 327 ----------\n",
      "modele  326  parametres / loss =  23962.67836867275  / info =  10.116852830984945\n",
      "modele  327  parametres / loss =  23957.52133396311  / info =  10.116737596709564\n",
      "\n",
      "\n",
      "---------- 328 ----------\n",
      "modele  327  parametres / loss =  23957.52133396311  / info =  10.116737596709564\n",
      "modele  328  parametres / loss =  23944.50393250137  / info =  10.116294095606644\n",
      "\n",
      "\n",
      "---------- 329 ----------\n",
      "modele  328  parametres / loss =  23944.50393250137  / info =  10.116294095606644\n",
      "modele  329  parametres / loss =  23913.755918405186  / info =  10.115109133787191\n",
      "\n",
      "\n",
      "---------- 330 ----------\n",
      "modele  329  parametres / loss =  23913.755918405186  / info =  10.115109133787191\n",
      "modele  330  parametres / loss =  23908.272964477117  / info =  10.114979827165111\n",
      "\n",
      "\n",
      "---------- 331 ----------\n",
      "modele  330  parametres / loss =  23908.272964477117  / info =  10.114979827165111\n",
      "modele  331  parametres / loss =  23900.011303490202  / info =  10.114734210868871\n",
      "\n",
      "\n",
      "---------- 332 ----------\n",
      "modele  331  parametres / loss =  23900.011303490202  / info =  10.114734210868871\n",
      "modele  332  parametres / loss =  23895.159976829163  / info =  10.114631205981652\n",
      "\n",
      "\n",
      "---------- 333 ----------\n",
      "modele  332  parametres / loss =  23895.159976829163  / info =  10.114631205981652\n",
      "modele  333  parametres / loss =  23889.90502546695  / info =  10.114511264818733\n",
      "\n",
      "\n",
      "---------- 334 ----------\n",
      "modele  333  parametres / loss =  23889.90502546695  / info =  10.114511264818733\n",
      "modele  334  parametres / loss =  23871.16676800314  / info =  10.113826598238958\n",
      "\n",
      "\n",
      "---------- 335 ----------\n",
      "modele  334  parametres / loss =  23871.16676800314  / info =  10.113826598238958\n",
      "modele  335  parametres / loss =  23866.291520113264  / info =  10.113722345726083\n",
      "\n",
      "\n",
      "---------- 336 ----------\n",
      "modele  335  parametres / loss =  23866.291520113264  / info =  10.113722345726083\n",
      "modele  336  parametres / loss =  23861.67335403993  / info =  10.113628825382195\n",
      "\n",
      "\n",
      "---------- 337 ----------\n",
      "modele  336  parametres / loss =  23861.67335403993  / info =  10.113628825382195\n",
      "modele  337  parametres / loss =  23844.02791961595  / info =  10.112989063267497\n",
      "\n",
      "\n",
      "---------- 338 ----------\n",
      "modele  337  parametres / loss =  23844.02791961595  / info =  10.112989063267497\n",
      "modele  338  parametres / loss =  23839.160456191687  / info =  10.112884904795424\n",
      "\n",
      "\n",
      "---------- 339 ----------\n",
      "modele  338  parametres / loss =  23839.160456191687  / info =  10.112884904795424\n",
      "modele  339  parametres / loss =  23834.488661827374  / info =  10.112788914160527\n",
      "\n",
      "\n",
      "---------- 340 ----------\n",
      "modele  339  parametres / loss =  23834.488661827374  / info =  10.112788914160527\n",
      "modele  340  parametres / loss =  23830.59499028472  / info =  10.112725537900467\n",
      "\n",
      "\n",
      "---------- 341 ----------\n",
      "modele  340  parametres / loss =  23830.59499028472  / info =  10.112725537900467\n",
      "modele  341  parametres / loss =  23814.169820806885  / info =  10.112136053118931\n",
      "\n",
      "\n",
      "---------- 342 ----------\n",
      "modele  341  parametres / loss =  23814.169820806885  / info =  10.112136053118931\n",
      "modele  342  parametres / loss =  23780.85833263919  / info =  10.110836264321684\n",
      "\n",
      "\n",
      "---------- 343 ----------\n",
      "modele  342  parametres / loss =  23780.85833263919  / info =  10.110836264321684\n",
      "modele  343  parametres / loss =  23776.035669988767  / info =  10.110733447768244\n",
      "\n",
      "\n",
      "---------- 344 ----------\n",
      "modele  343  parametres / loss =  23776.035669988767  / info =  10.110733447768244\n",
      "modele  344  parametres / loss =  23764.078082669803  / info =  10.110330395224729\n",
      "\n",
      "\n",
      "---------- 345 ----------\n",
      "modele  344  parametres / loss =  23764.078082669803  / info =  10.110330395224729\n",
      "modele  345  parametres / loss =  23735.65168146028  / info =  10.10923348716886\n",
      "\n",
      "\n",
      "---------- 346 ----------\n",
      "modele  345  parametres / loss =  23735.65168146028  / info =  10.10923348716886\n",
      "modele  346  parametres / loss =  23730.53111578583  / info =  10.109117730796951\n",
      "\n",
      "\n",
      "---------- 347 ----------\n",
      "modele  346  parametres / loss =  23730.53111578583  / info =  10.109117730796951\n",
      "modele  347  parametres / loss =  23722.89805367  / info =  10.108896023290656\n",
      "\n",
      "\n",
      "---------- 348 ----------\n",
      "modele  347  parametres / loss =  23722.89805367  / info =  10.108896023290656\n",
      "modele  348  parametres / loss =  23718.299798104294  / info =  10.108802172553816\n",
      "\n",
      "\n",
      "---------- 349 ----------\n",
      "modele  348  parametres / loss =  23718.299798104294  / info =  10.108802172553816\n",
      "modele  349  parametres / loss =  23713.550549214546  / info =  10.108701916866092\n",
      "\n",
      "\n",
      "---------- 350 ----------\n",
      "modele  349  parametres / loss =  23713.550549214546  / info =  10.108701916866092\n",
      "modele  350  parametres / loss =  23696.18944647571  / info =  10.108069531347148\n",
      "\n",
      "\n",
      "---------- 351 ----------\n",
      "modele  350  parametres / loss =  23696.18944647571  / info =  10.108069531347148\n",
      "modele  351  parametres / loss =  23691.57647723626  / info =  10.10797484104411\n",
      "\n",
      "\n",
      "---------- 352 ----------\n",
      "modele  351  parametres / loss =  23691.57647723626  / info =  10.10797484104411\n",
      "modele  352  parametres / loss =  23687.149767074152  / info =  10.107887976156599\n",
      "\n",
      "\n",
      "---------- 353 ----------\n",
      "modele  352  parametres / loss =  23687.149767074152  / info =  10.107887976156599\n",
      "modele  353  parametres / loss =  23682.90925278592  / info =  10.107808938410509\n",
      "\n",
      "\n",
      "---------- 354 ----------\n",
      "modele  353  parametres / loss =  23682.90925278592  / info =  10.107808938410509\n",
      "modele  354  parametres / loss =  23677.99747859408  / info =  10.10770151948429\n",
      "\n",
      "\n",
      "---------- 355 ----------\n",
      "modele  354  parametres / loss =  23677.99747859408  / info =  10.10770151948429\n",
      "modele  355  parametres / loss =  23661.02311855463  / info =  10.107084379139907\n",
      "\n",
      "\n",
      "---------- 356 ----------\n",
      "modele  355  parametres / loss =  23661.02311855463  / info =  10.107084379139907\n",
      "modele  356  parametres / loss =  23656.739860415903  / info =  10.107003336847088\n",
      "\n",
      "\n",
      "---------- 357 ----------\n",
      "modele  356  parametres / loss =  23656.739860415903  / info =  10.107003336847088\n",
      "modele  357  parametres / loss =  23652.44677490413  / info =  10.106921846281347\n",
      "\n",
      "\n",
      "---------- 358 ----------\n",
      "modele  357  parametres / loss =  23652.44677490413  / info =  10.106921846281347\n",
      "modele  358  parametres / loss =  23636.489772787576  / info =  10.106346973726067\n",
      "\n",
      "\n",
      "---------- 359 ----------\n",
      "modele  358  parametres / loss =  23636.489772787576  / info =  10.106346973726067\n",
      "modele  359  parametres / loss =  23632.192738517668  / info =  10.106265160565496\n",
      "\n",
      "\n",
      "---------- 360 ----------\n",
      "modele  359  parametres / loss =  23632.192738517668  / info =  10.106265160565496\n",
      "modele  360  parametres / loss =  23628.064512939356  / info =  10.106190458783113\n",
      "\n",
      "\n",
      "---------- 361 ----------\n",
      "modele  360  parametres / loss =  23628.064512939356  / info =  10.106190458783113\n",
      "modele  361  parametres / loss =  23624.363522349682  / info =  10.106133811146108\n",
      "\n",
      "\n",
      "---------- 362 ----------\n",
      "modele  361  parametres / loss =  23624.363522349682  / info =  10.106133811146108\n",
      "modele  362  parametres / loss =  23609.358838596054  / info =  10.105598473361646\n",
      "\n",
      "\n",
      "---------- 363 ----------\n",
      "modele  362  parametres / loss =  23609.358838596054  / info =  10.105598473361646\n",
      "modele  363  parametres / loss =  23578.483541468864  / info =  10.104389860804854\n",
      "\n",
      "\n",
      "---------- 364 ----------\n",
      "modele  363  parametres / loss =  23578.483541468864  / info =  10.104389860804854\n",
      "modele  364  parametres / loss =  23573.914092079845  / info =  10.104296044598991\n",
      "\n",
      "\n",
      "---------- 365 ----------\n",
      "modele  364  parametres / loss =  23573.914092079845  / info =  10.104296044598991\n",
      "modele  365  parametres / loss =  23562.963454126148  / info =  10.103931413137564\n",
      "\n",
      "\n",
      "---------- 366 ----------\n",
      "modele  365  parametres / loss =  23562.963454126148  / info =  10.103931413137564\n",
      "modele  366  parametres / loss =  23536.58440012684  / info =  10.102911272585933\n",
      "\n",
      "\n",
      "---------- 367 ----------\n",
      "modele  366  parametres / loss =  23536.58440012684  / info =  10.102911272585933\n",
      "modele  367  parametres / loss =  23531.740142216844  / info =  10.102805433182292\n",
      "\n",
      "\n",
      "---------- 368 ----------\n",
      "modele  367  parametres / loss =  23531.740142216844  / info =  10.102805433182292\n",
      "modele  368  parametres / loss =  23524.620970595162  / info =  10.10260285255312\n",
      "\n",
      "\n",
      "---------- 369 ----------\n",
      "modele  368  parametres / loss =  23524.620970595162  / info =  10.10260285255312\n",
      "modele  369  parametres / loss =  23520.550485531425  / info =  10.102529806735754\n",
      "\n",
      "\n",
      "---------- 370 ----------\n",
      "modele  369  parametres / loss =  23520.550485531425  / info =  10.102529806735754\n",
      "modele  370  parametres / loss =  23516.19651627382  / info =  10.102444676192189\n",
      "\n",
      "\n",
      "---------- 371 ----------\n",
      "modele  370  parametres / loss =  23516.19651627382  / info =  10.102444676192189\n",
      "modele  371  parametres / loss =  23500.519364866777  / info =  10.101877800520665\n",
      "\n",
      "\n",
      "---------- 372 ----------\n",
      "modele  371  parametres / loss =  23500.519364866777  / info =  10.101877800520665\n",
      "modele  372  parametres / loss =  23496.426825040042  / info =  10.101803638573182\n",
      "\n",
      "\n",
      "---------- 373 ----------\n",
      "modele  372  parametres / loss =  23496.426825040042  / info =  10.101803638573182\n",
      "modele  373  parametres / loss =  23492.495874791373  / info =  10.10173632466037\n",
      "\n",
      "\n",
      "---------- 374 ----------\n",
      "modele  373  parametres / loss =  23492.495874791373  / info =  10.10173632466037\n",
      "modele  374  parametres / loss =  23489.10512701708  / info =  10.101691981014863\n",
      "\n",
      "\n",
      "---------- 375 ----------\n",
      "modele  374  parametres / loss =  23489.10512701708  / info =  10.101691981014863\n",
      "modele  375  parametres / loss =  23475.109715057748  / info =  10.101195977763423\n",
      "\n",
      "\n",
      "---------- 376 ----------\n",
      "modele  375  parametres / loss =  23475.109715057748  / info =  10.101195977763423\n",
      "modele  376  parametres / loss =  23446.451628817133  / info =  10.100074445956581\n",
      "\n",
      "\n",
      "---------- 377 ----------\n",
      "modele  376  parametres / loss =  23446.451628817133  / info =  10.100074445956581\n",
      "modele  377  parametres / loss =  23442.24537795788  / info =  10.099995031678127\n",
      "\n",
      "\n",
      "---------- 378 ----------\n",
      "modele  377  parametres / loss =  23442.24537795788  / info =  10.099995031678127\n",
      "modele  378  parametres / loss =  23432.045798963733  / info =  10.099659843053447\n",
      "\n",
      "\n",
      "---------- 379 ----------\n",
      "modele  378  parametres / loss =  23432.045798963733  / info =  10.099659843053447\n",
      "modele  379  parametres / loss =  23407.58692086926  / info =  10.098715476183028\n",
      "\n",
      "\n",
      "---------- 380 ----------\n",
      "modele  379  parametres / loss =  23407.58692086926  / info =  10.098715476183028\n",
      "modele  380  parametres / loss =  23403.118000176037  / info =  10.098624540339856\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 381 ----------\n",
      "modele  380  parametres / loss =  23403.118000176037  / info =  10.098624540339856\n",
      "modele  381  parametres / loss =  23396.4812354166  / info =  10.09844091548356\n",
      "\n",
      "\n",
      "---------- 382 ----------\n",
      "modele  381  parametres / loss =  23396.4812354166  / info =  10.09844091548356\n",
      "modele  382  parametres / loss =  23392.607361099523  / info =  10.098375326691722\n",
      "\n",
      "\n",
      "---------- 383 ----------\n",
      "modele  382  parametres / loss =  23392.607361099523  / info =  10.098375326691722\n",
      "modele  383  parametres / loss =  23388.21683803857  / info =  10.098287620589959\n",
      "\n",
      "\n",
      "---------- 384 ----------\n",
      "modele  383  parametres / loss =  23388.21683803857  / info =  10.098287620589959\n",
      "modele  384  parametres / loss =  23372.85387558683  / info =  10.097730537132714\n",
      "\n",
      "\n",
      "---------- 385 ----------\n",
      "modele  384  parametres / loss =  23372.85387558683  / info =  10.097730537132714\n",
      "modele  385  parametres / loss =  23368.950975254163  / info =  10.09766353885979\n",
      "\n",
      "\n",
      "---------- 386 ----------\n",
      "modele  385  parametres / loss =  23368.950975254163  / info =  10.09766353885979\n",
      "modele  386  parametres / loss =  23365.07335214686  / info =  10.097597594532418\n",
      "\n",
      "\n",
      "---------- 387 ----------\n",
      "modele  386  parametres / loss =  23365.07335214686  / info =  10.097597594532418\n",
      "modele  387  parametres / loss =  23350.56984323735  / info =  10.097076667220561\n",
      "\n",
      "\n",
      "---------- 388 ----------\n",
      "modele  387  parametres / loss =  23350.56984323735  / info =  10.097076667220561\n",
      "modele  388  parametres / loss =  23346.66427684179  / info =  10.097009395369101\n",
      "\n",
      "\n",
      "---------- 389 ----------\n",
      "modele  388  parametres / loss =  23346.66427684179  / info =  10.097009395369101\n",
      "modele  389  parametres / loss =  23342.91441929591  / info =  10.096948766052723\n",
      "\n",
      "\n",
      "---------- 390 ----------\n",
      "modele  389  parametres / loss =  23342.91441929591  / info =  10.096948766052723\n",
      "modele  390  parametres / loss =  23339.69974066253  / info =  10.096911041176073\n",
      "\n",
      "\n",
      "---------- 391 ----------\n",
      "modele  390  parametres / loss =  23339.69974066253  / info =  10.096911041176073\n",
      "modele  391  parametres / loss =  23326.739091490264  / info =  10.096455582056848\n",
      "\n",
      "\n",
      "---------- 392 ----------\n",
      "modele  391  parametres / loss =  23326.739091490264  / info =  10.096455582056848\n",
      "modele  392  parametres / loss =  23300.15308215811  / info =  10.095415209581914\n",
      "\n",
      "\n",
      "---------- 393 ----------\n",
      "modele  392  parametres / loss =  23300.15308215811  / info =  10.095415209581914\n",
      "modele  393  parametres / loss =  23296.242692112915  / info =  10.095347368701116\n",
      "\n",
      "\n",
      "---------- 394 ----------\n",
      "modele  393  parametres / loss =  23296.242692112915  / info =  10.095347368701116\n",
      "modele  394  parametres / loss =  23286.79556293229  / info =  10.095041764873457\n",
      "\n",
      "\n",
      "---------- 395 ----------\n",
      "modele  394  parametres / loss =  23286.79556293229  / info =  10.095041764873457\n",
      "modele  395  parametres / loss =  23264.095965751752  / info =  10.094166505400038\n",
      "\n",
      "\n",
      "---------- 396 ----------\n",
      "modele  395  parametres / loss =  23264.095965751752  / info =  10.094166505400038\n",
      "modele  396  parametres / loss =  23259.944541454413  / info =  10.094088041781525\n",
      "\n",
      "\n",
      "---------- 397 ----------\n",
      "modele  396  parametres / loss =  23259.944541454413  / info =  10.094088041781525\n",
      "modele  397  parametres / loss =  23253.713034078133  / info =  10.09392009865521\n",
      "\n",
      "\n",
      "---------- 398 ----------\n",
      "modele  397  parametres / loss =  23253.713034078133  / info =  10.09392009865521\n",
      "modele  398  parametres / loss =  23250.017897313246  / info =  10.09386118079212\n",
      "\n",
      "\n",
      "---------- 399 ----------\n",
      "modele  398  parametres / loss =  23250.017897313246  / info =  10.09386118079212\n",
      "modele  399  parametres / loss =  23246.048237826006  / info =  10.093790428303711\n",
      "\n",
      "\n",
      "---------- 400 ----------\n",
      "modele  399  parametres / loss =  23246.048237826006  / info =  10.093790428303711\n",
      "modele  400  parametres / loss =  23231.763393755362  / info =  10.093275733075215\n",
      "\n",
      "\n",
      "---------- 401 ----------\n",
      "modele  400  parametres / loss =  23231.763393755362  / info =  10.093275733075215\n",
      "modele  401  parametres / loss =  23228.05552290769  / info =  10.093216116832966\n",
      "\n",
      "\n",
      "---------- 402 ----------\n",
      "modele  401  parametres / loss =  23228.05552290769  / info =  10.093216116832966\n",
      "modele  402  parametres / loss =  23224.495412338747  / info =  10.093162837389567\n",
      "\n",
      "\n",
      "---------- 403 ----------\n",
      "modele  402  parametres / loss =  23224.495412338747  / info =  10.093162837389567\n",
      "modele  403  parametres / loss =  23221.481306522568  / info =  10.093133047640988\n",
      "\n",
      "\n",
      "---------- 404 ----------\n",
      "modele  403  parametres / loss =  23221.481306522568  / info =  10.093133047640988\n",
      "modele  404  parametres / loss =  23209.403555074758  / info =  10.092712801181925\n",
      "\n",
      "\n",
      "---------- 405 ----------\n",
      "modele  404  parametres / loss =  23209.403555074758  / info =  10.092712801181925\n",
      "modele  405  parametres / loss =  23184.712975034607  / info =  10.09174841697902\n",
      "\n",
      "\n",
      "---------- 406 ----------\n",
      "modele  405  parametres / loss =  23184.712975034607  / info =  10.09174841697902\n",
      "modele  406  parametres / loss =  23181.103139537292  / info =  10.09169270590534\n",
      "\n",
      "\n",
      "---------- 407 ----------\n",
      "modele  406  parametres / loss =  23181.103139537292  / info =  10.09169270590534\n",
      "modele  407  parametres / loss =  23172.307588614523  / info =  10.091413206278876\n",
      "\n",
      "\n",
      "---------- 408 ----------\n",
      "modele  407  parametres / loss =  23172.307588614523  / info =  10.091413206278876\n",
      "modele  408  parametres / loss =  23151.24895985304  / info =  10.090604008808683\n",
      "\n",
      "\n",
      "---------- 409 ----------\n",
      "modele  408  parametres / loss =  23151.24895985304  / info =  10.090604008808683\n",
      "modele  409  parametres / loss =  23147.418225322188  / info =  10.090538529550964\n",
      "\n",
      "\n",
      "---------- 410 ----------\n",
      "modele  409  parametres / loss =  23147.418225322188  / info =  10.090538529550964\n",
      "modele  410  parametres / loss =  23141.56383124976  / info =  10.090385579762762\n",
      "\n",
      "\n",
      "---------- 411 ----------\n",
      "modele  410  parametres / loss =  23141.56383124976  / info =  10.090385579762762\n",
      "modele  411  parametres / loss =  23138.05647111221  / info =  10.090334007207872\n",
      "\n",
      "\n",
      "---------- 412 ----------\n",
      "modele  411  parametres / loss =  23138.05647111221  / info =  10.090334007207872\n",
      "modele  412  parametres / loss =  23134.027132573334  / info =  10.09025984869867\n",
      "\n",
      "\n",
      "---------- 413 ----------\n",
      "modele  412  parametres / loss =  23134.027132573334  / info =  10.09025984869867\n",
      "modele  413  parametres / loss =  23119.997786116655  / info =  10.089753227030108\n",
      "\n",
      "\n",
      "---------- 414 ----------\n",
      "modele  413  parametres / loss =  23119.997786116655  / info =  10.089753227030108\n",
      "modele  414  parametres / loss =  23116.468767625112  / info =  10.08970057615683\n",
      "\n",
      "\n",
      "---------- 415 ----------\n",
      "modele  414  parametres / loss =  23116.468767625112  / info =  10.08970057615683\n",
      "modele  415  parametres / loss =  23112.888576859063  / info =  10.089645687965477\n",
      "\n",
      "\n",
      "---------- 416 ----------\n",
      "modele  415  parametres / loss =  23112.888576859063  / info =  10.089645687965477\n",
      "modele  416  parametres / loss =  23099.625048503993  / info =  10.089171664711657\n",
      "\n",
      "\n",
      "---------- 417 ----------\n",
      "modele  416  parametres / loss =  23099.625048503993  / info =  10.089171664711657\n",
      "modele  417  parametres / loss =  23096.093952494106  / info =  10.089118789246434\n",
      "\n",
      "\n",
      "---------- 418 ----------\n",
      "modele  417  parametres / loss =  23096.093952494106  / info =  10.089118789246434\n",
      "modele  418  parametres / loss =  23092.705758203658  / info =  10.089072078601527\n",
      "\n",
      "\n",
      "---------- 419 ----------\n",
      "modele  418  parametres / loss =  23092.705758203658  / info =  10.089072078601527\n",
      "modele  419  parametres / loss =  23089.44947951674  / info =  10.089031059687379\n",
      "\n",
      "\n",
      "---------- 420 ----------\n",
      "modele  419  parametres / loss =  23089.44947951674  / info =  10.089031059687379\n",
      "modele  420  parametres / loss =  23085.708012331776  / info =  10.088969004270727\n",
      "\n",
      "\n",
      "---------- 421 ----------\n",
      "modele  420  parametres / loss =  23085.708012331776  / info =  10.088969004270727\n",
      "modele  421  parametres / loss =  23072.658634180778  / info =  10.088503586581727\n",
      "\n",
      "\n",
      "---------- 422 ----------\n",
      "modele  421  parametres / loss =  23072.658634180778  / info =  10.088503586581727\n",
      "modele  422  parametres / loss =  23069.38193409684  / info =  10.088461559915844\n",
      "\n",
      "\n",
      "---------- 423 ----------\n",
      "modele  422  parametres / loss =  23069.38193409684  / info =  10.088461559915844\n",
      "modele  423  parametres / loss =  23066.04457937699  / info =  10.088416883471824\n",
      "\n",
      "\n",
      "---------- 424 ----------\n",
      "modele  423  parametres / loss =  23066.04457937699  / info =  10.088416883471824\n",
      "modele  424  parametres / loss =  23053.678147527065  / info =  10.087980608255275\n",
      "\n",
      "\n",
      "---------- 425 ----------\n",
      "modele  424  parametres / loss =  23053.678147527065  / info =  10.087980608255275\n",
      "modele  425  parametres / loss =  23050.7312368544  / info =  10.087952771863224\n",
      "\n",
      "\n",
      "---------- 426 ----------\n",
      "modele  425  parametres / loss =  23050.7312368544  / info =  10.087952771863224\n",
      "modele  426  parametres / loss =  23039.594709724333  / info =  10.08756952393134\n",
      "\n",
      "\n",
      "---------- 427 ----------\n",
      "modele  426  parametres / loss =  23039.594709724333  / info =  10.08756952393134\n",
      "modele  427  parametres / loss =  23016.622112818302  / info =  10.08667193444631\n",
      "\n",
      "\n",
      "---------- 428 ----------\n",
      "modele  427  parametres / loss =  23016.622112818302  / info =  10.08667193444631\n",
      "modele  428  parametres / loss =  23013.21846511801  / info =  10.086624045700063\n",
      "\n",
      "\n",
      "---------- 429 ----------\n",
      "modele  428  parametres / loss =  23013.21846511801  / info =  10.086624045700063\n",
      "modele  429  parametres / loss =  23005.09592705966  / info =  10.086371032416114\n",
      "\n",
      "\n",
      "---------- 430 ----------\n",
      "modele  429  parametres / loss =  23005.09592705966  / info =  10.086371032416114\n",
      "modele  430  parametres / loss =  22985.476646943698  / info =  10.085617845329802\n",
      "\n",
      "\n",
      "---------- 431 ----------\n",
      "modele  430  parametres / loss =  22985.476646943698  / info =  10.085617845329802\n",
      "modele  431  parametres / loss =  22981.86736350404  / info =  10.085560808480068\n",
      "\n",
      "\n",
      "---------- 432 ----------\n",
      "modele  431  parametres / loss =  22981.86736350404  / info =  10.085560808480068\n",
      "modele  432  parametres / loss =  22976.308983162795  / info =  10.085418919840865\n",
      "\n",
      "\n",
      "---------- 433 ----------\n",
      "modele  432  parametres / loss =  22976.308983162795  / info =  10.085418919840865\n",
      "modele  433  parametres / loss =  22972.94676969351  / info =  10.085372575208284\n",
      "\n",
      "\n",
      "---------- 434 ----------\n",
      "modele  433  parametres / loss =  22972.94676969351  / info =  10.085372575208284\n",
      "modele  434  parametres / loss =  22969.721735471336  / info =  10.08533218135079\n",
      "\n",
      "\n",
      "---------- 435 ----------\n",
      "modele  434  parametres / loss =  22969.721735471336  / info =  10.08533218135079\n",
      "modele  435  parametres / loss =  22966.619920930338  / info =  10.085297132958434\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 436 ----------\n",
      "modele  435  parametres / loss =  22966.619920930338  / info =  10.085297132958434\n",
      "modele  436  parametres / loss =  22963.186360673833  / info =  10.085247619579611\n",
      "\n",
      "\n",
      "---------- 437 ----------\n",
      "modele  436  parametres / loss =  22963.186360673833  / info =  10.085247619579611\n",
      "modele  437  parametres / loss =  22950.978661115245  / info =  10.084815857765346\n",
      "\n",
      "\n",
      "---------- 438 ----------\n",
      "modele  437  parametres / loss =  22950.978661115245  / info =  10.084815857765346\n",
      "modele  438  parametres / loss =  22947.863946762518  / info =  10.084780136941877\n",
      "\n",
      "\n",
      "---------- 439 ----------\n",
      "modele  438  parametres / loss =  22947.863946762518  / info =  10.084780136941877\n",
      "modele  439  parametres / loss =  22944.873646795662  / info =  10.084749820027792\n",
      "\n",
      "\n",
      "---------- 440 ----------\n",
      "modele  439  parametres / loss =  22944.873646795662  / info =  10.084749820027792\n",
      "modele  440  parametres / loss =  22942.113259111786  / info =  10.084729507587452\n",
      "\n",
      "\n",
      "---------- 441 ----------\n",
      "modele  440  parametres / loss =  22942.113259111786  / info =  10.084729507587452\n",
      "modele  441  parametres / loss =  22931.672841659965  / info =  10.08437432747177\n",
      "\n",
      "\n",
      "---------- 442 ----------\n",
      "modele  441  parametres / loss =  22931.672841659965  / info =  10.08437432747177\n",
      "modele  442  parametres / loss =  22910.249357759225  / info =  10.083539659620488\n",
      "\n",
      "\n",
      "---------- 443 ----------\n",
      "modele  442  parametres / loss =  22910.249357759225  / info =  10.083539659620488\n",
      "modele  443  parametres / loss =  22907.092235891534  / info =  10.083501846217837\n",
      "\n",
      "\n",
      "---------- 444 ----------\n",
      "modele  443  parametres / loss =  22907.092235891534  / info =  10.083501846217837\n",
      "modele  444  parametres / loss =  22899.505877723954  / info =  10.083270611917568\n",
      "\n",
      "\n",
      "---------- 445 ----------\n",
      "modele  444  parametres / loss =  22899.505877723954  / info =  10.083270611917568\n",
      "modele  445  parametres / loss =  22881.22843423499  / info =  10.082572134359934\n",
      "\n",
      "\n",
      "---------- 446 ----------\n",
      "modele  445  parametres / loss =  22881.22843423499  / info =  10.082572134359934\n",
      "modele  446  parametres / loss =  22877.88327411757  / info =  10.082525926970915\n",
      "\n",
      "\n",
      "---------- 447 ----------\n",
      "modele  446  parametres / loss =  22877.88327411757  / info =  10.082525926970915\n",
      "modele  447  parametres / loss =  22872.641177054174  / info =  10.082396766883715\n",
      "\n",
      "\n",
      "---------- 448 ----------\n",
      "modele  447  parametres / loss =  22872.641177054174  / info =  10.082396766883715\n",
      "modele  448  parametres / loss =  22869.689460776142  / info =  10.082367708471475\n",
      "\n",
      "\n",
      "---------- 449 ----------\n",
      "modele  448  parametres / loss =  22869.689460776142  / info =  10.082367708471475\n",
      "modele  449  parametres / loss =  22865.474517217503  / info =  10.08228338887124\n",
      "\n",
      "\n",
      "---------- 450 ----------\n",
      "modele  449  parametres / loss =  22865.474517217503  / info =  10.08228338887124\n",
      "modele  450  parametres / loss =  22862.905715245848  / info =  10.08227103842448\n",
      "\n",
      "\n",
      "---------- 451 ----------\n",
      "modele  450  parametres / loss =  22862.905715245848  / info =  10.08227103842448\n",
      "modele  451  parametres / loss =  22853.05985186638  / info =  10.081940297723301\n",
      "\n",
      "\n",
      "---------- 452 ----------\n",
      "modele  451  parametres / loss =  22853.05985186638  / info =  10.081940297723301\n",
      "modele  452  parametres / loss =  22833.107313220935  / info =  10.08116683684216\n",
      "\n",
      "\n",
      "---------- 453 ----------\n",
      "modele  452  parametres / loss =  22833.107313220935  / info =  10.08116683684216\n",
      "modele  453  parametres / loss =  22830.221511126736  / info =  10.081140442110442\n",
      "\n",
      "\n",
      "---------- 454 ----------\n",
      "modele  453  parametres / loss =  22830.221511126736  / info =  10.081140442110442\n",
      "modele  454  parametres / loss =  22823.087658504828  / info =  10.080927919193279\n",
      "\n",
      "\n",
      "---------- 455 ----------\n",
      "modele  454  parametres / loss =  22823.087658504828  / info =  10.080927919193279\n",
      "modele  455  parametres / loss =  22806.097525135046  / info =  10.080283214501025\n",
      "\n",
      "\n",
      "---------- 456 ----------\n",
      "modele  455  parametres / loss =  22806.097525135046  / info =  10.080283214501025\n",
      "modele  456  parametres / loss =  22803.034541878336  / info =  10.080248900028351\n",
      "\n",
      "\n",
      "---------- 457 ----------\n",
      "modele  456  parametres / loss =  22803.034541878336  / info =  10.080248900028351\n",
      "modele  457  parametres / loss =  22798.08323301071  / info =  10.080131742680958\n",
      "\n",
      "\n",
      "---------- 458 ----------\n",
      "modele  457  parametres / loss =  22798.08323301071  / info =  10.080131742680958\n",
      "modele  458  parametres / loss =  22795.65653767575  / info =  10.080125294061062\n",
      "\n",
      "\n",
      "---------- 459 ----------\n",
      "modele  458  parametres / loss =  22795.65653767575  / info =  10.080125294061062\n",
      "modele  459  parametres / loss =  22786.377122793427  / info =  10.079818141757167\n",
      "\n",
      "\n",
      "---------- 460 ----------\n",
      "modele  459  parametres / loss =  22786.377122793427  / info =  10.079818141757167\n",
      "modele  460  parametres / loss =  22767.837041526258  / info =  10.07910416295714\n",
      "\n",
      "\n",
      "---------- 461 ----------\n",
      "modele  460  parametres / loss =  22767.837041526258  / info =  10.07910416295714\n",
      "modele  461  parametres / loss =  22765.21061762306  / info =  10.07908879954348\n",
      "\n",
      "\n",
      "---------- 462 ----------\n",
      "modele  461  parametres / loss =  22765.21061762306  / info =  10.07908879954348\n",
      "modele  462  parametres / loss =  22758.519013771907  / info =  10.078894816429466\n",
      "\n",
      "\n",
      "---------- 463 ----------\n",
      "modele  462  parametres / loss =  22758.519013771907  / info =  10.078894816429466\n",
      "modele  463  parametres / loss =  22742.7762355573  / info =  10.078302845849002\n",
      "\n",
      "\n",
      "---------- 464 ----------\n",
      "modele  463  parametres / loss =  22742.7762355573  / info =  10.078302845849002\n",
      "modele  464  parametres / loss =  22739.981029674575  / info =  10.078279933077027\n",
      "\n",
      "\n",
      "---------- 465 ----------\n",
      "modele  464  parametres / loss =  22739.981029674575  / info =  10.078279933077027\n",
      "modele  465  parametres / loss =  22735.260077404375  / info =  10.078172305710037\n",
      "\n",
      "\n",
      "---------- 466 ----------\n",
      "modele  465  parametres / loss =  22735.260077404375  / info =  10.078172305710037\n",
      "modele  466  parametres / loss =  22732.83665588002  / info =  10.078165706942956\n",
      "\n",
      "\n",
      "---------- 467 ----------\n",
      "modele  466  parametres / loss =  22732.83665588002  / info =  10.078165706942956\n",
      "modele  467  parametres / loss =  22727.255723877133  / info =  10.078020175896327\n",
      "\n",
      "\n",
      "---------- 468 ----------\n",
      "modele  467  parametres / loss =  22727.255723877133  / info =  10.078020175896327\n",
      "modele  468  parametres / loss =  22712.85772178001  / info =  10.077486462576214\n",
      "\n",
      "\n",
      "---------- 469 ----------\n",
      "modele  468  parametres / loss =  22712.85772178001  / info =  10.077486462576214\n",
      "modele  469  parametres / loss =  22710.25852939528  / info =  10.077472018979998\n",
      "\n",
      "\n",
      "---------- 470 ----------\n",
      "modele  469  parametres / loss =  22710.25852939528  / info =  10.077472018979998\n",
      "modele  470  parametres / loss =  22705.54328993445  / info =  10.077364371452536\n",
      "\n",
      "\n",
      "---------- 471 ----------\n",
      "modele  470  parametres / loss =  22705.54328993445  / info =  10.077364371452536\n",
      "modele  471  parametres / loss =  22692.228843493092  / info =  10.076877803160858\n",
      "\n",
      "\n",
      "---------- 472 ----------\n",
      "modele  471  parametres / loss =  22692.228843493092  / info =  10.076877803160858\n",
      "modele  472  parametres / loss =  22689.5535754085  / info =  10.07685990263139\n",
      "\n",
      "\n",
      "---------- 473 ----------\n",
      "modele  472  parametres / loss =  22689.5535754085  / info =  10.07685990263139\n",
      "modele  473  parametres / loss =  22685.488474365964  / info =  10.076780724789574\n",
      "\n",
      "\n",
      "---------- 474 ----------\n",
      "modele  473  parametres / loss =  22685.488474365964  / info =  10.076780724789574\n",
      "modele  474  parametres / loss =  22673.055852339967  / info =  10.07633253160913\n",
      "\n",
      "\n",
      "---------- 475 ----------\n",
      "modele  474  parametres / loss =  22673.055852339967  / info =  10.07633253160913\n",
      "modele  475  parametres / loss =  22670.32919102943  / info =  10.076312264394355\n",
      "\n",
      "\n",
      "---------- 476 ----------\n",
      "modele  475  parametres / loss =  22670.32919102943  / info =  10.076312264394355\n",
      "modele  476  parametres / loss =  22666.703827031055  / info =  10.07625233491663\n",
      "\n",
      "\n",
      "---------- 477 ----------\n",
      "modele  476  parametres / loss =  22666.703827031055  / info =  10.07625233491663\n",
      "modele  477  parametres / loss =  22664.295722994935  / info =  10.07624608956302\n",
      "\n",
      "\n",
      "---------- 478 ----------\n",
      "modele  477  parametres / loss =  22664.295722994935  / info =  10.07624608956302\n",
      "modele  478  parametres / loss =  22655.6957284963  / info =  10.075966566340423\n",
      "\n",
      "\n",
      "---------- 479 ----------\n",
      "modele  478  parametres / loss =  22655.6957284963  / info =  10.075966566340423\n",
      "modele  479  parametres / loss =  22638.438805309615  / info =  10.075304572576801\n",
      "\n",
      "\n",
      "---------- 480 ----------\n",
      "modele  479  parametres / loss =  22638.438805309615  / info =  10.075304572576801\n",
      "modele  480  parametres / loss =  22635.976105695  / info =  10.075295782673193\n",
      "\n",
      "\n",
      "---------- 481 ----------\n",
      "modele  480  parametres / loss =  22635.976105695  / info =  10.075295782673193\n",
      "modele  481  parametres / loss =  22629.770754822028  / info =  10.075121608431422\n",
      "\n",
      "\n",
      "---------- 482 ----------\n",
      "modele  481  parametres / loss =  22629.770754822028  / info =  10.075121608431422\n",
      "modele  482  parametres / loss =  22615.11484319573  / info =  10.074573760085904\n",
      "\n",
      "\n",
      "---------- 483 ----------\n",
      "modele  482  parametres / loss =  22615.11484319573  / info =  10.074573760085904\n",
      "modele  483  parametres / loss =  22612.495242457044  / info =  10.074557919309147\n",
      "\n",
      "\n",
      "---------- 484 ----------\n",
      "modele  483  parametres / loss =  22612.495242457044  / info =  10.074557919309147\n",
      "modele  484  parametres / loss =  22607.972979490172  / info =  10.074457909748887\n",
      "\n",
      "\n",
      "---------- 485 ----------\n",
      "modele  484  parametres / loss =  22607.972979490172  / info =  10.074457909748887\n",
      "modele  485  parametres / loss =  22605.73827479701  / info =  10.074459058996183\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.fit(x.reshape(n,1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "961301bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x, test.pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "955d99d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAikElEQVR4nO3df5DU5Z0n8PdnhsY0JOtABBdaPYhF4ckR6dyU4nK3ZTRKohE7lj8X7rjalNZWXe6CeHMZVi5AgpHNXIx7Vbt7pUluuYMzICEtxk2Q06S2jhKzoz04EpxDoyINC7PBUVe6oJn53B/9/bJt0z3T/X2e789+v6qmZqa7v99+5tszn3n6eT7P5xFVBRERJUtH2A0gIiL7GNyJiBKIwZ2IKIEY3ImIEojBnYgogSaF3QAAuOiii3TOnDlhN4OIKFZefvnlf1DVGfXui0RwnzNnDvr7+8NuBhFRrIjIO43u47AMEVECMbgTESUQgzsRUQIxuBMRJRCDOxFRAkUiW4aIqN3kC0X07R7C0ZESZnel0bN0PnLZjLXzM7gTEQUsXyhizc5BlMqjAIDiSAlrdg4CgLUAz2EZIqKA9e0eOhfYXaXyKPp2D1l7DgZ3IqKAFUdKLd3uBYM7EVGA8oViw/s6Raw9T6zH3P2ekCAisskda29k1OLOeLHtubsXqThSgqLyduaBbQNYm2984YiIwlRvrL1apitt7bliG9zrXSQFsGXfYQZ4Ioqko+OMqac6BT1L51t7rtgG9/EmHrbsOzzuuBYRURhmj9MzF9hLgwRiHNwnmnh46KfsvRNRtIzXMz8zqlZHHSYM7iLyIxE5ISKvVd02XUT2iMgh5/O0qvvWiMgbIjIkIkuttbTGRBMPH50ZZe+diGJly77D1s7VTM/9rwF8sea2XgDPq+o8AM8730NErgRwD4AFzjF/KSKd1lrbog3PHAjrqYmIzmNzkdJEJgzuqvq3AE7W3HwbgM3O15sB5Kpu/7GqnlbVtwC8AeBqO01t3XunymE9NRHROflCEUs2vWB1kdJEvI65X6yqxwDA+TzTuT0D4N2qxx1xbjuPiNwvIv0i0j88POyxGURE0Vadth0k2xOq9WY56w6Oq+rjqtqtqt0zZtTd39WKub3PYsmmFzj+TkShmCi33S9eg/txEZkFAM7nE87tRwBcWvW4SwAc9d48c+4CpzU7BxngiShQ+UIx8B67y2tw3wVgpfP1SgBPV91+j4hcICJzAcwD8GuzJtphu+IaEdF48oUienbsD+35J6wtIyJPArgOwEUicgTAOgCbAGwXka8COAzgTgBQ1QMish3AbwCcBfDvVTX49yMNjLc6jIjIpr7dQyiP2qsV06oJg7uq3tvgrhsaPP5hAA+bNMov460OIyKyKezOZGxXqHphs24DEdF4vHQmWTjMI5YDJqKgfP6K1rMAWTiMiCjC8oUinvz1uxM/sAYLhwHo9LBhCVMhiSgIG545gNGx8CZTgRgH9+/dtajlY1ZtG2CAJyLfeSl9Mm1KymobYhvcvb59YTExIvKT1w7kulsXWG1HbIO7VywmRkR+yReK6Hmq9YVLtjfqAGIe3G3uFE5EZKpv9xDKHsba/Ridj3Vwv/eaSyd+EBGRz0xL+vrRUY11cN+YW4gViy9r+bhrHt7jQ2uIqB3ZKOk70c5yXsQ6uAOVAN+q4x+esbpXIRG1LxslfW2uTHXFPrh7tWXfYWS/9RxTI4nIiI0aMn6URmnb4A5UMmd6duxngCciz0wLEnalU76URklEcO9Ke0/+L48q67wTkWc9S+fX3YKuWeuX2c1vdyUiuK9ftgCpDu+XN6ydUogo/nLZjOdURr967UAT9dzjwL04q7YNhNsQIqImpTrEt147kJCeO1AJ8I/dvSjsZhBRG8kXipi/9ueeju278ypfy5AnJrgDZst3OalKRK1w89tPnx1r+di3N93i+/4SiQrugPfJVU6qElErbOS3+ylxwd3rGFZxpMTeOxE1zWsihh8LlupJXHA3eavDnHciaoZJnAhqL+fEBXfA+39G5rwTUTPW7/K2L8QFkzoC28s5kcHdZFEBc96JaCIjJW/7QniZfPUqkcE9l81guYdqkUREE4lL0cFEBnegUi1ySsrbj8dxdyKqZ21+EFv2HfZ8vEmplFYlNrgDwORJnZ6O47g7EdXKF4pGgd3vFam1jIK7iDwgIgdE5DUReVJEPiEi00Vkj4gccj5Ps9XYVr3vcVysOFLCkk0vsAdPROdseMbbJKrL7xWptTwHdxHJAPiPALpV9V8A6ARwD4BeAM+r6jwAzzvfh8KkFGdxpISep5gaSUQV753y1lkEKhl8QQZ2wHxYZhKAtIhMAjAFwFEAtwHY7Ny/GUDO8Dk861k6H+mUt6EZACiPqeeUJyIiV1C57dU8B3dVLQL4rwAOAzgG4H1VfQ7Axap6zHnMMQAz6x0vIveLSL+I9A8PD3ttxrhy2QweuX2h0YowrylPRJQM7ubXXvlZ1nc8JsMy01Dppc8FMBvAVBFZ0ezxqvq4qnaraveMGTO8NmNCuWwGe3uvx9ubbjEqqE9E7cfG5tdBTqJWMxmW+QKAt1R1WFXLAHYC+AMAx0VkFgA4n0+YN9MOLwX1RZgaSdSuTIuDTZsSTq8dMAvuhwEsFpEpIiIAbgBwEMAuACudx6wE8LRZE8OlCqzZOcgAT9SGTDa/7uwQrLs1nF47YLATk6q+JCI7ALwC4CyAAoDHAXwSwHYR+Soq/wDutNHQMJXKo3hw+34AZoXJiKh9fC/g1MdaRtvsqeo6AOtqbj6NSi8+cpZcPh173zzp6dhRVazZWVl2zABPlHzXPLzH896oKxZfFnqcSPQK1Vp3dpvVmymVR7l6lagNrM0P4viHZzwfvzG30GJrvGmr4G4jMJuMwRFRPGw1KDMgEUnLa6vgbiMwm6x6JaLoW5sf9DwcAwDLr4lGRdq2Cu42AvPnr/AvJ5+IwpUvFI167ZM7JRJDMkCbBXfTcgQA8OSv32VaJFFC9e0eMuq1nxk1Odouo2yZuHFnr/t2D+HoSMnTizg6pli9feBj5yOiZEjSTmxtFdyBSkB2g3K+UMSqbQMtn2PMWdjkno+I4s/GO/IgN+OYSFsNy9TKZTOYOtnbME2pPGpc35mIwpcvFLFow3OeOnrVgt6MYyJtHdwB4OGveJ/8eO9UmePvRDHmFgYzrf6a6UoHvhnHRNo+uOeyGaO3UlzURBRfpoXBXHt7r49UYAcY3AGYleTkoiai+LLx97ticTTy2msxuKPSe/e8qIwlgYliy3TtS5Ty2msxuDuWe/zvqwr07OBeq0RxZLr93XfvuMpSS+xjcHdszC3Eksunezq2PMq9VoniqP8db1ViXVEbZ6/G4F5l633Xej52pMTMGaK4efKldz0fa7I3cxAY3C1i5gxRvIyqt3IBqQ4xHtLxG4N7DZO0yCQtXSZKurX5Qc/HRi2nvR4G9xqmK8yWP/GipZYQkV/yhSK2eKz++NjdiyIf2AEG9/PkshlMm+K99+51Gz8iCsba/KDnUgNxCewAg3td625dYFQamBOrRNG0Nj/oucfeIdHOjqnF4F5HLpvBI7d7X5jAgmJE0WSSHTMWnVLtTWFwbyCXzeCxuxd5Ova9U2ZFiIjIH16zY4Dopz7WYnAfh8lbMA7NEEWPyd7VUU99rMXg7pPV2wcY4IkiZPkTL3reQk9iNt4OMLhPyGve+5gCD/3Uex4tEdmzNj9olMlmMJoTGgb3CZjkvX90ZpS9d6IIMJlIBeI33g4YBncR6RKRHSLyuogcFJFrRWS6iOwRkUPO52m2GhuGXDZjVK/5gW0cniEKU75QNJpITXVGv9RAPaY99z8H8AtVvQLAVQAOAugF8LyqzgPwvPN9rG3MLfScOaPg8AxRWPKFotHeqJ0C9N0R/VID9XgO7iLyewD+EMAPAUBVz6jqCIDbAGx2HrYZQM6sidFg8uJyeIYoHCYdqw4Abz5ySywDO2DWc/8MgGEA/0NECiLyAxGZCuBiVT0GAM7nmfUOFpH7RaRfRPqHh4cNmhEckzQqZs8QBe+jM973R33U47v1qDAJ7pMAfA7AX6lqFsBHaGEIRlUfV9VuVe2eMWOGQTOCk055v1zMniEKTr5QxJX/5eeej582JRXbHrvLJLgfAXBEVV9yvt+BSrA/LiKzAMD5fMKsidFRKo8ZHc/hGSL/5QtF9OzYj1MGf68jCVhlPsnrgar69yLyrojMV9UhADcA+I3zsRLAJufz01ZaGgGzu9LGNdtXbRvAhmcOYN2tC2LfMyCKinyhiL7dQzg6UkKHiFF2DGC+cXYUmGbL/AcAW0XkVQCLAHwHlaB+o4gcAnCj830i9Cydb1Qt0vXeqTLW7BxkL57IgnyhiDU7B1EcKUFhVj8GiMcuS83w3HMHAFUdANBd564bTM4bVW5Pe8MzB4yLg5XKo1i/6wB770SG+nYPoVT2PnF63vlisMtSM7hCtUW5bAaFb95ktB2fi5tqE5k7anF7yzhtxjERBnePTLfjc3FTbSIztsbH582cmpjADjC4e2ZalsDFTbWJzNiaC9uz+jrzxkQIg7uBjbmFVgL8og3PcXiGyKNcNoNLpn3C6BxTJ5v/c4gaBndDG3Pet+NzjZSYPUPkVb5QxKETHxmd45TBStaoYnCPiFJ5lOPvRC0yLQzmSkJeey0GdwvmzZxq5Tw2Z/2J2sH6Xeab0adTnYnIa6/F4G6BrYmYJPYeiPySLxQxUjJbbzJtSgqP3L4wUVkyLqNFTPRPOi0seS6OlLBk0wvoWTo/kb9sRDbkC0Ws33XAKLB3oFL1Mcl/Z+y5W3LvNZdaOU9xpMTJVaIG3FIDpj32326Kb532ZjG4W7Ixt9Da2DsnV4nqs1FqII77oXrB4G7RntXXWcl7Bzi5SlSPjUV/SZw8rYfB3TIbee8AcKGF2jVESWJrqDLpwzEuBncf2Hjb99GZsxx3J6piY6jSRsG/uGBw94GNt33lUcWGZ8xzeImSwsaQjK2Cf3HA4O6DXDaDJZdPNz7Pe6dYEpgIANbmzfcfTsK+qK1gcPfJ1vuutVKMiL13amf5QhHZbz2HLfsOG58rCfuitoLB3UcPf2UhxPAc750qY8mmF9iDp7bj5rSb7nrmarcV4AzuPsplM1huqeY7FzZRu9nwzAFr2+cltX7MeBjcfWar5jsXNlE7yReK1nrsma50YuvHjIfBPQAbcwsxbYp5ClZxpMTeO7UFWx2ZTFcae3uvb7vADjC4B2bdrXZSsFZvG2CAp0TLF4pW0h7bcSimGoN7QGz1HMZgp4Y1URS5k6g2tONQTDUG9wDZKlhkWhGPKIryhSIe2D5gbRK1nQM7wOAeqJ6l861d8OVPvGjpTEThyxeK6HlqPwy3RDinXSo/jofBPUC5bAaP3r0I6ZT5Zd/75knM6X2WOfCUCH27h1AesxTZ0T6VH8djHGVEpFNECiLyM+f76SKyR0QOOZ+nmTczOXLZDA5++0vWChgxB56SwMYEarV2H5IB7PTcvw7gYNX3vQCeV9V5AJ53vqcaNgsYMQee4sx2x4RDMhVGwV1ELgFwC4AfVN18G4DNztebAeRMniOpctmMldx3Fzf3oLhas/NVa+cScEjGZdpzfwzAf0YlQ891saoeAwDn88x6B4rI/SLSLyL9w8PDhs2Ip3W3LjCuPeNqt7oZlAz5QhGl8tjED2yCAFi++DIOyTg8B3cR+TKAE6r6spfjVfVxVe1W1e4ZM2Z4bUas5bIZ2JpCOvEBV69S/PyppV57piuN79+9yNpOaEkwyeDYJQCWicjNAD4B4PdEZAuA4yIyS1WPicgsACdsNDSpOkUwaiH/qzwG9OzYD4CTSRQP+UIRpyz02h+7exF/5+vw3HNX1TWqeomqzgFwD4AXVHUFgF0AVjoPWwngaeNWJpiNwO4qjypXr1Ik5QtFLNn0AuZWpe+u2jZgfN4VHIZpyI88900AbhSRQwBudL6nBmzP7I+UylZ2rSGyxS0pUBwpQVFJe3zwqf3G552S6uAwzDisBHdV/ZWqftn5+neqeoOqznM+n7TxHEnVs3Q+0inzHZuqbdl3GIs2PMcxeIqEvt1D55UUGLWwYOk7t3/W+BxJZjLmTha4byn7dg/h6EgJIoCNhXojpTJ6nuIYPIXPjzTdyZ3C3+sJsPxABOSyGeztvR5vbboFj961yFp6ZHmMY/AUvikW9hKu9d07rrJ+zqRhzz1i3N6IjckmgBUkKXwfnbFT5dHFSdTmMLhHkO0AT5QEAuD7THtsGodlIiqXzVh5cS6YxJeYkoGBvTX8y4+wCy3Unjl9doypkRSaz677hZXzzJs5lYG9RQzuETZiaff3LfsOMy2SAvfZdb/AB6fNx9vnzZyKPauvM29Qm+GYe4TN7kpbq3P9gDN+z94P+SlfKKJv95C139sll0/H1vuutXKudsOee4TZXOCkqEzQZr/FxU1kX75QxKINz2HVtgGrG2+8/TuWsvaKPfcIq17gZOsP5r1T5XO7y7MXTza45QVsbWxdjfsUeCdqsXCVV93d3drf3x92MyJtTu+z1s85dXInTp0ZxeyuNHqWzmewJ0+WbHrB+jZ5rkxXGnt7r/fl3EkgIi+rane9+9hzb2Pu4hJ3H1aAvXlqnV+961SHcFclAxxzj4kll0/39fzch5W8yBeK6BBbBTNq+HTadsHgHhNb77vW9wDP8U1qRb5QRM9T+63uSVCtPKrscBhgcI+Rrfddi660vU21a3EfVmpWvlDEA9sGULZRwnQc7HB4x+AeM+uXLfDtRZvzaQZ3mpibHRNEKgY7HN4xuMdMLpvBo3cv8qUHv/fNkyxVQBOqt/mGH9KpTk6oGmBwj6FcNoOBdTdZ36IPqJQqYICn8fiV9rhi8WXIdKUhqKRAPnL7QmZvGWAqZIz1LJ3vy+KRLfsOc29KOk9lOOZVX879GCs+Wseee4zlshk8crs/QZglCqhavlDEqm0DKJXHrJ+bgd0fDO4xl8tm8Njdi6yflyloVM2vHvu0KSkGdp8wuCdALpuxPsFaHClhTu+zmNP7LBZtYLGxdrb8iRd96bELgHW3LrB+XqpgcE+I9csWWKsgWWukVEbPU/sZ4NvQ8idexN43T/pybgXLXfiJwT0h3PH3aRZ2b6qnPKa+vTWn6HFL+PoV2AH4ku1F/4TBPUFy2QwK37zJlzF4ACiVx7D8iRd9OTdFh1tWYKRkZyewelgUzH8M7gmUy2Z868HvffMk5vQ+iyWbXuAwTQLlC0U8uH2/r2UFRIC+O6/ikIzPPAd3EblURH4pIgdF5ICIfN25fbqI7BGRQ87nafaaS81ad+sCdHb4V1avOFLiOHzCrM0P4oFtA74VAgMqq06/fxdTH4NgsojpLIAHVfUVEfkUgJdFZA+AfwfgeVXdJCK9AHoBfMO8qdQK94/ngW0DvtUAKY8p1u86wD/UmHL3Oz06UsLkSR04fdZ+Rky1aVNSWHfrAv6+BMRzcFfVYwCOOV9/KCIHAWQA3AbgOudhmwH8CgzuoXD/iFY5m2P7wc9xWfKPO67uDr/4Hdi5UCl4VsbcRWQOgCyAlwBc7AR+9x/AzAbH3C8i/SLSPzw8bKMZVEcum8GKxZf5+hwcg4+fP935qu/lel0rFl/GwB4C4+AuIp8E8BMAq1T1g2aPU9XHVbVbVbtnzJhh2gwax8acfymSLo7Bx0e+UMQpHxYl1TNv5lTWKQqJUXAXkRQqgX2rqu50bj4uIrOc+2cBOGHWRLIhiJWA5THFA9sGGOAjzM2GCcKKxZdhz+rrAnkuOp9JtowA+CGAg6r6aNVduwCsdL5eCeBp780jW3LZDKZO9mcFazUFsGbnIAN8BLmbbPiZDeOa3CnssYfMpOe+BMC/AXC9iAw4HzcD2ATgRhE5BOBG53uKgIe/shCpTv93HS6VR7F+1wHfn4ealy8UsXr7QCCbbADAd++4KpDnocZMsmX+LxrvT36D1/OSf4LInnGNlMrIF4qcSIuAfKGInh37EdD8KbrSrPQYBVyh2maCyJ5xPbidE6xRsOGZAyiPBhTZUSliR+HjTkxtyB0L3brvsK+bHI+qouepyuQde3LBqV6cdGE6FehaBNZnjw7RACZXJtLd3a39/f1hN6Mt5QtFfOMnr/q+iMXFVYr+cbfB86P2ejPSqU7uexowEXlZVbvr3sfgTkBlIVLQGOjtqV1xGhR30m12Vxo9S+fztQzYeMGdwzIEAFhy+XRfa3fX896pMlZtG0D/OyeZNmeob/dQ4IGdPfVoY3AnAMDW+671dded8WzZdxjPvnoMI6fK7AF6kC8UURwpBf68DOzRxmEZqiuMYZpqHLJpjrswKaj8ddeKxZfx3VYEcFiGWpbqAEKalwNQGbJZs3MQADNtGnFLCQSx4rQWA3v0Mc+d6uq7c1HYTUCpPIq+3UNhNyNy1uYHMXfNs1jl88YajXDv03hgz53qymUz6H/nJLbsOxxqO8IYS44aN289Ctcinerk3qcxweBODW3MLcRbw/8YyiRrtTm9z6IrncL6Zckbg69ecDS7K43PXzEDv3x9+Nz3cz6dDv36CyoF4TKc7I4VTqjShNbmB0PvwbuSNJEX1mRoszipHX1cxERWRGV4IAlBJ8zJ0GZkutLY23t92M2gCTBbhqzIZTPIZTO48dFf4dCJj0JrR3UmDYCPDWvEYdggyLrqXh2NwPg+mWFwp5btWX0dPrvuF/jgdHjDCaXy6Hmli4sjpVikT/btHorsUIxrNjNiYo/DMuTZ3N5nfa0q6ZUI4P5aR2UitnriNIrXrBrLCsTHeMMyzHMnz5YHVBe+VdX9lZFSpX7N2vxg4wN8tjY/iFXbBlCMcGB3C4BlutIM7AnBnjsZCbtMQSumTu5EqrMD75ca17CpTU1sZQy/esK5UwSjqkinOkIrwduMDgEevWsRg3lMcUKVfJPpSoeePdOsj86MAqiMdRdHSujZsR/rdx3ASKl8Lhi7Od3uY6onbt3HAh/P2MkXih+7D8C5ydIoB/ZOAb7HwJ5Y7LmTkajnalN9kzoEb3zn5rCbQYY45k6+yWUzeOT2hehKp8JuCrXgnqsvDbsJ5DMGdzKWy2YwsO6mwDbeJnO/fH047CaQzxjcyZqNuYV47O5FrBoYA1yklHycUCWr3FWsQGU8fvX2AQS8+xs1gYuUko/BnXzjBvnalaQULpbtbQ8cliFf5bIZTraGZNqU1Lkhsk6pLFPiIqX24VvPXUS+CODPAXQC+IGqbvLruSja1i9bwHTJEIycKqPwzZvCbgaFxJeeu4h0AvgLAF8CcCWAe0XkSj+ei6LPTZfMdKUhqPQep07uDLtZicdx9fbmV8/9agBvqOpvAUBEfgzgNgC/8en5KOKqJ1oBLn7yG8fVya8x9wyAd6u+P+Lcdo6I3C8i/SLSPzzMnNt2U92bJ++kzm1d6RTH1cm3nnu937mPJcSp6uMAHgcq5Qd8agdFmNubZy++dalOQd8dVwGI32YlFAy/gvsRANXrmy8BcNSn56KYc4NRFLbwi4PabQYZzKkev4L73wGYJyJzARQB3APgj3x6LkoAtxcfpc24oybVIei78yoGc2qKL8FdVc+KyNcA7EYlFfJHqnrAj+eiZNmYWwgA+N8vHW77la3pVAc+kerEyKnG9eeJGmHJX4q0OG0GYkvtsAtRI9ysg2IrTpuBeNUhwJhWflb2zskWBneKtJ6l8xOdSdMhwG8fuSXsZlACsbYMRVq91a1LLp9eN9c2jv7oGtbAJ3+w506RV7u6Ffj4RtYXplMQqdRSuTCdwoenz2I04rOxnSK495pLz00gE9nG4E6xVC/gu6oD/+yuND5/xQz85OUjkdisOp3q5OpRCgSzZahtrM0PYuu+w7D1G9+VTmHqBZPO/ROZ8+k09r558mOP6QBw4ZQU0xnJF8yWIUJl31AvgV2cj+p+fzrVifXLzk9XrH3XwGBOYWFwp7Yx3r6hAnxs7L42MDcbtMcbLiIKEoM7tY3ZDXLmM11p7O29ftxjGbQpbpgKSW2jZ+l8pFMf3ySEdc8pqdhzp7ZRXX2SY+KUdAzu1FY4vELtgsMyREQJxOBORJRADO5ERAnE4E5ElEAM7kRECRSJ2jIiMgzgHY+HXwTgHyw2x6aoto3tag3b1Rq2qzUm7fpnqjqj3h2RCO4mRKS/UeGcsEW1bWxXa9iu1rBdrfGrXRyWISJKIAZ3IqIESkJwfzzsBowjqm1ju1rDdrWG7WqNL+2K/Zg7ERGdLwk9dyIiqsHgTkSUQLEI7iJyp4gcEJExEemuuW+NiLwhIkMisrTB8dNFZI+IHHI+T/OhjdtEZMD5eFtEBho87m0RGXQeF8jGsSKyXkSKVe27ucHjvuhcxzdEpDeAdvWJyOsi8qqI/FREuho8zvdrNtHPLhX/zbn/VRH5nB/tqPO8l4rIL0XkoPM38PU6j7lORN6ven2/GVDbxn1dwrhmIjK/6joMiMgHIrKq5jGBXC8R+ZGInBCR16puayoWWflbVNXIfwD45wDmA/gVgO6q268EsB/ABQDmAngTQGed478LoNf5uhfAn/nc3u8B+GaD+94GcFHA1289gP80wWM6nev3GQCTnet6pc/tugnAJOfrP2v0uvh9zZr52QHcDODnqOzItxjASwG9drMAfM75+lMA/l+dtl0H4GdB/k4187qEdc1qXte/R2WhT+DXC8AfAvgcgNeqbpswFtn6W4xFz11VD6rqUJ27bgPwY1U9rapvAXgDwNUNHrfZ+XozgJwvDUWltwLgLgBP+vUcPrkawBuq+ltVPQPgx6hcN9+o6nOqetb5dh+AS/x8vnE087PfBuB/asU+AF0iMsvvhqnqMVV9xfn6QwAHAcSlIH0o16zKDQDeVFWvq9+NqOrfAjhZc3MzscjK32Isgvs4MgDerfr+COr/4l+sqseAyh8LgJk+tulfAziuqoca3K8AnhORl0Xkfh/bUetrzlvjHzV4K9jstfTLH6PSy6vH72vWzM8e9vWBiMwBkAXwUp27rxWR/SLycxFZEFCTJnpdwr5m96BxJyuM6wU0F4usXLfI7MQkIv8HwO/XueshVX260WF1bvMtt7PJNt6L8XvtS1T1qIjMBLBHRF53/sP71jYAfwXg26hcm2+jMmz0x7WnqHOs8bVs5pqJyEMAzgLY2uA0vlyz6mbWua32Zw/0d62WiHwSwE8ArFLVD2rufgWVoYd/dOZT8gDmBdCsiV6X0K6ZiEwGsAzAmjp3h3W9mmXlukUmuKvqFzwcdgTApVXfXwLgaJ3HHReRWap6zHlbeMKPNorIJAC3A/iX45zjqPP5hIj8FJW3YMaBqtnrJyJPAPhZnbuavZZW2yUiKwF8GcAN6gw41jmHL9esSjM/uy/XpxkikkIlsG9V1Z2191cHe1X9GxH5SxG5SFV9LZLVxOsS2jUD8CUAr6jq8do7wrpejmZikZXrFvdhmV0A7hGRC0RkLir/fX/d4HErna9XAmj0TsDUFwC8rqpH6t0pIlNF5FPu16hMKL5W77E21YxzfqXBc/4dgHkiMtfp9dyDynXzs11fBPANAMtU9VSDxwRxzZr52XcB+LdOBshiAO+7b6/95Mzh/BDAQVV9tMFjft95HETkalT+rn/nc7uaeV1CuWaOhu+gw7heVZqJRXb+Fv2eMbbxgUpAOgLgNIDjAHZX3fcQKjPLQwC+VHX7D+Bk1gD4NIDnARxyPk/3qZ1/DeBPam6bDeBvnK8/g8rM934AB1AZmgji+v0vAIMAXnV+SWbVts35/mZUsjHeDKJtqEyAvwtgwPn472Fds3o/O4A/cV9PVN4q/4Vz/yCqsrZ8vkb/CpW35K9WXaeba9r2Nefa7EdlYvoPAmhX3dclItdsCirB+sKq2wK/Xqj8czkGoOzEr682ikV+/C2y/AARUQLFfViGiIjqYHAnIkogBnciogRicCciSiAGdyKiBGJwJyJKIAZ3IqIE+v9+VVAmuf+LvQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "611e9fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'left': {9.795444972970165: 0.6690551892265603,\n",
       "   -4.771837638494603: 9.72080142307727,\n",
       "   -3.3830514659412048: 2.4010921812145094,\n",
       "   -2.963360610525045: 1.0879230874230017,\n",
       "   -2.4379806442969247: 0.6295824604237676,\n",
       "   -1.6470094280315601: 0.40903658267006,\n",
       "   -1.1765545933840933: 0.0746623555252985,\n",
       "   6.854522576813596: -0.025393708489936825,\n",
       "   -1.0623920190247087: 0.0580845007492433,\n",
       "   6.532104401821169: -0.02174357564764925,\n",
       "   -9.052995572534304: 3.8761446447440626,\n",
       "   2.9969429135503955: -0.031472814662015874,\n",
       "   -5.259715369317674: -0.15765270582858482,\n",
       "   -0.8799146087473199: 0.08971252633485252,\n",
       "   3.0541573671494526: -0.020949822107024195,\n",
       "   -5.188044191730446: -0.1343704077953333,\n",
       "   -0.9071062306059224: 0.0840333366343771,\n",
       "   3.0496403271303727: -0.021339697693325377,\n",
       "   -5.121160334507198: -0.11713714294226822,\n",
       "   -0.9205189513560639: 0.0793441915484254,\n",
       "   3.038679419153772: -0.021526513265112828,\n",
       "   -5.056027539850599: -0.10390825856147315,\n",
       "   -0.9377653993331556: 0.07557085272081715,\n",
       "   3.022528080837247: -0.021530259025850682,\n",
       "   -5.023244510767492: -0.09441170474599622,\n",
       "   -0.9494354039148948: 0.07229049681473391,\n",
       "   3.0109559021535963: -0.021392005242645205,\n",
       "   -4.9805865933510685: -0.08646277856634767,\n",
       "   -0.9594665462204954: 0.06940489876326995,\n",
       "   2.9898906285206865: -0.02114704066555544,\n",
       "   -4.941271062698384: -0.08007687366329447,\n",
       "   -0.9642499203761202: 0.06682358252234973,\n",
       "   2.9712618818387937: -0.020847078273927417,\n",
       "   -4.917555191758623: -0.0750803575248069,\n",
       "   -0.9676558851381462: 0.06450013654439608,\n",
       "   2.9553730206650615: -0.020512780887486287,\n",
       "   -4.889449476460528: -0.07071000822842677,\n",
       "   -0.9726773208055223: 0.0624260061451533,\n",
       "   2.954334571596985: -0.02028697117291713,\n",
       "   -8.68667921816649: 1.2906355899558084,\n",
       "   2.7853677175574156: -0.022452477616546087,\n",
       "   -5.032589261064703: -0.11599046603623347,\n",
       "   -0.842706134296842: 0.06987767638574131,\n",
       "   2.837821240590402: -0.01684161754300954,\n",
       "   -4.970517631915725: -0.09906097176275021,\n",
       "   -0.8799180703235411: 0.06575290911963788,\n",
       "   2.8378230498460546: -0.017111474037430536,\n",
       "   -4.927935998719297: -0.08743703409197888,\n",
       "   -0.9015069203804271: 0.062258438054901034,\n",
       "   2.8334912888312145: -0.017233550683402564,\n",
       "   -4.893340397578191: -0.07833811636503189,\n",
       "   -0.9133252427577346: 0.05921647371144569,\n",
       "   2.829607719723181: -0.017245538830529806,\n",
       "   -4.844718473996343: -0.07066148407056982,\n",
       "   -0.9205190181500272: 0.056622059995651425,\n",
       "   2.8328895035246036: -0.01727407802926658,\n",
       "   -4.813121097388181: -0.06442598123430038,\n",
       "   -0.9351576073657502: 0.054480327935480806,\n",
       "   2.8210430300652183: -0.0171089925688483,\n",
       "   -8.276569177334409: 0.613489155958076,\n",
       "   2.7021923783400887: -0.018351269136805543,\n",
       "   -4.93057258665485: -0.09846351912024644,\n",
       "   -0.8181952029730583: 0.060214012332890914,\n",
       "   2.7451685509188186: -0.01453663280794757,\n",
       "   -4.889448902249701: -0.084390614816459,\n",
       "   -0.8485317927726133: 0.056480179566893295,\n",
       "   2.7487552212028743: -0.014803693172365993,\n",
       "   -4.838454813830064: -0.07397505468566817,\n",
       "   -0.8837331847052883: 0.053522685433508396,\n",
       "   2.7436266938303198: -0.014929389319162992,\n",
       "   -4.798887220637108: -0.0660729631364304,\n",
       "   -0.9017318263073812: 0.0509484078063725,\n",
       "   2.736765119162596: -0.014951304552927685,\n",
       "   -4.781837275744622: -0.06010087783798148,\n",
       "   -0.9110518858523541: 0.048674078396565436,\n",
       "   2.7341055489286523: -0.014872270699641631,\n",
       "   -8.081550768437548: 0.4274040739122888,\n",
       "   2.619051482931053: -0.015822052359494347,\n",
       "   -4.868173730538352: -0.08676793949173889,\n",
       "   -0.7946388865448122: 0.05323873634768903,\n",
       "   2.6827425001378016: -0.012938775213153886,\n",
       "   -4.810745823741373: -0.07347455515069524,\n",
       "   -0.8389383245953621: 0.0499952587058868,\n",
       "   2.6842448273321153: -0.01319109131428079,\n",
       "   -4.788979216893317: -0.06478924540501489,\n",
       "   -0.867966022225871: 0.047256117826440305,\n",
       "   2.6827428785783907: -0.013306706492914511,\n",
       "   -4.751828678387544: -0.05771761284482914,\n",
       "   -0.8870725926996498: 0.04494068258114189,\n",
       "   2.6803514791072067: -0.013320172595637099,\n",
       "   -7.883358161385201: 0.3083671368126562,\n",
       "   2.5672781865201277: -0.013794089216624991,\n",
       "   -4.818256417504245: -0.0789612500668621,\n",
       "   -0.7617620268805662: 0.04846616691493669,\n",
       "   2.618617458400615: -0.011736680453782965,\n",
       "   -4.785546224304543: -0.06730086146908042,\n",
       "   -0.828547111570793: 0.04553647067767264,\n",
       "   2.620223709807441: -0.011997232789422246,\n",
       "   -4.750456547407074: -0.058866261360699765,\n",
       "   -0.8485327221915545: 0.04293799942603712,\n",
       "   2.6190562587729893: -0.012145361747438792,\n",
       "   -4.716750708452145: -0.05226484884958496,\n",
       "   -0.8802561377944196: 0.04087054439879797,\n",
       "   2.617353794774083: -0.01217392464184484,\n",
       "   -7.773108805906299: 0.2613545949588639,\n",
       "   2.5337841505683687: -0.012842204413321893,\n",
       "   -4.789955363831669: -0.07191819977221296,\n",
       "   -0.7445720173407773: 0.044125448439703216,\n",
       "   2.567275512610213: -0.01078494613651408,\n",
       "   -4.747727899706412: -0.060756041888561874,\n",
       "   -0.8069052784752938: 0.04140582720787132,\n",
       "   2.579545770957585: -0.011014011743229241,\n",
       "   -4.714585730319471: -0.05308841375617491,\n",
       "   -0.8436162887021932: 0.03911951698962069,\n",
       "   2.568417699727467: -0.011160062738851624,\n",
       "   -7.6552999356197855: 0.20409614536074983,\n",
       "   2.514805702806481: -0.01102637577388852,\n",
       "   -4.768085274089783: -0.06819942112985058,\n",
       "   -0.7259543500062917: 0.04156178958334784,\n",
       "   2.5344635873996912: -0.009968635205480103,\n",
       "   -4.733913214111329: -0.05792941581306169,\n",
       "   -0.7952712811516842: 0.03891333438624267,\n",
       "   2.5365301555840634: -0.010216899647014195,\n",
       "   -4.685041276678348: -0.050183089437560564,\n",
       "   -0.8325536226266579: 0.036699499434011566,\n",
       "   2.536527331512551: -0.010362312009890884,\n",
       "   -7.593855670269091: 0.18006562923540786,\n",
       "   2.4777575442266544: -0.010189828102117538,\n",
       "   -4.747726733793249: -0.0638805515369515,\n",
       "   -0.7185523734031298: 0.03885283503608861,\n",
       "   2.508910864038098: -0.009340360717066608,\n",
       "   -4.704247458842732: -0.05385570495097876,\n",
       "   -0.7905026382407548: 0.03640620868947851,\n",
       "   2.5167823236409435: -0.009582199661616723,\n",
       "   -4.65748145826939: -0.04669008602639039,\n",
       "   -0.8300689737366017: 0.03435847667920389,\n",
       "   2.5167794117727853: -0.00970873862411435,\n",
       "   -7.525442675806323: 0.1619699602146994,\n",
       "   2.454728994483373: -0.009653778772987223,\n",
       "   -4.7285178403177595: -0.059550471908176354,\n",
       "   -0.6829573925778298: 0.03619881250037577,\n",
       "   2.487816472183446: -0.00878190452602988,\n",
       "   -4.673144062004802: -0.049996249561750486,\n",
       "   -0.7839040703652201: 0.03406524658065207,\n",
       "   2.4914342982462556: -0.009010641265979401,\n",
       "   -4.647347488182362: -0.04360020003386988,\n",
       "   -0.8285508068622812: 0.03216287587830785,\n",
       "   2.4914352725683115: -0.009139987373774909,\n",
       "   -7.4536143686567895: 0.1468517466532076,\n",
       "   2.4265107374364696: -0.009215539839511816,\n",
       "   -4.704246924433359: -0.055477157503131834,\n",
       "   -0.6591818517204363: 0.03378619344226264,\n",
       "   2.494263057499204: -0.008394413444958183,\n",
       "   -4.656307520029101: -0.0461329528667281,\n",
       "   -0.7645223078149521: 0.031847642830955765,\n",
       "   2.4987151813316526: -0.008594970858254778,\n",
       "   -7.373740365660984: 0.11541912317768567,\n",
       "   2.456515138275435: -0.007695254902651291,\n",
       "   -4.701061256340837: -0.05401872314062231,\n",
       "   -0.6344068533954611: 0.0327687778522419,\n",
       "   2.4740870581756176: -0.007949149871515954,\n",
       "   -4.654720597994903: -0.045584957220249106,\n",
       "   -0.7445721453476541: 0.03080605777498771,\n",
       "   2.486139105620958: -0.008167190447365403,\n",
       "   -4.623048058744037: -0.03950943975272047,\n",
       "   -0.8046953303197427: 0.029114708004958537,\n",
       "   2.4861378898325976: -0.008294836271895964,\n",
       "   -7.334334363599399: 0.11985216043087574,\n",
       "   2.425305473153036: -0.008125686270431167,\n",
       "   -4.665745703681049: -0.049481247820498994,\n",
       "   -0.6344064402549047: 0.03045794418309512,\n",
       "   2.461686582687936: -0.0076036972030159205,\n",
       "   -4.633909080893344: -0.041555676149955756,\n",
       "   -0.7445710052256855: 0.028702687178304667,\n",
       "   2.4740845472625157: -0.007783960283733266,\n",
       "   -7.299849326608729: 0.09921265228986904,\n",
       "   2.440592683664746: -0.006935904609458837,\n",
       "   -4.658377962534653: -0.04822186945096209,\n",
       "   -0.6156511454080058: 0.029532464457822157,\n",
       "   2.4547298549991594: -0.007212470099159457,\n",
       "   -4.633903345134622: -0.04095417333126329,\n",
       "   -0.7351712965484912: 0.027782639166087926,\n",
       "   2.4558131822518687: -0.007413797470618849,\n",
       "   -7.232028094059698: 0.08751713684943176,\n",
       "   2.454730451773963: -0.006485902079799109,\n",
       "   -4.657034725688753: -0.04626274690254955,\n",
       "   -0.6104982754286513: 0.028413010938347267,\n",
       "   2.4579116791832694: -0.006996798953165614,\n",
       "   -4.623048069801969: -0.03915864323459632,\n",
       "   -0.7136612523402295: 0.026663160688697937,\n",
       "   2.461689541922939: -0.007188119237649123,\n",
       "   -7.200448461700445: 0.08201048765856013,\n",
       "   2.4405921690860537: -0.006097636367503167,\n",
       "   -4.651641627228528: -0.04454367585994259,\n",
       "   -0.6021072941875025: 0.027275638662478372,\n",
       "   2.4547304230871556: -0.0067097989257091,\n",
       "   -4.6162355801476975: -0.03762596910266261,\n",
       "   -0.6405138513817041: 0.025322786697275373,\n",
       "   2.4558093678976887: -0.006923649790132644,\n",
       "   -7.1897400036778665: 0.07840737083046415,\n",
       "   2.433461225645685: -0.005896581581426688,\n",
       "   -4.647348160049394: -0.04253018626184671,\n",
       "   -0.5989797564247713: 0.026164386315603347,\n",
       "   2.45325266984154: -0.006494020741110151,\n",
       "   -4.610014045570228: -0.03590947396869203,\n",
       "   -0.6391000580104629: 0.02431284659384563,\n",
       "   2.4552547185279456: -0.006695988157388681,\n",
       "   -7.172873417893252: 0.07469208070800881,\n",
       "   2.4265099475128706: -0.0056409190667974325,\n",
       "   -4.638194246339184: -0.040703154386414656,\n",
       "   -0.5977787716387918: 0.025122807909713718,\n",
       "   2.4334626207485566: -0.006266719810648862,\n",
       "   -4.603203566634696: -0.034390259206213636,\n",
       "   -0.6378799099185432: 0.02335169648175919,\n",
       "   2.4442316604476715: -0.006452509495895652,\n",
       "   -7.153501186687854: 0.07091421379386545,\n",
       "   2.4265069779039288: -0.005445415416569555,\n",
       "   -4.62776271069098: -0.03885488090397154,\n",
       "   -0.5966317448476061: 0.02413461188825164,\n",
       "   2.43070337663612: -0.006061351830623532,\n",
       "   -4.601705120226859: -0.03292607061867011,\n",
       "   -0.6361670686175093: 0.022439081636579653,\n",
       "   2.440593777443703: -0.0062366783230150625,\n",
       "   -7.109331742507037: 0.06662784048835357,\n",
       "   2.430702590004087: -0.005303505933722103,\n",
       "   -4.619389516802575: -0.03704741309003766,\n",
       "   -0.5918603400322756: 0.023206233476840993,\n",
       "   2.4442320205565884: -0.005887965242272382,\n",
       "   -4.600374022960148: -0.03148987988219441,\n",
       "   -0.6261786099060233: 0.021573136730151002,\n",
       "   2.4532578268613956: -0.006055859299939096,\n",
       "   -7.0529399122621275: 0.06253788523153324,\n",
       "   2.4253048316702763: -0.005074099339171078,\n",
       "   -4.613054666755856: -0.035722600355928204,\n",
       "   -0.5871743686096419: 0.02232553322386693,\n",
       "   2.429651011618599: -0.005651402055346208,\n",
       "   -4.591072994208065: -0.030314266504459016,\n",
       "   -0.6174032738460404: 0.02073149819030266,\n",
       "   2.440592279394649: -0.005818972621560224,\n",
       "   -7.022768296595613: 0.05950122707780951,\n",
       "   2.425079212968649: -0.004939328444608838,\n",
       "   -4.610011848667649: -0.03434755497305718,\n",
       "   -0.5850530986508687: 0.021483906480959873,\n",
       "   2.428960258976872: -0.005445449130019157,\n",
       "   -4.584676498201824: -0.029126342000303812,\n",
       "   -0.6107656281093266: 0.019939101865306036,\n",
       "   2.4307040332396035: -0.005612189566600766,\n",
       "   -6.993403567861184: 0.05684851003979407,\n",
       "   2.410742503639121: -0.004789245233670114,\n",
       "   -4.602735244364105: -0.03309378631129101,\n",
       "   -0.5762336263759005: 0.02066946265758364,\n",
       "   2.425081568720087: -0.005232095950678384,\n",
       "   -4.5808342965631415: -0.02807359116639879,\n",
       "   -0.6094412989892553: 0.01919112963854777,\n",
       "   2.426509052927383: -0.005390396773953397,\n",
       "   -6.940359569446853: 0.05370439953075048,\n",
       "   2.424991210593138: -0.0047262896078913,\n",
       "   -4.600753971571523: -0.03176692263748667,\n",
       "   -0.5761659632705616: 0.01992670845402517,\n",
       "   2.4268618126335424: -0.005074276894664042,\n",
       "   -4.567933621044884: -0.026886495444017177,\n",
       "   -0.5977764795897835: 0.018480096933982972,\n",
       "   2.429649957726923: -0.005224574113053128,\n",
       "   -6.915573554348061: 0.051731218576882815,\n",
       "   2.4159488013644976: -0.004577843847527259,\n",
       "   -4.597901326206696: -0.03073464507070134,\n",
       "   -0.5558256375274763: 0.019175734749151446,\n",
       "   2.4265053280087368: -0.004885218686763477,\n",
       "   -4.561796212150373: -0.025945744063295577,\n",
       "   -0.5965632462645606: 0.01782608150619072,\n",
       "   2.4268635874454425: -0.005030392834546281,\n",
       "   -6.893549406231068: 0.04973663223506484,\n",
       "   2.403756021676578: -0.004421716024977503,\n",
       "   -4.591071713514407: -0.029790857246521266,\n",
       "   -0.5474828930632816: 0.01848032727677309,\n",
       "   2.4107398718021273: -0.0046793227023276375,\n",
       "   -4.556948895333117: -0.025127173878010296,\n",
       "   -0.5873094329749399: 0.017155489431699214,\n",
       "   2.415944857421485: -0.004816740473261396,\n",
       "   -6.8576637676103305: 0.04754894479427608,\n",
       "   2.378908640223161: -0.0043091870831812465,\n",
       "   -4.5831420439914075: -0.028858040824202725,\n",
       "   -0.5279568157576259: 0.017754844401067713,\n",
       "   2.396443315129701: -0.004470637506769704,\n",
       "   -4.553830123118177: -0.024316483354087853,\n",
       "   -0.5871702381571984: 0.01650930700826301,\n",
       "   2.403291722239908: -0.004605720442655641,\n",
       "   -6.825658975946932: 0.045764871065182476,\n",
       "   2.3789101997015654: -0.004290358339291924,\n",
       "   -4.570788154129777: -0.027684372210976693,\n",
       "   -0.5241966301154013: 0.017115023887687198,\n",
       "   2.3995854393698717: -0.004327103689897989,\n",
       "   -4.5518589184622: -0.023400460168204986,\n",
       "   -0.5850527410308859: 0.015921397859055993,\n",
       "   2.4037287674872077: -0.004456670447246036,\n",
       "   -6.817281586900774: 0.04464129473538773},\n",
       "  'right': {2.4608534822808554: 10.619573294279029,\n",
       "   6.8186206894942565: 5.175046011789663,\n",
       "   -6.008002185454937: -0.24490966071681547,\n",
       "   6.337590801579631: 1.3838284943860812,\n",
       "   -9.803162026430938: -0.07338925551896915,\n",
       "   5.591986986485101: 0.43213236213480727,\n",
       "   -5.25884813216035: -0.0423409357410646,\n",
       "   5.179788895691087: 0.17627864354476602,\n",
       "   -5.093667069134106: -0.02523565900603858,\n",
       "   4.91224932470926: 0.09857707260134109,\n",
       "   -0.008535523707174586: 0.030180442773733746,\n",
       "   6.704940522212597: -0.16615591799488685,\n",
       "   -0.08511023388981166: 0.02598068192846126,\n",
       "   3.0647757642724374: -0.04402817404113297,\n",
       "   -0.0851062728239647: 0.024801956436135548,\n",
       "   3.0107714422506224: -0.04128411124550633,\n",
       "   -0.03528385143503869: 0.02330273040375449,\n",
       "   2.971055261274739: -0.038265099726572806,\n",
       "   -0.06504130969430079: 0.022406587505144113,\n",
       "   2.9251296826044584: -0.03660307909866703,\n",
       "   -0.08510752994482136: 0.021450288373437584,\n",
       "   2.8835382099990383: -0.03486890353555162,\n",
       "   -0.03760694488759276: 0.02026690758239654,\n",
       "   2.835508299066974: -0.03246319053419837,\n",
       "   -0.11612619841068686: 0.01972284166320841,\n",
       "   2.8047106613310104: -0.03174404132754218,\n",
       "   -0.06273195851439352: 0.018710602916257292,\n",
       "   2.7646873370689806: -0.029692168827006944,\n",
       "   -0.03156126391901363: 0.01775253249680419,\n",
       "   2.7451719383636037: -0.027964415843165454,\n",
       "   -0.07170119436840462: 0.01729595294895495,\n",
       "   2.731367763914039: -0.02732476795441505,\n",
       "   -0.03527930516483874: 0.01644385658928056,\n",
       "   2.6931440637984747: -0.025669660400438974,\n",
       "   -0.07907455164351423: 0.016019029055873244,\n",
       "   2.677725552614349: -0.025085808518030155,\n",
       "   -0.03760706379264568: 0.015268223215667927,\n",
       "   2.6511544307066157: -0.023662365955360403,\n",
       "   -0.012302621517835437: 0.014544977527229021,\n",
       "   2.6186161844265192: -0.02232890058827582,\n",
       "   -0.03445838598840831: 0.014148728331379057,\n",
       "   2.6020321516615676: -0.02171869741539247,\n",
       "   -0.05137550424653544: 0.013752604397231917,\n",
       "   2.5645304846241297: -0.021010946644139415,\n",
       "   -0.02768061448414204: 0.013131186304692557,\n",
       "   2.547673767394393: -0.019939562377873227,\n",
       "   -0.04325913144009152: 0.01278941803180529,\n",
       "   2.5365299502899656: -0.019421760756266036,\n",
       "   -0.01360277766363558: 0.012242682094929482,\n",
       "   2.5302263932804454: -0.01849827976227099,\n",
       "   -0.03527651764628751: 0.011922775838260603,\n",
       "   9.326305715588264: 3.195944545686031,\n",
       "   2.6186143224101914: -0.05711814175498166,\n",
       "   -0.11906926071766895: 0.013438451281919391,\n",
       "   2.592507663599092: -0.020768254650200248,\n",
       "   -0.07170195344150541: 0.012845204281231922,\n",
       "   2.5619240345064176: -0.01961770553172281,\n",
       "   -0.035279505375038704: 0.012289352129672549,\n",
       "   2.5469328236886057: -0.0186271864849432,\n",
       "   -0.010373874002837367: 0.011752451831298692,\n",
       "   2.535678673748051: -0.017717088524101345,\n",
       "   -0.007377511575073653: 0.011345455374530276,\n",
       "   2.5265834193912657: -0.01706836372424607,\n",
       "   0.04854268314075675: 0.010915389693285142,\n",
       "   2.5089086985524145: -0.016244293891654178,\n",
       "   -0.00853942325789335: 0.010637506631684609,\n",
       "   2.4979456198977354: -0.015919611647531737,\n",
       "   0.048541136182055265: 0.010243957679613638,\n",
       "   5.424493682445869: 0.03600961602287975,\n",
       "   2.5066507758041014: -0.0313806814780732,\n",
       "   0.04854155558205054: 0.010331350384247106,\n",
       "   5.37332490019363: 0.032296544199009534,\n",
       "   2.508912733505555: -0.0301365004218912,\n",
       "   0.05244696857402681: 0.010361469022695866,\n",
       "   2.4987132592444095: -0.015388596313702126,\n",
       "   0.07301897981043189: 0.010050471558622383,\n",
       "   5.345391701109835: 0.03197061828529802,\n",
       "   2.506368452283176: -0.02965651341716194,\n",
       "   0.07539192058264832: 0.01009017435002275,\n",
       "   2.4914339071637848: -0.014919297659531323,\n",
       "   0.0730196882501892: 0.009803372620333813,\n",
       "   8.782602668641331: 0.39796816653315825,\n",
       "   2.520109800463417: -0.029998776608795463,\n",
       "   0.036926666720550456: 0.010204863688204847,\n",
       "   2.506364815689773: -0.015202555556262024,\n",
       "   0.07914103400683548: 0.00981389959590608,\n",
       "   2.491432822313486: -0.014498503977362,\n",
       "   0.1121878326048951: 0.009442917979050097,\n",
       "   5.240953058814349: 0.027343476528255736,\n",
       "   2.498712941155925: -0.027009895360043796,\n",
       "   0.11224646899441541: 0.009445768026443207,\n",
       "   2.48613731542357: -0.013880264340986567,\n",
       "   0.11218633959295808: 0.00918983566864621,\n",
       "   5.228079252426755: 0.027394380897508393,\n",
       "   2.490658374830727: -0.026682865439046032,\n",
       "   0.11224169512327807: 0.009210567010065318,\n",
       "   2.4695053636022957: -0.013490559212384044,\n",
       "   0.11218725406997487: 0.008957054070494934,\n",
       "   5.208617563157778: 0.027258896108234613,\n",
       "   2.486139738796516: -0.026346116233027506,\n",
       "   0.1122430160456356: 0.008986581223857934,\n",
       "   5.171961653430224: 0.025161575487608282,\n",
       "   2.490656905890448: -0.025556508346334588,\n",
       "   0.11279690322027494: 0.008981781010963056,\n",
       "   2.4663899457011267: -0.013149859168182856,\n",
       "   0.11279187589453321: 0.00873120795562725,\n",
       "   5.155085283303443: 0.025345942851230138,\n",
       "   2.4815218583689553: -0.02529602931405613,\n",
       "   0.11652574697794732: 0.008744558150593317,\n",
       "   2.4579110781138542: -0.012777990150900203,\n",
       "   0.11279493954256237: 0.008495287116528442,\n",
       "   5.137397386424955: 0.02537999252090538,\n",
       "   2.4663880760419503: -0.024969008768000928,\n",
       "   0.11652586443751475: 0.008520935640668881,\n",
       "   5.1111793144217454: 0.023599116165129785,\n",
       "   2.4777541716472156: -0.024284317944635533,\n",
       "   0.12488693270380491: 0.008522887971614511,\n",
       "   2.4565155547066713: -0.012437066954415413,\n",
       "   0.12401400783098282: 0.008282038012018662,\n",
       "   5.1095596728946004: 0.02391963796492508,\n",
       "   2.4616869816256672: -0.024011193025424375,\n",
       "   0.1272737023842411: 0.008298280194589733,\n",
       "   2.4552527268193343: -0.012104304545612754,\n",
       "   0.17491288616657233: 0.008012781938712117,\n",
       "   5.102692228605684: 0.024274801624509082,\n",
       "   2.4565174151065623: -0.023718426345725307,\n",
       "   0.18320600748704252: 0.008063471592171148,\n",
       "   5.085908990322544: 0.02257762282571,\n",
       "   2.4616842673170227: -0.023021320523330006,\n",
       "   0.1852720730534228: 0.00807900666893524,\n",
       "   2.4552529100151936: -0.01169342025467146,\n",
       "   0.1832092414488392: 0.007860426976987803,\n",
       "   5.059696203163614: 0.02255585650083656,\n",
       "   2.4565148848815817: -0.022795453511701733,\n",
       "   0.18527038379688077: 0.007883779799810347,\n",
       "   2.449118828369666: -0.011397605560760318,\n",
       "   0.1844347852209536: 0.007668558086110909,\n",
       "   5.052498627914605: 0.022599098181295525,\n",
       "   2.455277444792852: -0.022560509023889508,\n",
       "   0.18527165865640824: 0.007697520970438255,\n",
       "   5.0281087570071685: 0.02107970554255136,\n",
       "   2.4565156045676617: -0.021924566548000114,\n",
       "   0.1932929434301503: 0.007708519043236471,\n",
       "   2.449116409541321: -0.01113407052019923,\n",
       "   0.19166961909703267: 0.007495743239338528,\n",
       "   5.010913352205418: 0.02121792656587981,\n",
       "   2.4552530662860166: -0.02175245048015175,\n",
       "   0.1934180657807905: 0.007509654348249505,\n",
       "   2.4405922446739052: -0.01083027943164604,\n",
       "   0.19329741602611963: 0.0073035827838282144,\n",
       "   5.003226642398812: 0.02132696274388406,\n",
       "   2.4491140991789924: -0.021529769580370398,\n",
       "   0.1934211202528611: 0.007324436348368132,\n",
       "   4.997794415592874: 0.02007000596172143,\n",
       "   2.4547298160098356: -0.02095566637322628,\n",
       "   0.1952047607566762: 0.0073268883782035025,\n",
       "   2.440593039603998: -0.010565059612230632,\n",
       "   0.22487281716869478: 0.0070688683297587766,\n",
       "   4.969951259888142: 0.020348318621046888,\n",
       "   2.4491126862359773: -0.020761291351938466,\n",
       "   0.22621689474379195: 0.007090210539388431,\n",
       "   4.955788084993924: 0.019138494828056723,\n",
       "   2.4547284806321663: -0.020231370397808708,\n",
       "   0.20090054247887873: 0.00715286907627524,\n",
       "   2.4361653045625964: -0.010298424626528692,\n",
       "   0.22856644534977755: 0.006899297020027751,\n",
       "   4.944504513772752: 0.019346358126772545,\n",
       "   2.444234190997098: -0.020058901566109105,\n",
       "   0.23076983935074058: 0.006914713826433981,\n",
       "   2.4296492107579666: -0.009904037343877256,\n",
       "   0.2285689186158067: 0.0067295938017536945,\n",
       "   7.1177325071847415: -0.04952829183913221,\n",
       "   0.24462222872270198: 0.006116988067244347,\n",
       "   5.002425431155023: 0.025100789701632185,\n",
       "   2.4107414434104473: -0.02164846818127276,\n",
       "   0.23952156228605206: 0.006328799703884813,\n",
       "   4.969409760882433: 0.022843942195437048,\n",
       "   2.4234813264700645: -0.020868010343784778,\n",
       "   0.2285677173194864: 0.00640813456293673,\n",
       "   4.944520814295516: 0.021054892840426033,\n",
       "   2.426510454797313: -0.02017557146543814,\n",
       "   0.22856946318543375: 0.006468092021754691,\n",
       "   7.086114489919715: -0.04519536468161776}}}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "402a7adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6690551892265603\n",
      "9.72080142307727\n",
      "2.4010921812145094\n",
      "1.0879230874230017\n",
      "0.6295824604237676\n",
      "0.40903658267006\n",
      "0.0746623555252985\n",
      "-0.025393708489936825\n",
      "0.0580845007492433\n",
      "-0.02174357564764925\n",
      "3.8761446447440626\n",
      "-0.031472814662015874\n",
      "-0.15765270582858482\n",
      "0.08971252633485252\n",
      "-0.020949822107024195\n",
      "-0.1343704077953333\n",
      "0.0840333366343771\n",
      "-0.021339697693325377\n",
      "-0.11713714294226822\n",
      "0.0793441915484254\n",
      "-0.021526513265112828\n",
      "-0.10390825856147315\n",
      "0.07557085272081715\n",
      "-0.021530259025850682\n",
      "-0.09441170474599622\n",
      "0.07229049681473391\n",
      "-0.021392005242645205\n",
      "-0.08646277856634767\n",
      "0.06940489876326995\n",
      "-0.02114704066555544\n",
      "-0.08007687366329447\n",
      "0.06682358252234973\n",
      "-0.020847078273927417\n",
      "-0.0750803575248069\n",
      "0.06450013654439608\n",
      "-0.020512780887486287\n",
      "-0.07071000822842677\n",
      "0.0624260061451533\n",
      "-0.02028697117291713\n",
      "1.2906355899558084\n",
      "-0.022452477616546087\n",
      "-0.11599046603623347\n",
      "0.06987767638574131\n",
      "-0.01684161754300954\n",
      "-0.09906097176275021\n",
      "0.06575290911963788\n",
      "-0.017111474037430536\n",
      "-0.08743703409197888\n",
      "0.062258438054901034\n",
      "-0.017233550683402564\n",
      "-0.07833811636503189\n",
      "0.05921647371144569\n",
      "-0.017245538830529806\n",
      "-0.07066148407056982\n",
      "0.056622059995651425\n",
      "-0.01727407802926658\n",
      "-0.06442598123430038\n",
      "0.054480327935480806\n",
      "-0.0171089925688483\n",
      "0.613489155958076\n",
      "-0.018351269136805543\n",
      "-0.09846351912024644\n",
      "0.060214012332890914\n",
      "-0.01453663280794757\n",
      "-0.084390614816459\n",
      "0.056480179566893295\n",
      "-0.014803693172365993\n",
      "-0.07397505468566817\n",
      "0.053522685433508396\n",
      "-0.014929389319162992\n",
      "-0.0660729631364304\n",
      "0.0509484078063725\n",
      "-0.014951304552927685\n",
      "-0.06010087783798148\n",
      "0.048674078396565436\n",
      "-0.014872270699641631\n",
      "0.4274040739122888\n",
      "-0.015822052359494347\n",
      "-0.08676793949173889\n",
      "0.05323873634768903\n",
      "-0.012938775213153886\n",
      "-0.07347455515069524\n",
      "0.0499952587058868\n",
      "-0.01319109131428079\n",
      "-0.06478924540501489\n",
      "0.047256117826440305\n",
      "-0.013306706492914511\n",
      "-0.05771761284482914\n",
      "0.04494068258114189\n",
      "-0.013320172595637099\n",
      "0.3083671368126562\n",
      "-0.013794089216624991\n",
      "-0.0789612500668621\n",
      "0.04846616691493669\n",
      "-0.011736680453782965\n",
      "-0.06730086146908042\n",
      "0.04553647067767264\n",
      "-0.011997232789422246\n",
      "-0.058866261360699765\n",
      "0.04293799942603712\n",
      "-0.012145361747438792\n",
      "-0.05226484884958496\n",
      "0.04087054439879797\n",
      "-0.01217392464184484\n",
      "0.2613545949588639\n",
      "-0.012842204413321893\n",
      "-0.07191819977221296\n",
      "0.044125448439703216\n",
      "-0.01078494613651408\n",
      "-0.060756041888561874\n",
      "0.04140582720787132\n",
      "-0.011014011743229241\n",
      "-0.05308841375617491\n",
      "0.03911951698962069\n",
      "-0.011160062738851624\n",
      "0.20409614536074983\n",
      "-0.01102637577388852\n",
      "-0.06819942112985058\n",
      "0.04156178958334784\n",
      "-0.009968635205480103\n",
      "-0.05792941581306169\n",
      "0.03891333438624267\n",
      "-0.010216899647014195\n",
      "-0.050183089437560564\n",
      "0.036699499434011566\n",
      "-0.010362312009890884\n",
      "0.18006562923540786\n",
      "-0.010189828102117538\n",
      "-0.0638805515369515\n",
      "0.03885283503608861\n",
      "-0.009340360717066608\n",
      "-0.05385570495097876\n",
      "0.03640620868947851\n",
      "-0.009582199661616723\n",
      "-0.04669008602639039\n",
      "0.03435847667920389\n",
      "-0.00970873862411435\n",
      "0.1619699602146994\n",
      "-0.009653778772987223\n",
      "-0.059550471908176354\n",
      "0.03619881250037577\n",
      "-0.00878190452602988\n",
      "-0.049996249561750486\n",
      "0.03406524658065207\n",
      "-0.009010641265979401\n",
      "-0.04360020003386988\n",
      "0.03216287587830785\n",
      "-0.009139987373774909\n",
      "0.1468517466532076\n",
      "-0.009215539839511816\n",
      "-0.055477157503131834\n",
      "0.03378619344226264\n",
      "-0.008394413444958183\n",
      "-0.0461329528667281\n",
      "0.031847642830955765\n",
      "-0.008594970858254778\n",
      "0.11541912317768567\n",
      "-0.007695254902651291\n",
      "-0.05401872314062231\n",
      "0.0327687778522419\n",
      "-0.007949149871515954\n",
      "-0.045584957220249106\n",
      "0.03080605777498771\n",
      "-0.008167190447365403\n",
      "-0.03950943975272047\n",
      "0.029114708004958537\n",
      "-0.008294836271895964\n",
      "0.11985216043087574\n",
      "-0.008125686270431167\n",
      "-0.049481247820498994\n",
      "0.03045794418309512\n",
      "-0.0076036972030159205\n",
      "-0.041555676149955756\n",
      "0.028702687178304667\n",
      "-0.007783960283733266\n",
      "0.09921265228986904\n",
      "-0.006935904609458837\n",
      "-0.04822186945096209\n",
      "0.029532464457822157\n",
      "-0.007212470099159457\n",
      "-0.04095417333126329\n",
      "0.027782639166087926\n",
      "-0.007413797470618849\n",
      "0.08751713684943176\n",
      "-0.006485902079799109\n",
      "-0.04626274690254955\n",
      "0.028413010938347267\n",
      "-0.006996798953165614\n",
      "-0.03915864323459632\n",
      "0.026663160688697937\n",
      "-0.007188119237649123\n",
      "0.08201048765856013\n",
      "-0.006097636367503167\n",
      "-0.04454367585994259\n",
      "0.027275638662478372\n",
      "-0.0067097989257091\n",
      "-0.03762596910266261\n",
      "0.025322786697275373\n",
      "-0.006923649790132644\n",
      "0.07840737083046415\n",
      "-0.005896581581426688\n",
      "-0.04253018626184671\n",
      "0.026164386315603347\n",
      "-0.006494020741110151\n",
      "-0.03590947396869203\n",
      "0.02431284659384563\n",
      "-0.006695988157388681\n",
      "0.07469208070800881\n",
      "-0.0056409190667974325\n",
      "-0.040703154386414656\n",
      "0.025122807909713718\n",
      "-0.006266719810648862\n",
      "-0.034390259206213636\n",
      "0.02335169648175919\n",
      "-0.006452509495895652\n",
      "0.07091421379386545\n",
      "-0.005445415416569555\n",
      "-0.03885488090397154\n",
      "0.02413461188825164\n",
      "-0.006061351830623532\n",
      "-0.03292607061867011\n",
      "0.022439081636579653\n",
      "-0.0062366783230150625\n",
      "0.06662784048835357\n",
      "-0.005303505933722103\n",
      "-0.03704741309003766\n",
      "0.023206233476840993\n",
      "-0.005887965242272382\n",
      "-0.03148987988219441\n",
      "0.021573136730151002\n",
      "-0.006055859299939096\n",
      "0.06253788523153324\n",
      "-0.005074099339171078\n",
      "-0.035722600355928204\n",
      "0.02232553322386693\n",
      "-0.005651402055346208\n",
      "-0.030314266504459016\n",
      "0.02073149819030266\n",
      "-0.005818972621560224\n",
      "0.05950122707780951\n",
      "-0.004939328444608838\n",
      "-0.03434755497305718\n",
      "0.021483906480959873\n",
      "-0.005445449130019157\n",
      "-0.029126342000303812\n",
      "0.019939101865306036\n",
      "-0.005612189566600766\n",
      "0.05684851003979407\n",
      "-0.004789245233670114\n",
      "-0.03309378631129101\n",
      "0.02066946265758364\n",
      "-0.005232095950678384\n",
      "-0.02807359116639879\n",
      "0.01919112963854777\n",
      "-0.005390396773953397\n",
      "0.05370439953075048\n",
      "-0.0047262896078913\n",
      "-0.03176692263748667\n",
      "0.01992670845402517\n",
      "-0.005074276894664042\n",
      "-0.026886495444017177\n",
      "0.018480096933982972\n",
      "-0.005224574113053128\n",
      "0.051731218576882815\n",
      "-0.004577843847527259\n",
      "-0.03073464507070134\n",
      "0.019175734749151446\n",
      "-0.004885218686763477\n",
      "-0.025945744063295577\n",
      "0.01782608150619072\n",
      "-0.005030392834546281\n",
      "0.04973663223506484\n",
      "-0.004421716024977503\n",
      "-0.029790857246521266\n",
      "0.01848032727677309\n",
      "-0.0046793227023276375\n",
      "-0.025127173878010296\n",
      "0.017155489431699214\n",
      "-0.004816740473261396\n",
      "0.04754894479427608\n",
      "-0.0043091870831812465\n",
      "-0.028858040824202725\n",
      "0.017754844401067713\n",
      "-0.004470637506769704\n",
      "-0.024316483354087853\n",
      "0.01650930700826301\n",
      "-0.004605720442655641\n",
      "0.045764871065182476\n",
      "-0.004290358339291924\n",
      "-0.027684372210976693\n",
      "0.017115023887687198\n",
      "-0.004327103689897989\n",
      "-0.023400460168204986\n",
      "0.015921397859055993\n",
      "-0.004456670447246036\n",
      "0.04464129473538773\n",
      "10.619573294279029\n",
      "5.175046011789663\n",
      "-0.24490966071681547\n",
      "1.3838284943860812\n",
      "-0.07338925551896915\n",
      "0.43213236213480727\n",
      "-0.0423409357410646\n",
      "0.17627864354476602\n",
      "-0.02523565900603858\n",
      "0.09857707260134109\n",
      "0.030180442773733746\n",
      "-0.16615591799488685\n",
      "0.02598068192846126\n",
      "-0.04402817404113297\n",
      "0.024801956436135548\n",
      "-0.04128411124550633\n",
      "0.02330273040375449\n",
      "-0.038265099726572806\n",
      "0.022406587505144113\n",
      "-0.03660307909866703\n",
      "0.021450288373437584\n",
      "-0.03486890353555162\n",
      "0.02026690758239654\n",
      "-0.03246319053419837\n",
      "0.01972284166320841\n",
      "-0.03174404132754218\n",
      "0.018710602916257292\n",
      "-0.029692168827006944\n",
      "0.01775253249680419\n",
      "-0.027964415843165454\n",
      "0.01729595294895495\n",
      "-0.02732476795441505\n",
      "0.01644385658928056\n",
      "-0.025669660400438974\n",
      "0.016019029055873244\n",
      "-0.025085808518030155\n",
      "0.015268223215667927\n",
      "-0.023662365955360403\n",
      "0.014544977527229021\n",
      "-0.02232890058827582\n",
      "0.014148728331379057\n",
      "-0.02171869741539247\n",
      "0.013752604397231917\n",
      "-0.021010946644139415\n",
      "0.013131186304692557\n",
      "-0.019939562377873227\n",
      "0.01278941803180529\n",
      "-0.019421760756266036\n",
      "0.012242682094929482\n",
      "-0.01849827976227099\n",
      "0.011922775838260603\n",
      "3.195944545686031\n",
      "-0.05711814175498166\n",
      "0.013438451281919391\n",
      "-0.020768254650200248\n",
      "0.012845204281231922\n",
      "-0.01961770553172281\n",
      "0.012289352129672549\n",
      "-0.0186271864849432\n",
      "0.011752451831298692\n",
      "-0.017717088524101345\n",
      "0.011345455374530276\n",
      "-0.01706836372424607\n",
      "0.010915389693285142\n",
      "-0.016244293891654178\n",
      "0.010637506631684609\n",
      "-0.015919611647531737\n",
      "0.010243957679613638\n",
      "0.03600961602287975\n",
      "-0.0313806814780732\n",
      "0.010331350384247106\n",
      "0.032296544199009534\n",
      "-0.0301365004218912\n",
      "0.010361469022695866\n",
      "-0.015388596313702126\n",
      "0.010050471558622383\n",
      "0.03197061828529802\n",
      "-0.02965651341716194\n",
      "0.01009017435002275\n",
      "-0.014919297659531323\n",
      "0.009803372620333813\n",
      "0.39796816653315825\n",
      "-0.029998776608795463\n",
      "0.010204863688204847\n",
      "-0.015202555556262024\n",
      "0.00981389959590608\n",
      "-0.014498503977362\n",
      "0.009442917979050097\n",
      "0.027343476528255736\n",
      "-0.027009895360043796\n",
      "0.009445768026443207\n",
      "-0.013880264340986567\n",
      "0.00918983566864621\n",
      "0.027394380897508393\n",
      "-0.026682865439046032\n",
      "0.009210567010065318\n",
      "-0.013490559212384044\n",
      "0.008957054070494934\n",
      "0.027258896108234613\n",
      "-0.026346116233027506\n",
      "0.008986581223857934\n",
      "0.025161575487608282\n",
      "-0.025556508346334588\n",
      "0.008981781010963056\n",
      "-0.013149859168182856\n",
      "0.00873120795562725\n",
      "0.025345942851230138\n",
      "-0.02529602931405613\n",
      "0.008744558150593317\n",
      "-0.012777990150900203\n",
      "0.008495287116528442\n",
      "0.02537999252090538\n",
      "-0.024969008768000928\n",
      "0.008520935640668881\n",
      "0.023599116165129785\n",
      "-0.024284317944635533\n",
      "0.008522887971614511\n",
      "-0.012437066954415413\n",
      "0.008282038012018662\n",
      "0.02391963796492508\n",
      "-0.024011193025424375\n",
      "0.008298280194589733\n",
      "-0.012104304545612754\n",
      "0.008012781938712117\n",
      "0.024274801624509082\n",
      "-0.023718426345725307\n",
      "0.008063471592171148\n",
      "0.02257762282571\n",
      "-0.023021320523330006\n",
      "0.00807900666893524\n",
      "-0.01169342025467146\n",
      "0.007860426976987803\n",
      "0.02255585650083656\n",
      "-0.022795453511701733\n",
      "0.007883779799810347\n",
      "-0.011397605560760318\n",
      "0.007668558086110909\n",
      "0.022599098181295525\n",
      "-0.022560509023889508\n",
      "0.007697520970438255\n",
      "0.02107970554255136\n",
      "-0.021924566548000114\n",
      "0.007708519043236471\n",
      "-0.01113407052019923\n",
      "0.007495743239338528\n",
      "0.02121792656587981\n",
      "-0.02175245048015175\n",
      "0.007509654348249505\n",
      "-0.01083027943164604\n",
      "0.0073035827838282144\n",
      "0.02132696274388406\n",
      "-0.021529769580370398\n",
      "0.007324436348368132\n",
      "0.02007000596172143\n",
      "-0.02095566637322628\n",
      "0.0073268883782035025\n",
      "-0.010565059612230632\n",
      "0.0070688683297587766\n",
      "0.020348318621046888\n",
      "-0.020761291351938466\n",
      "0.007090210539388431\n",
      "0.019138494828056723\n",
      "-0.020231370397808708\n",
      "0.00715286907627524\n",
      "-0.010298424626528692\n",
      "0.006899297020027751\n",
      "0.019346358126772545\n",
      "-0.020058901566109105\n",
      "0.006914713826433981\n",
      "-0.009904037343877256\n",
      "0.0067295938017536945\n",
      "-0.04952829183913221\n",
      "0.006116988067244347\n",
      "0.025100789701632185\n",
      "-0.02164846818127276\n",
      "0.006328799703884813\n",
      "0.022843942195437048\n",
      "-0.020868010343784778\n",
      "0.00640813456293673\n",
      "0.021054892840426033\n",
      "-0.02017557146543814\n",
      "0.006468092021754691\n",
      "-0.04519536468161776\n"
     ]
    }
   ],
   "source": [
    "d=test.models\n",
    "for f in d.keys():\n",
    "    for s in d[f].keys():\n",
    "        for k in d[f][s].keys():\n",
    "            print(d[f][s][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "33a4e08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0053157806396484375\n",
      "0.0010025501251220703\n"
     ]
    }
   ],
   "source": [
    "n=1000000\n",
    "x=np.random.uniform(-10,10,n)\n",
    "a=2\n",
    "strat=time.time()\n",
    "t1=np.sum(x**2)\n",
    "end=time.time()\n",
    "\n",
    "print(end-strat)\n",
    "\n",
    "strat=time.time()\n",
    "t2=np.dot(x,x)\n",
    "end=time.time()\n",
    "\n",
    "print(end-strat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e4e930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96483e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73889607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5d237e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ef8038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746fbe39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a4993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a7a1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae5f01b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44cee96a",
   "metadata": {},
   "source": [
    "# version avec interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acaba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a faire : \n",
    "\n",
    "#integrer step dans fit\n",
    "#integrer frac_ech\n",
    "#integrer nbr para dans info\n",
    "#integrer w\n",
    "#integrer moment\n",
    "#integrer intercept parmi variable\n",
    "#integrer lr exp\n",
    "#integrer fonction predict\n",
    "#integrer fonction pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d545a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComponentWiseBoostingRegressor:\n",
    "    \n",
    "    \"\"\"\n",
    "    component-wise ReLu boosting regression (avec feature quanti)\n",
    "\n",
    "    deg_inter : entier indiquant le degre d'interaction entre les variables. \n",
    "\n",
    "    form_inter : string indiquant le type d'interaction. uniquement valable si deg_inter>0\n",
    "                 - 'ind' : indicatrice \n",
    "                 - 'mult' : multiplication\n",
    "\n",
    "    step_bef_inter : entier indiquant le nombre d'tapes  considrer avant d'inclure des intractions dans le modle.\n",
    "                     uniquement valable si deg_inter>0\n",
    "\n",
    "    min_obs_bin : entier indiquant le nombre minimal d'observations pour effectuer une regression\n",
    "\n",
    "    strat_k : \n",
    "             - None indiquant que l'on va chercher tous les bins de taille d'au moins min_obs_bin observations\n",
    "             - 'brent' indiquant que l'on va chercher via l'algo de brent\n",
    "    \n",
    "    frac_ech : float (entre 0 et 1) indiquant la fraction d'echantillon a utiliser pour rechercher le meilleur k. \n",
    "    \n",
    "\n",
    "    w : \n",
    "        - None indiquant que les individus ont le meme poids\n",
    "        - array contenant les poids des individus \n",
    "\n",
    "    mode_learning_rate : string indiquant le type de pas \n",
    "                        - 'fixe' indiquant que l'utilise un pas fixe \n",
    "                        - 'steepest' indiquant que celui ci sera estime a chaque iteration\n",
    "                        - 'exp' indiquant que celui ci diminue exponentiellement avec le nombre d'tapes\n",
    "    \n",
    "    fix_learning_rate : float indiquant la taille du pas (de dpart si mode_learning_rate='exp' et constant si mode_learning_rate='fixe')\n",
    "                        Uniquement utilisable si mode_learning_rate!='steepest' \n",
    "    \n",
    "    dec_learning_rate : float indiquant le taux de decroissance du pas. Uniquemen utilisable si mode_learning_rate='exp'\n",
    "\n",
    "\n",
    "    loss : string indiquant la loss a utiliser. \n",
    "           - 'sse' : ...,\n",
    "           - 'sae' : ......, \n",
    "           - 'huber' : ....,\n",
    "           \n",
    "    delta_huber : uniquement valable si loss='huber'\n",
    "                  - (\"fixe\", x) : tuple (string, float) indiquant que l'on utilise un delta fixe=x\n",
    "                  - (\"z\", x) : tuple (string, float) indiquant que l'on utilise un delta = x*std\n",
    "                  - (\"quantile\", x) : tuple (string, float) indiquant que l'on utilise un delta variable = au x quantile\n",
    "\n",
    "\n",
    "    stop : tuple indiquant la regle d'arret.\n",
    "           - ('m', x) : tuple (string, int) indiquant que l'on arrete apres x etapes\n",
    "           - ('info', x) : tuple (string, float) indiquant que l'on arrete lorsque la critere d'info augmente. \n",
    "                           x represente la penalite du nombre de parametre. \n",
    "                           critere info de la forme : log(sum error)+(x*k)/n avec k le nombre de parametres\n",
    "           - ('grad', x) : tuple (string, float) indiquant que l'on arrete lorsque la gradient est inferieur a x\n",
    "           - ('dif_loss', x) : tuple (string, float) indiquant que l'on arrete lorsque la difference de loss est inferieur a x\n",
    "\n",
    "\n",
    "    moment : booleen indiquant si l'on utilise le boosting avec moment \n",
    "    \n",
    "    verbose : boolean indiquant si l'on affiche un resume de l'avance\n",
    "\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stop=('m', 50), mode_learning_rate=\"fixe\", fix_learning_rate=0.1, loss=\"sse\",\n",
    "                 min_obs_bin=50, deg_inter=0, moment=False, strat_k=None, form_inter=\"ind\", step_bef_inter=0,\n",
    "                 delta_huber=(\"z\", 3), w=None, verbose=True, dec_learning_rate=0, frac_ech=1):\n",
    "        \n",
    "        self.stop = stop\n",
    "        \n",
    "        self.mode_learning_rate=mode_learning_rate\n",
    "        \n",
    "        if mode_learning_rate!=\"steepest\":\n",
    "            self.learning_rate = fix_learning_rate\n",
    "            if mode_learning_rate==\"exp\":\n",
    "                self.dec_learning_rate=dec_learning_rate\n",
    "        else : \n",
    "            self.learning_rate=0\n",
    "\n",
    "        self.loss_func = Loss(loss, delta_huber)\n",
    "        self.min_obs_bin = min_obs_bin\n",
    "        self.form_inter=form_inter\n",
    "        self.step_bef_inter=step_bef_inter\n",
    "        self.deg_inter = deg_inter\n",
    "        self.moment = moment\n",
    "        \n",
    "        self.strat_k=strat_k\n",
    "        self.frac_ech=frac_ech\n",
    "        \n",
    "        self.verbose=verbose\n",
    "        self.w = w\n",
    "        \n",
    "        self.intercept=0\n",
    "        #ajouter opti intercept lors de la recherche du meilleur modele\n",
    "        \n",
    "        self.models = dict()\n",
    "        self.degre_model=0\n",
    "        self.nbr_para=0\n",
    "        # avec interaction : degree -> features -> sides -> ks : beta\n",
    "        \n",
    "        #temporaire\n",
    "#         self.pred=None\n",
    "    \n",
    "    \n",
    "    def compute_feature(self, x, f, s, k):\n",
    "        if self.form_inter==\"ind\":\n",
    "            if s==\"left\":\n",
    "                relu_val = np.where(x[:, f]<=k, 1, 0)\n",
    "            else : \n",
    "                relu_val = np.where(x[:, f]>=k, 1, 0)\n",
    "        else : \n",
    "            if s==\"left\":\n",
    "                relu_val = np.where(x[:, f]<=k, k-x[:, f], 0)\n",
    "            else : \n",
    "                relu_val = np.where(x[:, f]>=k, x[:, f]-k, 0)\n",
    "        return relu_val\n",
    "        \n",
    "    def best_model(self, X, res, y, y_pred_actu, step):\n",
    "        best_score = float('inf')\n",
    "        best_beta, best_k, best_side, best_y_pred, best_feature = None, None, None, None, None\n",
    "        \n",
    "        #calculer ici info sur res et passer aux fonctions \n",
    "        if self.strat_k is None : \n",
    "            somme_y2=np.dot(res, res)\n",
    "            \n",
    "        \n",
    "        #paralleliser\n",
    "        #ajouter intercept dans choix meilleur modele\n",
    "        for j in range(0, X.shape[1], 1):\n",
    "            if self.strat_k is None : \n",
    "                temp_beta, temp_k, temp_side, temp_erreur, temp_y_pred = best_relu_greedy(X[:,j], res, self.min_obs_bin, self.w)\n",
    "            else  : \n",
    "                temp_beta, temp_k, temp_side, temp_erreur, temp_y_pred = best_relu_brent(X[:,j], res, self.min_obs_bin, self.w)\n",
    "            \n",
    "            if temp_erreur < best_score:\n",
    "                best_score = temp_erreur\n",
    "                best_beta, best_k, best_side, best_y_pred = temp_beta, temp_k, temp_side, temp_y_pred\n",
    "                best_feature = (j)\n",
    "        best_deg=0\n",
    "        \n",
    "        if self.deg_inter>0 and step>= self.step_bef_inter: \n",
    "            for d in range(0, min(self.deg_inter, self.degre_model+1), 1):\n",
    "                for m in self.models[d]:\n",
    "                    var_inter=np.ones((X.shape[0],))\n",
    "                    for f,s,k in zip(*m[d]):\n",
    "                        #stocker ces resultats ? une partie ? \n",
    "                        var_inter*=self.compute_feature(X, f, s, k)\n",
    "                    \n",
    "                    #conserver cas ou c'est trop petit ? \n",
    "                    filtre = var_inter>0\n",
    "                    if np.sum(filtre)<2*self.min_obs_bin:\n",
    "                        continue\n",
    "                    \n",
    "                    #paralleliser\n",
    "                    for j in range (0, X.shape[1], 1):\n",
    "                        if self.strat_k is None : \n",
    "                            temp_beta, temp_k, temp_side, temp_erreur, temp_y_pred = best_relu_greedy_inter(X[:,j], res,\n",
    "                                                                                                            self.min_obs_bin,\n",
    "                                                                                                            self.w, var_inter,\n",
    "                                                                                                            self.form_inter,\n",
    "                                                                                                            filtre)\n",
    "                        else : \n",
    "                            temp_beta, temp_k, temp_side, temp_erreur, temp_y_pred = best_relu_brent_inter(X[:,j], res, self.min_obs_bin, self.w, var_inter)\n",
    "\n",
    "                        if temp_erreur < best_score:\n",
    "                            best_score = temp_erreur\n",
    "                            best_beta, best_k, best_side, best_y_pred = temp_beta, (*k, temp_k), (*s,temp_side), temp_y_pred\n",
    "                            best_feature = (*f, j)\n",
    "                            best_deg=d+1\n",
    "      \n",
    "        if self.mode_learning_rate==\"steepest\": \n",
    "            self.learning_rate=self.loss_func.opti_slope(y-y_pred_actu, best_y_pred, self.w)\n",
    "                 \n",
    "        return (best_beta, best_k, best_side, best_y_pred, best_feature, best_deg)\n",
    "                 \n",
    "    def update_model_new_res(self, best_beta, best_k, best_side, best_y_pred, best_feature, best_deg, y, y_pred_actu):\n",
    "        if self.models.get(best_deg):\n",
    "            if self.models[best_deg].get((best_feature, best_side, best_k)):\n",
    "                self.models[best_deg][(best_feature, best_side, best_k)] += self.learning_rate * best_beta\n",
    "            else : \n",
    "                self.models[best_deg][(best_feature, best_side, best_k)] = self.learning_rate * best_beta\n",
    "                self.nbr_para+=1\n",
    "        else : \n",
    "            self.models[best_deg]={(best_feature, best_side, best_k) : self.learning_rate * best_beta}\n",
    "            self.degre_model+=1\n",
    "\n",
    "        return (y_pred_actu+self.learning_rate*best_y_pred,\n",
    "                self.loss_func.gradient(y-y_pred_actu, self.learning_rate*best_y_pred, self.w))\n",
    "      \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.models = dict()\n",
    "        self.degre_model=0\n",
    "        self.nbr_para=0\n",
    "        \n",
    "        #ici tier features \n",
    "        \n",
    "        y_pred_actu=np.zeros((X.shape[0],))\n",
    "        \n",
    "        residuals = self.loss_func.gradient(y, y_pred_actu, self.w)\n",
    "        if self.stop[0]=='m':\n",
    "            for i in range(0,self.stop[1],1):\n",
    "                if self.verbose : \n",
    "                    print(10*'-', i, 10*'-')\n",
    "                best_beta, best_k, best_side, best_y_pred, best_feature, best_deg = self.best_model(X, residuals, y, y_pred_actu, i)\n",
    "                y_pred_actu, residuals=self.update_model_new_res(best_beta, best_k, best_side, best_y_pred, best_feature, best_deg, y, y_pred_actu)\n",
    "            \n",
    "#             self.pred=y_pred_actu\n",
    "            \n",
    "        elif self.stop[0]=='info':\n",
    "            nbr_param=0\n",
    "            prec_loss=self.loss_func.loss(y, y_pred_actu, self.w)\n",
    "            prec_info = ((self.stop[1]*nbr_param)/X.shape[0])+np.log(prec_loss)\n",
    "            best_beta, best_k, best_side, best_y_pred, best_feature = self.best_model(X, residuals, y, y_pred_actu)\n",
    "            nbr_param+=1\n",
    "            new_loss=self.loss_func.loss(y-y_pred_actu, self.learning_rate*best_y_pred, self.w)\n",
    "            new_info = ((self.stop[1]*nbr_param)/X.shape[0])+np.log(new_loss)\n",
    "            if self.verbose :\n",
    "                print(10*'-', nbr_param, 10*'-')\n",
    "                print(\"modele \", nbr_param-1, \" parametres / loss = \", prec_loss, \" / info = \", prec_info)\n",
    "                print(\"modele \", nbr_param, \" parametres / loss = \", new_loss, \" / info = \", new_info)\n",
    "                print('\\n')\n",
    "            while new_info<prec_info:\n",
    "                y_pred_actu, residuals=self.update_model_new_res(best_beta, best_k, best_side, best_y_pred, best_feature, y, y_pred_actu)\n",
    "                prec_loss=new_loss\n",
    "                prec_info=new_info\n",
    "                best_beta, best_k, best_side, best_y_pred, best_feature = self.best_model(X, residuals,  y, y_pred_actu)\n",
    "                nbr_param+=1\n",
    "                new_loss=self.loss_func.loss(y-y_pred_actu, self.learning_rate*best_y_pred, self.w)\n",
    "                new_info=((self.stop[1]*nbr_param)/X.shape[0])+np.log(new_loss)\n",
    "                if self.verbose: \n",
    "                    print(10*'-', nbr_param, 10*'-')\n",
    "                    print(\"modele \", nbr_param-1, \" parametres / loss = \", prec_loss, \" / info = \", prec_info)\n",
    "                    print(\"modele \", nbr_param, \" parametres / loss = \", new_loss, \" / info = \", new_info)\n",
    "                    print('\\n')\n",
    "                    \n",
    "#             self.pred=y_pred_actu\n",
    "                \n",
    "        elif self.stop[0]=='grad' : \n",
    "            best_beta, best_k, best_side, best_y_pred, best_feature = self.best_model(X, residuals, y, y_pred_actu)\n",
    "            grad=np.max(np.abs(self.learning_rate*best_y_pred))\n",
    "            if self.verbose : \n",
    "                nbr_etape=1\n",
    "                print(10*'-', nbr_etape, 10*'-')\n",
    "                print(grad)\n",
    "                print('\\n')\n",
    "                nbr_etape+=1\n",
    "            while grad>self.stop[1]:\n",
    "                y_pred_actu, residuals=self.update_model_new_res(best_beta, best_k, best_side, best_y_pred, best_feature, y, y_pred_actu)\n",
    "                best_beta, best_k, best_side, best_y_pred, best_feature = self.best_model(X, residuals, y, y_pred_actu)\n",
    "                grad=np.max(np.abs(self.learning_rate*best_y_pred))\n",
    "                if self.verbose : \n",
    "                    print(10*'-', nbr_etape, 10*'-')\n",
    "                    print(grad)\n",
    "                    print('\\n')\n",
    "                    nbr_etape+=1\n",
    "                    \n",
    "#             self.pred=y_pred_actu\n",
    "                \n",
    "        elif self.stop[0]=='dif_loss':\n",
    "            prec_loss = self.loss_func.loss(y, y_pred_actu, self.w)\n",
    "            best_beta, best_k, best_side, best_y_pred, best_feature = self.best_model(X, residuals, y, y_pred_actu)\n",
    "            new_loss = self.loss_func.loss(y-y_pred_actu, self.learning_rate*best_y_pred, self.w)\n",
    "            if self.verbose :\n",
    "                etape=1\n",
    "                print(10*'-', etape, 10*'-')\n",
    "                print(\"modele \", etape-1, \" loss = \", prec_loss)\n",
    "                print(\"modele \", etape, \" loss = \", new_loss)\n",
    "                print('\\n')\n",
    "                etape+=1\n",
    "            while (prec_loss-new_loss)>self.stop[1]:\n",
    "                y_pred_actu, residuals=self.update_model_new_res(best_beta, best_k, best_side, best_y_pred, best_feature, y, y_pred_actu)\n",
    "                prec_loss=new_loss\n",
    "                best_beta, best_k, best_side, best_y_pred, best_feature = self.best_model(X, residuals, y, y_pred_actu)\n",
    "                new_loss=self.loss_func.loss(y-y_pred_actu, self.learning_rate*best_y_pred, self.w)\n",
    "                if self.verbose :\n",
    "                    print(10*'-', etape, 10*'-')\n",
    "                    print(\"modele \", etape-1, \" loss = \", prec_loss)\n",
    "                    print(\"modele \", etape, \" loss = \", new_loss)\n",
    "                    print('\\n')\n",
    "                    etape+=1\n",
    "                    \n",
    "#             self.pred=y_pred_actu\n",
    "                    \n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99c6863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_relu_greedy_inter(x, y, min_obs, w=None, var_inter, form, filtre):\n",
    "    \n",
    "    somme_y2=np.dot(y,y)\n",
    "    x_filtre=x[filtre]\n",
    "    y_filtre=y[filtre]\n",
    "    \n",
    "    if form==\"ind\":\n",
    "        return best_relu_greedy()\n",
    "    else : \n",
    "        var_inter_filtre=var_inter[filtre]\n",
    "        \n",
    "        \n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b959fde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#?proposer fit residus avec autre chose que mse?\n",
    "\n",
    "#=> eviter recalcule\n",
    "    #stocker ordre variables \n",
    "    #stocker sommex et somme_x_left\n",
    "\n",
    "#ne pas calculer somme total mais celle de droite directement\n",
    "#utiliser np.dot(x,x) au lieu de np.sum(x**2)\n",
    "    \n",
    "#ajouter poids\n",
    "\n",
    "#optimiser recherche dans cas o x est discret => eliminer redondance dans x_sorted\n",
    "\n",
    "\n",
    "#cas interaction : calculer somme y**2 => garder x, var inter et y ou var inter!=0 \n",
    "                    #=> trier x et appliquer cet ordre a x, var inter et y \n",
    "                    #=> k utiliser via x_sorted\n",
    "                    #=>a la place de somme_x_left et right utiliser somme_x_left et right * var_inter_left et right\n",
    "\n",
    "def best_relu_greedy(x, deg_inter=0, y, min_obs, w=None):\n",
    "    \n",
    "    n=x.shape[0]\n",
    "    idx_sorted = np.argsort(x)\n",
    "    x_sorted, y_sorted = x[idx_sorted], y[idx_sorted]\n",
    "\n",
    "    somme_y=np.sum(y)\n",
    "    somme_x=np.sum(x)\n",
    "    somme_x2=np.dot(x,x)\n",
    "    somme_y2=np.dot(y,y)\n",
    "    somme_xy=np.dot(x, y)\n",
    "\n",
    "    k=(x_sorted[min_obs-1]+x_sorted[min_obs])/2\n",
    "    #h(k-x)1_{x<=k}\n",
    "    x_left=x_sorted[:min_obs]\n",
    "    y_left=y_sorted[:min_obs]\n",
    "    n_obs_left=min_obs\n",
    "    somme_x_left=np.sum(x_left)\n",
    "    somme_y_left=np.sum(y_left)\n",
    "    somme_x2_left=np.dot(x_left,x_left)\n",
    "    somme_xy_left=np.dot(x_left, y_left)\n",
    "\n",
    "    cov_left=(k*somme_y_left-somme_xy_left)\n",
    "    ecart_x2_left=((n_obs_left*(k**2))-(2*k*somme_x_left)+somme_x2_left)\n",
    "    beta_left=cov_left/ecart_x2_left\n",
    "    erreur_left=somme_y2-(2*beta_left*cov_left)+(beta_left**2)*ecart_x2_left\n",
    "    erreur_min=erreur_left\n",
    "    beta_min=beta_left\n",
    "    side=\"left\"\n",
    "    k_min=k\n",
    "\n",
    "\n",
    "\n",
    "    somme_y_right=somme_y-somme_y_left\n",
    "    somme_x_right=somme_x-somme_x_left\n",
    "    somme_x2_right=somme_x2-somme_x2_left\n",
    "    somme_xy_right=somme_xy-somme_xy_left\n",
    "    n_obs_right=n-n_obs_left\n",
    "    cov_right=(somme_xy_right-k*somme_y_right)\n",
    "    ecart_x2_right=((n_obs_right*(k**2))-(2*k*somme_x_right)+somme_x2_right)\n",
    "    beta_right=cov_right/ecart_x2_right\n",
    "    erreur_right=somme_y2-(2*beta_right*cov_right)+(beta_right**2)*ecart_x2_right\n",
    "    if erreur_right<erreur_min:\n",
    "        erreur_min=erreur_right\n",
    "        beta_min=beta_right\n",
    "        side=\"right\"\n",
    "        k_min=k\n",
    "\n",
    "\n",
    "    for i in range(min_obs+1, n-min_obs, 1):\n",
    "        k=(x_sorted[i-1]+x_sorted[i])/2\n",
    "        somme_x_left+=x_sorted[i-1]\n",
    "        somme_y_left+=y_sorted[i-1]\n",
    "        somme_x2_left+=x_sorted[i-1]**2\n",
    "        somme_xy_left+=y_sorted[i-1]*x_sorted[i-1]\n",
    "        n_obs_left+=1\n",
    "        cov_left=(k*somme_y_left-somme_xy_left)\n",
    "        ecart_x2_left=((n_obs_left*(k**2))-(2*k*somme_x_left)+somme_x2_left)\n",
    "        beta_left=cov_left/ecart_x2_left\n",
    "        erreur_left=somme_y2-(2*beta_left*cov_left)+(beta_left**2)*ecart_x2_left\n",
    "        if erreur_left<erreur_min:\n",
    "            erreur_min=erreur_left\n",
    "            beta_min=beta_left\n",
    "            side=\"left\"\n",
    "            k_min=k\n",
    "\n",
    "\n",
    "        somme_y_right=somme_y-somme_y_left\n",
    "        somme_x_right=somme_x-somme_x_left\n",
    "        somme_x2_right=somme_x2-somme_x2_left\n",
    "        somme_xy_right=somme_xy-somme_xy_left\n",
    "        n_obs_right=n-n_obs_left\n",
    "        cov_right=(somme_xy_right-k*somme_y_right)\n",
    "        ecart_x2_right=((n_obs_right*(k**2))-(2*k*somme_x_right)+somme_x2_right)\n",
    "        beta_right=cov_right/ecart_x2_right\n",
    "        erreur_right=somme_y2-(2*beta_right*cov_right)+(beta_right**2)*ecart_x2_right\n",
    "        if erreur_right<erreur_min:\n",
    "            erreur_min=erreur_right\n",
    "            beta_min=beta_right\n",
    "            side=\"right\"\n",
    "            k_min=k\n",
    "    \n",
    "    #faire une fonction compute\n",
    "    if side==\"left\":\n",
    "        y_pred=np.where(x<=k_min, beta_min*(k_min-x), 0)\n",
    "    else : \n",
    "        y_pred=np.where(x>=k_min, beta_min*(x-k_min), 0)\n",
    "        \n",
    "    return (beta_min, k_min, side, erreur_min, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eda70a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajouter poids\n",
    "\n",
    "def best_relu_brent(x, y, min_obs, w=None):\n",
    "    x_sorted= np.sort(x)\n",
    "    x_min=x_sorted[min_obs-1]\n",
    "    x_max=x_sorted[n-min_obs]\n",
    "    \n",
    "    beta_z_store = {'left': None, 'right': None}\n",
    "    \n",
    "    def objective_right(k):\n",
    "        z=np.where(x<=k, 0, x-k)\n",
    "        beta=np.dot(y,z)/np.dot(z,z)\n",
    "        beta_z_store['right'] = (beta, z)\n",
    "        r = np.sum((y-beta*z)**2)\n",
    "        return r\n",
    "    \n",
    "    def objective_left(k):\n",
    "        z=np.where(x>=k, 0, k-x)\n",
    "        beta=np.dot(y,z)/np.dot(z,z)\n",
    "        beta_z_store['left'] = (beta, z)\n",
    "        r = np.sum((y-beta*z)**2)\n",
    "        return r\n",
    "    \n",
    "    result_left = minimize_scalar(objective_left, bounds=(x_min, x_max), method='bounded')\n",
    "    result_right = minimize_scalar(objective_right, bounds=(x_min, x_max), method='bounded')\n",
    "    \n",
    "    \n",
    "    if result_left.fun < result_right.fun:\n",
    "        beta_min, z = beta_z_store['left']\n",
    "        y_pred=beta_min*z\n",
    "        return (beta_min, result_left.x, \"left\", result_left.fun, y_pred)\n",
    "    else : \n",
    "        beta_min, z = beta_z_store['right']\n",
    "        y_pred=beta_min*z\n",
    "        return (beta_min, result_right.x, \"right\", result_right.fun, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658c5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self, name=\"sse\", delta=(\"fixe\", 3)):\n",
    "        \n",
    "        \"\"\"\n",
    "        name : string indiquant le type de loss \n",
    "               - 'sse' : ....., \n",
    "               - 'sae' : ......, \n",
    "               - 'huber' : ......, \n",
    "        \n",
    "        delta : uniquement valable si name = 'huber'\n",
    "               - (\"fixe\", x) : tuple (string, float) indiquant que l'on utilise un delta fixe=x\n",
    "               - (\"z\", x) : tuple (string, float) indiquant que l'on utilise un delta = x*std\n",
    "               - (\"quantile\", x) : tuple (string, float) indiquant que l'on utilise un delta variable = au x quantile\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.name = name\n",
    "        self.delta_method = delta[0]\n",
    "        self.delta_val = delta[1]\n",
    "\n",
    "    def loss(self, y_true, y_pred, w):\n",
    "        diff = y_true - y_pred\n",
    "        if self.name == \"sse\":\n",
    "            if w is None : \n",
    "                return 0.5 * np.sum(diff ** 2)\n",
    "        elif self.name == \"sae\":\n",
    "            if w is None : \n",
    "                return np.sum(np.abs(diff))\n",
    "        elif self.name == \"huber\":\n",
    "            if w is None : \n",
    "                abs_diff = np.abs(diff)\n",
    "                if self.delta_method==\"fixe\" : \n",
    "                    delta = self.delta_val\n",
    "                elif self.delta_method==\"z\" : \n",
    "                    delta=self.delta_val*np.std(abs_diff)\n",
    "                else : \n",
    "                    delta = np.quantile(abs_diff, self.delta_val)\n",
    "                return np.sum(np.where(abs_diff <= delta,\n",
    "                                            0.5 * diff ** 2,\n",
    "                                            delta * (abs_diff - 0.5 * delta))) \n",
    "                \n",
    "        \n",
    "    def gradient(self, y_true, y_pred, w):\n",
    "        if self.name == \"sse\":\n",
    "            if w is None : \n",
    "                return y_true - y_pred\n",
    "        elif self.name == \"sae\":\n",
    "            if w is None : \n",
    "                return np.sign(y_true - y_pred)\n",
    "        elif self.name == \"huber\":\n",
    "            if w is None : \n",
    "                diff = y_true - y_pred\n",
    "                abs_diff = np.abs(diff)\n",
    "                if self.delta_method==\"fixe\" : \n",
    "                    delta = self.delta_val\n",
    "                elif self.delta_method==\"z\" : \n",
    "                    delta=self.delta_val*np.std(abs_diff)\n",
    "                else : \n",
    "                    delta = np.quantile(abs_diff, self.delta_val)\n",
    "                grad = np.where(abs_diff <= delta, diff, delta * np.sign(diff))\n",
    "                return grad\n",
    "        \n",
    "    def opti_slope(self, y_true, y_pred, w):\n",
    "        if self.name==\"sse\":\n",
    "            return 1\n",
    "        elif self.name==\"sae\":\n",
    "            return weighted_median_interpolated(y_pred, y_true, w)\n",
    "        else : \n",
    "            if w is None : \n",
    "                if self.delta_method==\"fixe\" : \n",
    "                    delta = self.delta_val\n",
    "                elif self.delta_method==\"z\" : \n",
    "                    delta=self.delta_val*np.std(np.abs(y_true))\n",
    "                else : \n",
    "                    delta = np.quantile(np.abs(y_true), self.delta_val)\n",
    "                return opt_huber_slope(delta, y_pred, y_true,  w)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0484a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_huber_slope(delta, y_pred, y_true,  w):\n",
    "    def objective(a):\n",
    "        diff = y_true - a*y_pred\n",
    "        abs_diff = np.abs(diff)\n",
    "        r=np.sum(np.where(abs_diff <= delta, 0.5 * diff ** 2, delta * (abs_diff - 0.5 * delta))) \n",
    "        return r\n",
    "    \n",
    "    result = minimize_scalar(objective, method='brent')\n",
    "    return result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bfd539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajouter poids \n",
    "\n",
    "def weighted_median_interpolated(x, y, w):\n",
    "\n",
    "    y_w=y/x\n",
    "    w=np.abs(x)\n",
    "    \n",
    "    # Sort by values\n",
    "    sorted_indices = np.argsort(y_w)\n",
    "    y_w_sorted = y_w[sorted_indices]\n",
    "    weights_sorted = w[sorted_indices]\n",
    "\n",
    "    total_weight = np.sum(weights_sorted)\n",
    "    cum_weights = np.cumsum(weights_sorted)\n",
    "\n",
    "    # Find where cumulative weight crosses 50%\n",
    "    cutoff = 0.5 * total_weight\n",
    "    idx = np.searchsorted(cum_weights, cutoff)\n",
    "\n",
    "    if idx == 0:\n",
    "        return y_w_sorted[0]\n",
    "    elif cum_weights[idx] == cutoff or weights_sorted[idx] == 0:\n",
    "        return y_w_sorted[idx]\n",
    "    else:\n",
    "        # Linear interpolation between previous and current\n",
    "        w1 = cum_weights[idx - 1]\n",
    "        w2 = cum_weights[idx]\n",
    "        v1 = y_w_sorted[idx - 1]\n",
    "        v2 = y_w_sorted[idx]\n",
    "        return v1 + (cutoff - w1) / (w2 - w1) * (v2 - v1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a751134a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a0824f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c85c75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d950a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f821aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
