{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5ef5b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yoann\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import time\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f5a2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1000000\n",
    "min_obs=int(np.ceil(np.sqrt(n)))\n",
    "# min_obs=10\n",
    "x=np.random.uniform(-10,10,(n,))\n",
    "# y=x**2+np.random.normal(0,2,(n,))\n",
    "y=np.sin(x)+np.random.normal(0,0.1,(n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276c30be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(beta)\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c873f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1000\n",
    "min_obs=int(np.ceil(np.sqrt(n)))\n",
    "# min_obs=10\n",
    "x=np.random.uniform(-10,10,(n,))\n",
    "y=np.sin(x)+np.random.normal(0,1,(n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b6e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(k, x, y):\n",
    "    z=np.maximum(x-k, 0)\n",
    "    num=np.dot(y, z)**2\n",
    "    denom=np.sum(z**2)\n",
    "    return -num/denom\n",
    "\n",
    "k_vals=np.linspace(-10,10,n)\n",
    "f_vals=np.array([f(k, x, y) for k in k_vals])\n",
    "\n",
    "plt.scatter(k_vals, f_vals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a1d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_k(x, y):\n",
    "    \n",
    "    idx_sorted = np.argsort(x)\n",
    "    x_sorted= x[idx_sorted]\n",
    "    \n",
    "    x_min=x_sorted[min_obs-1]\n",
    "    x_max=x_sorted[n-min_obs]\n",
    "    \n",
    "    def objective_right(k):\n",
    "        z=np.maximum(x-k, 0)\n",
    "        beta=np.dot(y,z)/np.sum(z**2)\n",
    "        r = np.sum((y-beta*z)**2)\n",
    "        return r\n",
    "    \n",
    "    def objective_left(k):\n",
    "        z=np.maximum(k-x, 0)\n",
    "        beta=np.dot(y,z)/np.sum(z**2)\n",
    "        r = np.sum((y-beta*z)**2)\n",
    "        return r\n",
    "    \n",
    "    result_left = minimize_scalar(objective_left, bounds=(x_min, x_max), method='bounded')\n",
    "    result_right = minimize_scalar(objective_right, bounds=(x_min, x_max), method='bounded')\n",
    "    if result_left.fun < result_right.fun:\n",
    "        return (result_left.x, \"left\", result_left.fun)\n",
    "    else : \n",
    "        return (result_right.x, \"right\", result_right.fun)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "o=optimize_k(x, y)\n",
    "end = time.time()\n",
    "\n",
    "print(o)\n",
    "print(\"time : \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743b2fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "beta_min, k_min, side, erreur_min, y_pred = best_relu(x,y,min_obs)\n",
    "end = time.time()\n",
    "print(beta_min)\n",
    "print(k_min)\n",
    "print(side)\n",
    "print(erreur_min)\n",
    "print(\"time : \", end-start)\n",
    "# y_prim=y-y_pred\n",
    "# beta_min, k_min, side, erreur_min, y_pred = best_relu(x,y_prim,min_obs)\n",
    "# for i in range(1000):\n",
    "#     y_prim=y_prim-y_pred\n",
    "#     beta_min, k_min, side, erreur_min, y_pred = best_relu(x,y_prim,min_obs)\n",
    "# y_prim=y_prim-y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4712d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y_prim)\n",
    "xmin, xmax = ax.get_xbound()\n",
    "if side==\"left\":\n",
    "    x_line = [xmin, k_min]\n",
    "    y_line = [(k_min-xmin)*beta_min , 0]\n",
    "else : \n",
    "    x_line = [k_min, xmax]\n",
    "    y_line = [0, (xmax-k_min)*beta_min]\n",
    "\n",
    "# Create and add the line\n",
    "line = mlines.Line2D(x_line, y_line, color='red', linewidth=2)\n",
    "ax.add_line(line)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8d2bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_abs_slope(x, y):\n",
    "    def objective(a):\n",
    "        r = np.abs(y-a*x).sum()\n",
    "        return r\n",
    "    \n",
    "    result = minimize_scalar(objective, method='brent')\n",
    "    return result.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c52228",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weighted_median_interpolated(x, y):\n",
    "\n",
    "    y_w=y/x\n",
    "    w=np.abs(x)\n",
    "    \n",
    "    # Sort by values\n",
    "    sorted_indices = np.argsort(y_w)\n",
    "    y_w_sorted = y_w[sorted_indices]\n",
    "    weights_sorted = w[sorted_indices]\n",
    "\n",
    "    total_weight = np.sum(weights_sorted)\n",
    "    cum_weights = np.cumsum(weights_sorted)\n",
    "\n",
    "    # Find where cumulative weight crosses 50%\n",
    "    cutoff = 0.5 * total_weight\n",
    "    idx = np.searchsorted(cum_weights, cutoff)\n",
    "\n",
    "    if idx == 0:\n",
    "        return y_w_sorted[0]\n",
    "    elif cum_weights[idx] == cutoff or weights_sorted[idx] == 0:\n",
    "        return y_w_sorted[idx]\n",
    "    else:\n",
    "        # Linear interpolation between previous and current\n",
    "        w1 = cum_weights[idx - 1]\n",
    "        w2 = cum_weights[idx]\n",
    "        v1 = y_w_sorted[idx - 1]\n",
    "        v2 = y_w_sorted[idx]\n",
    "        return v1 + (cutoff - w1) / (w2 - w1) * (v2 - v1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4c24cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.optimize import minimize_scalar\n",
    "\n",
    "n=1000000\n",
    "x=np.random.uniform(-10,10,(n,))\n",
    "y=x**2+np.random.normal(0,1,(n,))\n",
    "# y=np.sin(x)+np.random.normal(0,2,(n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aadf6e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12.905459533635588,\n",
       " 2.9007115804096095,\n",
       " 'right',\n",
       " 1006497752.4508799,\n",
       " array([26.48077624, 30.6404222 ,  9.73633169, ..., 52.25347099,\n",
       "         0.        ,  0.        ]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_relu_brent(x, y, min_obs=int(np.ceil(np.sqrt(n))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b250361d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12.905458631271298,\n",
       " 2.9007112492788156,\n",
       " 'right',\n",
       " 1006497752.450814,\n",
       " array([26.48077866, 30.64042433,  9.73633528, ..., 52.25347161,\n",
       "         0.        ,  0.        ]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_relu_greedy(x,y,min_obs=int(np.ceil(np.sqrt(n))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d089b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.95261653,  5.27493329,  3.65514668, ...,  6.94965465,\n",
       "       -5.91112068, -2.26381649])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937ab76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "beta_min, k_min, side, erreur_min, y_pred = best_relu(x,y,min_obs)\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b1775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "sol_opt=optimize_abs_slope(y_pred, y)\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f1d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sol_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ad635",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "sol_w=weighted_median_interpolated(y_pred, y)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d928de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sol_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf1a593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3682bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c54b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212c3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca8148fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajouter poids \n",
    "\n",
    "def weighted_median_interpolated(x, y, w):\n",
    "\n",
    "    y_w=y/x\n",
    "    w=np.abs(x)\n",
    "    \n",
    "    # Sort by values\n",
    "    sorted_indices = np.argsort(y_w)\n",
    "    y_w_sorted = y_w[sorted_indices]\n",
    "    weights_sorted = w[sorted_indices]\n",
    "\n",
    "    total_weight = np.sum(weights_sorted)\n",
    "    cum_weights = np.cumsum(weights_sorted)\n",
    "\n",
    "    # Find where cumulative weight crosses 50%\n",
    "    cutoff = 0.5 * total_weight\n",
    "    idx = np.searchsorted(cum_weights, cutoff)\n",
    "\n",
    "    if idx == 0:\n",
    "        return y_w_sorted[0]\n",
    "    elif cum_weights[idx] == cutoff or weights_sorted[idx] == 0:\n",
    "        return y_w_sorted[idx]\n",
    "    else:\n",
    "        # Linear interpolation between previous and current\n",
    "        w1 = cum_weights[idx - 1]\n",
    "        w2 = cum_weights[idx]\n",
    "        v1 = y_w_sorted[idx - 1]\n",
    "        v2 = y_w_sorted[idx]\n",
    "        return v1 + (cutoff - w1) / (w2 - w1) * (v2 - v1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ab71763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_huber_slope(delta, y_pred, y_true,  w):\n",
    "    def objective(a):\n",
    "        diff = y_true - a*y_pred\n",
    "        abs_diff = np.abs(diff)\n",
    "        r=np.sum(np.where(abs_diff <= delta, 0.5 * diff ** 2, self.delta * (abs_diff - 0.5 * delta))) \n",
    "        return r\n",
    "    \n",
    "    result = minimize_scalar(objective, method='brent')\n",
    "    return result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "4c572392",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self, name=\"sse\", delta=(\"fixe\", 3)):\n",
    "        \n",
    "        \"\"\"\n",
    "        name : string indiquant le type de loss \n",
    "               - 'sse' : ....., \n",
    "               - 'sae' : ......, \n",
    "               - 'huber' : ......, \n",
    "        \n",
    "        delta : uniquement valable si name = 'huber'\n",
    "               - (\"fixe\", x) : tuple (string, float) indiquant que l'on utilise un delta fixe=x\n",
    "               - (\"z\", x) : tuple (string, float) indiquant que l'on utilise un delta = x*std\n",
    "               - (\"quantile\", x) : tuple (string, float) indiquant que l'on utilise un delta variable = au x quantile\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.name = name\n",
    "        self.delta_method = delta[0]\n",
    "        self.delta_val = delta[1]\n",
    "\n",
    "    def loss(self, y_true, y_pred, w):\n",
    "        diff = y_true - y_pred\n",
    "        if self.name == \"sse\":\n",
    "            if w is None : \n",
    "                return 0.5 * np.sum(diff ** 2)\n",
    "        elif self.name == \"sae\":\n",
    "            if w is None : \n",
    "                return np.sum(np.abs(diff))\n",
    "        elif self.name == \"huber\":\n",
    "            if w is None : \n",
    "                abs_diff = np.abs(diff)\n",
    "                if self.delta_method==\"fixe\" : \n",
    "                    delta = self.delta_val\n",
    "                elif self.delta_method==\"z\" : \n",
    "                    delta=self.delta_val*np.std(abs_diff)\n",
    "                else : \n",
    "                    delta = np.quantile(abs_diff, self.delta_val)\n",
    "                return np.sum(np.where(abs_diff <= delta,\n",
    "                                            0.5 * diff ** 2,\n",
    "                                            self.delta * (abs_diff - 0.5 * delta))) \n",
    "                \n",
    "        \n",
    "    def gradient(self, y_true, y_pred, w):\n",
    "        if self.name == \"sse\":\n",
    "            if w is None : \n",
    "                return y_true - y_pred\n",
    "        elif self.name == \"sae\":\n",
    "            if w is None : \n",
    "                return np.sign(y_true - y_pred)\n",
    "        elif self.name == \"huber\":\n",
    "            if w is None : \n",
    "                diff = y_true - y_pred\n",
    "                abs_diff = np.abs(diff)\n",
    "                if self.delta_method==\"fixe\" : \n",
    "                    delta = self.delta_val\n",
    "                elif self.delta_method==\"z\" : \n",
    "                    delta=self.delta_val*np.std(abs_diff)\n",
    "                else : \n",
    "                    delta = np.quantile(abs_diff, self.delta_val)\n",
    "                grad = np.where(abs_diff <= delta, diff, delta * np.sign(diff))\n",
    "                return grad\n",
    "        \n",
    "    def opti_slope(self, y_true, y_pred, w):\n",
    "        if self.name==\"sse\":\n",
    "            return 1\n",
    "        elif self.name==\"sae\":\n",
    "            return weighted_median_interpolated(y_pred, y_true, w)\n",
    "        else : \n",
    "            if w is None : \n",
    "                if self.delta_method==\"fixe\" : \n",
    "                    delta = self.delta_val\n",
    "                elif self.delta_method==\"z\" : \n",
    "                    delta=self.delta_val*np.std(np.abs(y_true))\n",
    "                else : \n",
    "                    delta = np.quantile(np.abs(y_true), self.delta_val)\n",
    "                return opt_huber_slope(delta, y_pred, y_true,  w)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "68ad1259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#?proposer fit residus avec autre chose que mse?\n",
    "\n",
    "#=> eviter recalcule\n",
    "    #stocker ordre variables \n",
    "    #stocker sommex et somme_x_left \n",
    "    \n",
    "#ajouter poids\n",
    "\n",
    "def best_relu_greedy(x, y, min_obs, w=None):\n",
    "    n=x.shape[0]\n",
    "    idx_sorted = np.argsort(x)\n",
    "    x_sorted, y_sorted = x[idx_sorted], y[idx_sorted]\n",
    "\n",
    "    somme_y=np.sum(y)\n",
    "    somme_x=np.sum(x)\n",
    "    somme_x2=np.sum(x**2)\n",
    "    somme_y2=np.sum(y**2)\n",
    "    somme_xy=np.dot(x, y)\n",
    "\n",
    "    k=(x_sorted[min_obs-1]+x_sorted[min_obs])/2\n",
    "    #h(k-x)1_{x<=k}\n",
    "    x_left=x_sorted[:min_obs]\n",
    "    y_left=y_sorted[:min_obs]\n",
    "    n_obs_left=min_obs\n",
    "    somme_x_left=np.sum(x_left)\n",
    "    somme_y_left=np.sum(y_left)\n",
    "    somme_x2_left=np.sum(x_left**2)\n",
    "    somme_xy_left=np.dot(x_left, y_left)\n",
    "\n",
    "    cov_left=(k*somme_y_left-somme_xy_left)\n",
    "    ecart_x2_left=((n_obs_left*(k**2))-(2*k*somme_x_left)+somme_x2_left)\n",
    "    beta_left=cov_left/ecart_x2_left\n",
    "    erreur_left=somme_y2-(2*beta_left*cov_left)+(beta_left**2)*ecart_x2_left\n",
    "    erreur_min=erreur_left\n",
    "    beta_min=beta_left\n",
    "    side=\"left\"\n",
    "    k_min=k\n",
    "\n",
    "\n",
    "\n",
    "    somme_y_right=somme_y-somme_y_left\n",
    "    somme_x_right=somme_x-somme_x_left\n",
    "    somme_x2_right=somme_x2-somme_x2_left\n",
    "    somme_xy_right=somme_xy-somme_xy_left\n",
    "    n_obs_right=n-n_obs_left\n",
    "    cov_right=(somme_xy_right-k*somme_y_right)\n",
    "    ecart_x2_right=((n_obs_right*(k**2))-(2*k*somme_x_right)+somme_x2_right)\n",
    "    beta_right=cov_right/ecart_x2_right\n",
    "    erreur_right=somme_y2-(2*beta_right*cov_right)+(beta_right**2)*ecart_x2_right\n",
    "    if erreur_right<erreur_min:\n",
    "        erreur_min=erreur_right\n",
    "        beta_min=beta_right\n",
    "        side=\"right\"\n",
    "        k_min=k\n",
    "\n",
    "\n",
    "    for i in range(min_obs+1, n-min_obs, 1):\n",
    "        k=(x_sorted[i-1]+x_sorted[i])/2\n",
    "        somme_x_left+=x_sorted[i-1]\n",
    "        somme_y_left+=y_sorted[i-1]\n",
    "        somme_x2_left+=x_sorted[i-1]**2\n",
    "        somme_xy_left+=y_sorted[i-1]*x_sorted[i-1]\n",
    "        n_obs_left+=1\n",
    "        cov_left=(k*somme_y_left-somme_xy_left)\n",
    "        ecart_x2_left=((n_obs_left*(k**2))-(2*k*somme_x_left)+somme_x2_left)\n",
    "        beta_left=cov_left/ecart_x2_left\n",
    "        erreur_left=somme_y2-(2*beta_left*cov_left)+(beta_left**2)*ecart_x2_left\n",
    "        if erreur_left<erreur_min:\n",
    "            erreur_min=erreur_left\n",
    "            beta_min=beta_left\n",
    "            side=\"left\"\n",
    "            k_min=k\n",
    "\n",
    "\n",
    "        somme_y_right=somme_y-somme_y_left\n",
    "        somme_x_right=somme_x-somme_x_left\n",
    "        somme_x2_right=somme_x2-somme_x2_left\n",
    "        somme_xy_right=somme_xy-somme_xy_left\n",
    "        n_obs_right=n-n_obs_left\n",
    "        cov_right=(somme_xy_right-k*somme_y_right)\n",
    "        ecart_x2_right=((n_obs_right*(k**2))-(2*k*somme_x_right)+somme_x2_right)\n",
    "        beta_right=cov_right/ecart_x2_right\n",
    "        erreur_right=somme_y2-(2*beta_right*cov_right)+(beta_right**2)*ecart_x2_right\n",
    "        if erreur_right<erreur_min:\n",
    "            erreur_min=erreur_right\n",
    "            beta_min=beta_right\n",
    "            side=\"right\"\n",
    "            k_min=k\n",
    "    \n",
    "    if side==\"left\":\n",
    "        y_pred=np.where(x<=k_min, beta_min*(k_min-x), 0)\n",
    "    else : \n",
    "        y_pred=np.where(x>=k_min, beta_min*(x-k_min), 0)\n",
    "        \n",
    "    return (beta_min, k_min, side, erreur_min, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "141945d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajouter poids\n",
    "#voir pour optimiser et ne pas recalculer \n",
    "\n",
    "def best_relu_brent(x, y, min_obs, w=None):\n",
    "    x_sorted= np.sort(x)\n",
    "    x_min=x_sorted[min_obs-1]\n",
    "    x_max=x_sorted[n-min_obs]\n",
    "    \n",
    "    def objective_right(k):\n",
    "        z=np.maximum(x-k, 0)\n",
    "        beta=np.dot(y,z)/np.sum(z**2)\n",
    "        r = np.sum((y-beta*z)**2)\n",
    "        return r\n",
    "    \n",
    "    def objective_left(k):\n",
    "        z=np.maximum(k-x, 0)\n",
    "        beta=np.dot(y,z)/np.sum(z**2)\n",
    "        r = np.sum((y-beta*z)**2)\n",
    "        return r\n",
    "    \n",
    "    result_left = minimize_scalar(objective_left, bounds=(x_min, x_max), method='bounded')\n",
    "    result_right = minimize_scalar(objective_right, bounds=(x_min, x_max), method='bounded')\n",
    "    if result_left.fun < result_right.fun:\n",
    "        z=np.maximum(result_left.x-x, 0)\n",
    "        beta_min=np.dot(y,z)/np.sum(z**2)\n",
    "        y_pred=beta_min*z\n",
    "        return (beta_min, result_left.x, \"left\", result_left.fun, y_pred)\n",
    "    else : \n",
    "        z=np.maximum(x-result_right.x, 0)\n",
    "        beta_min=np.dot(y,z)/np.sum(z**2)\n",
    "        y_pred=beta_min*z\n",
    "        return (beta_min, result_right.x, \"right\", result_right.fun, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "022418d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComponentWiseBoostingRegressor:\n",
    "    \n",
    "    \"\"\"\n",
    "    component-wise ReLu boosting regression (avec feature quanti)\n",
    "\n",
    "    deg_inter : entier indiquant le degre d'interaction entre les variables. \n",
    "\n",
    "    form_inter : string indiquant le type d'interaction. uniquement valable si deg_inter>0\n",
    "                 - 'ind' : indicatrice \n",
    "                 - 'mult' : multiplication\n",
    "\n",
    "    step_bef_inter : entier indiquant le nombre d'étapes à considérer avant d'inclure des intéractions dans le modèle.\n",
    "                     uniquement valable si deg_inter>0\n",
    "\n",
    "    min_obs_bin : entier indiquant le nombre minimal d'observations pour effectuer une regression\n",
    "\n",
    "    strat_k : \n",
    "             - None indiquant que l'on va chercher tous les bins de taille d'au moins min_obs_bin observations\n",
    "             - entier indiquant .........\n",
    "             - 'brent' indiquant que l'on va chercher via l'algo de brent\n",
    "\n",
    "    w : \n",
    "        - None indiquant que les individus ont le meme poids\n",
    "        - array contenant les poids des individus \n",
    "\n",
    "    mode_learning_rate : string indiquant le type de pas \n",
    "                        - 'fixe' indiquant que l'utilise un pas fixe \n",
    "                        - 'steepest' indiquant que celui ci sera estime a chaque iteration\n",
    "    \n",
    "    fix_learning_rate : float indiquant la taille du pas. Uniquement utilisable si mode_learning_rate='fixe'\n",
    "\n",
    "\n",
    "    loss : string indiquant la loss a utiliser. \n",
    "           - 'sse' : ...,\n",
    "           - 'sae' : ......, \n",
    "           - 'huber' : ....,\n",
    "           \n",
    "    delta_huber : uniquement valable si loss='huber'\n",
    "                  - (\"fixe\", x) : tuple (string, float) indiquant que l'on utilise un delta fixe=x\n",
    "                  - (\"z\", x) : tuple (string, float) indiquant que l'on utilise un delta = x*std\n",
    "                  - (\"quantile\", x) : tuple (string, float) indiquant que l'on utilise un delta variable = au x quantile\n",
    "\n",
    "\n",
    "    stop : tuple indiquant la regle d'arret.\n",
    "           - ('m', x) : tuple (string, int) indiquant que l'on arrete apres x etapes\n",
    "           - ('info', x) : tuple (string, float) indiquant que l'on arrete lorsque la critere d'info augmente. \n",
    "                           x represente la penalite du nombre de parametre\n",
    "           - ('grad', x) : tuple (string, float) indiquant que l'on arrete lorsque la gradient est inferieur a x\n",
    "           - ('dif_loss', x) : tuple (string, float) indiquant que l'on arrete lorsque la difference de loss est inferieur a x\n",
    "\n",
    "\n",
    "    moment : booleen indiquant si l'on utilise le boosting avec moment \n",
    "    \n",
    "    verbose : boolean indiquant si l'on affiche un resume de l'avance\n",
    "\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stop=('m', 50), mode_learning_rate=\"fixe\", fix_learning_rate=0.1, loss=\"sse\",\n",
    "                 min_obs_bin=50, deg_inter=0, moment=False, strat_k=None, form_inter=\"ind\", step_bef_inter=0,\n",
    "                 delta_huber=(\"fixe\", 3), w=None, verbose=True):\n",
    "        \n",
    "        self.stop = stop\n",
    "        \n",
    "        self.mode_learning_rate=mode_learning_rate\n",
    "        \n",
    "        if mode_learning_rate==\"fixe\":\n",
    "            self.learning_rate = fix_learning_rate\n",
    "        else : \n",
    "            self.learning_rate=0\n",
    "\n",
    "        self.w = w\n",
    "        self.loss_func = Loss(loss)\n",
    "        self.min_obs_bin = min_obs_bin\n",
    "        self.deg_inter = deg_inter\n",
    "        self.moment = moment\n",
    "        self.strat_k=strat_k\n",
    "        \n",
    "        self.verbose=verbose\n",
    "        \n",
    "        \n",
    "        self.intercept=0\n",
    "        #ajouter opti intercept lors de la recherche du meilleur modele\n",
    "        self.models = dict()\n",
    "        #structure dict model : indice feature -> side -> k : beta\n",
    "        \n",
    "        \n",
    "        #temporaire\n",
    "        self.pred=None\n",
    "    \n",
    "        \n",
    "    def best_model(self, X, res, y, y_pred_actu):\n",
    "        best_score = float('inf')\n",
    "        best_beta, best_k, best_side, best_y_pred, best_feature = None, None, None, None, None\n",
    "        \n",
    "        #paralleliser\n",
    "        for j in range(0, X.shape[1], 1):\n",
    "            if self.strat_k is None : \n",
    "                temp_beta, temp_k, temp_side, temp_erreur, temp_y_pred = best_relu_greedy(X[:,j], res, self.min_obs_bin, self.w)\n",
    "            elif self.strat_k==\"brent\" : \n",
    "                temp_beta, temp_k, temp_side, temp_erreur, temp_y_pred = best_relu_brent(X[:,j], res, self.min_obs_bin, self.w)\n",
    "            else : \n",
    "                #ajouter methode bins\n",
    "                return None\n",
    "            if temp_erreur < best_score:\n",
    "                best_score = temp_erreur\n",
    "                best_beta, best_k, best_side, best_y_pred = temp_beta, temp_k, temp_side, temp_y_pred\n",
    "                best_feature = j\n",
    "                \n",
    "        #ajouter intercept dans choix meilleur modele\n",
    "        #ajouter interaction dans choix meilleur modele\n",
    "                \n",
    "        if self.mode_learning_rate==\"steepest\": \n",
    "            self.learning_rate=self.loss_func.opti_slope(y-y_pred_actu, best_y_pred, self.w)\n",
    "                 \n",
    "        return (best_beta, best_k, best_side, best_y_pred, best_feature)\n",
    "                 \n",
    "    def update_model_new_res(self, best_beta, best_k, best_side, best_y_pred, best_feature, y, y_pred_actu):\n",
    "        if self.models.get(best_feature):\n",
    "            if self.models[best_feature].get(best_side):\n",
    "                if self.models[best_feature][best_side].get(best_k):\n",
    "                    self.models[best_feature][best_side][best_k]+=(self.learning_rate*best_beta)\n",
    "                else : \n",
    "                    self.models[best_feature][best_side][best_k]=(self.learning_rate*best_beta)\n",
    "            else :\n",
    "                self.models[best_feature][best_side]={best_k : self.learning_rate*best_beta}\n",
    "        else : \n",
    "            self.models[best_feature]={best_side : {best_k : self.learning_rate*best_beta}}\n",
    "        return (y_pred_actu+self.learning_rate*best_y_pred,\n",
    "                self.loss_func.gradient(y-y_pred_actu, self.learning_rate*best_y_pred, self.w))\n",
    "        \n",
    "      \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.models = dict()\n",
    "          \n",
    "        y_pred_actu=np.zeros((X.shape[0],))\n",
    "        residuals = self.loss_func.gradient(y, y_pred_actu, self.w)\n",
    "        if self.stop[0]=='m':\n",
    "            for i in range(0,self.stop[1],1):\n",
    "                if self.verbose : \n",
    "                    print(10*'-', i, 10*'-')\n",
    "                best_beta, best_k, best_side, best_y_pred, best_feature = self.best_model(X, residuals, y, y_pred_actu)\n",
    "                y_pred_actu, residuals=self.update_model_new_res(best_beta, best_k, best_side, best_y_pred, best_feature, y, y_pred_actu)\n",
    "            self.pred=y_pred_actu\n",
    "        elif self.stop[0]=='info':\n",
    "            nbr_param=0\n",
    "            prec_loss=self.loss_func.loss(y, y_pred_actu, self.w)\n",
    "            prec_info = self.stop[1]*nbr_param+np.log(prec_loss)\n",
    "            best_beta, best_k, best_side, best_y_pred, best_feature = self.best_model(X, residuals, y, y_pred_actu)\n",
    "            nbr_param+=1\n",
    "            new_loss=self.loss_func.loss(y-y_pred_actu, self.learning_rate*best_y_pred, self.w)\n",
    "            new_info = self.stop[1]*nbr_param+np.log(new_loss)\n",
    "            if self.verbose :\n",
    "                print(10*'-', nbr_param, 10*'-')\n",
    "                print(\"modele \", nbr_param-1, \" parametres / loss = \", prec_loss, \" / info = \", prec_info)\n",
    "                print(\"modele \", nbr_param, \" parametres / loss = \", new_loss, \" / info = \", new_info)\n",
    "                print('\\n')\n",
    "            while new_info<prec_info:\n",
    "                y_pred_actu, residuals=self.update_model_new_res(best_beta, best_k, best_side, best_y_pred, best_feature, y, y_pred_actu)\n",
    "                prec_loss=new_loss\n",
    "                prec_info=new_info\n",
    "                best_beta, best_k, best_side, best_y_pred, best_feature = self.best_model(X, residuals,  y, y_pred_actu)\n",
    "                nbr_param+=1\n",
    "                new_loss=self.loss_func.loss(y-y_pred_actu, self.learning_rate*best_y_pred, self.w)\n",
    "                new_info=self.stop[1]*nbr_param-np.log(new_loss)\n",
    "                if self.verbose: \n",
    "                    print(10*'-', nbr_param, 10*'-')\n",
    "                    print(\"modele \", nbr_param-1, \" parametres / loss = \", prec_loss, \" / info = \", prec_info)\n",
    "                    print(\"modele \", nbr_param, \" parametres / loss = \", new_loss, \" / info = \", new_info)\n",
    "                    print('\\n')\n",
    "                \n",
    "        elif self.stop[0]=='grad' : \n",
    "            best_beta, best_k, best_side, best_y_pred, best_feature = self.best_model(X, residuals, y, y_pred_actu)\n",
    "            grad=np.max(np.abs(self.learning_rate*best_y_pred))\n",
    "            if self.verbose : \n",
    "                nbr_etape=1\n",
    "                print(10*'-', nbr_etape, 10*'-')\n",
    "                print(grad)\n",
    "                print('\\n')\n",
    "                nbr_etape+=1\n",
    "            while grad>self.stop[1]:\n",
    "                y_pred_actu, residuals=self.update_model_new_res(best_beta, best_k, best_side, best_y_pred, best_feature, y, y_pred_actu)\n",
    "                best_beta, best_k, best_side, best_y_pred, best_feature = self.best_model(X, residuals, y, y_pred_actu)\n",
    "                grad=np.max(np.abs(self.learning_rate*best_y_pred))\n",
    "                if self.verbose : \n",
    "                    print(10*'-', nbr_etape, 10*'-')\n",
    "                    print(grad)\n",
    "                    print('\\n')\n",
    "                    nbr_etape+=1\n",
    "            self.pred=y_pred_actu\n",
    "                \n",
    "        elif self.stop[0]=='dif_loss':\n",
    "            prec_loss = self.loss_func.loss(y, y_pred_actu, self.w)\n",
    "            best_beta, best_k, best_side, best_y_pred, best_feature = self.best_model(X, residuals, y, y_pred_actu)\n",
    "            new_loss = self.loss_func.loss(y-y_pred_actu, self.learning_rate*best_y_pred, self.w)\n",
    "            if self.verbose :\n",
    "                etape=1\n",
    "                print(10*'-', etape, 10*'-')\n",
    "                print(\"modele \", etape-1, \" loss = \", prec_loss)\n",
    "                print(\"modele \", etape, \" loss = \", new_loss)\n",
    "                print('\\n')\n",
    "                etape+=1\n",
    "            while (prec_loss-new_loss)>self.stop[1]:\n",
    "                y_pred_actu, residuals=self.update_model_new_res(best_beta, best_k, best_side, best_y_pred, best_feature, y, y_pred_actu)\n",
    "                prec_loss=new_loss\n",
    "                best_beta, best_k, best_side, best_y_pred, best_feature = self.best_model(X, residuals, y, y_pred_actu)\n",
    "                new_loss=self.loss_func.loss(y-y_pred_actu, self.learning_rate*best_y_pred, self.w)\n",
    "                if self.verbose :\n",
    "                    print(10*'-', etape, 10*'-')\n",
    "                    print(\"modele \", etape-1, \" loss = \", prec_loss)\n",
    "                    print(\"modele \", etape, \" loss = \", new_loss)\n",
    "                    print('\\n')\n",
    "                    etape+=1\n",
    "            self.pred=y_pred_actu\n",
    "                    \n",
    "                    \n",
    "    def reecriture(self):\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416c4397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa75727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "fb119b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10000\n",
    "min_obs=int(np.ceil(np.sqrt(n)))\n",
    "x=np.random.uniform(-10,10,(n,4))\n",
    "y=(x[:,0]**2+np.sin(x[:,1]))*x[:,2]+np.random.normal(0,2,(n,))\n",
    "# y=np.sin(x)+np.random.normal(0,0.1,(n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "9d0561ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=ComponentWiseBoostingRegressor(stop=('dif_loss', 0.01), mode_learning_rate=\"steepest\",loss=\"sae\",\n",
    "                 min_obs_bin=min_obs, deg_inter=0, moment=False, strat_k='brent', form_inter=\"ind\", step_bef_inter=0,\n",
    "                 delta_huber=(\"fixe\", 3), w=None, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "56ca2fea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 1 ----------\n",
      "modele  0  loss =  1683784.4326964032\n",
      "modele  1  loss =  1488006.6407288967\n",
      "\n",
      "\n",
      "---------- 2 ----------\n",
      "modele  1  loss =  1488006.6407288967\n",
      "modele  2  loss =  1297991.6157464003\n",
      "\n",
      "\n",
      "---------- 3 ----------\n",
      "modele  2  loss =  1297991.6157464003\n",
      "modele  3  loss =  1278094.8584237965\n",
      "\n",
      "\n",
      "---------- 4 ----------\n",
      "modele  3  loss =  1278094.8584237965\n",
      "modele  4  loss =  1253496.1562393175\n",
      "\n",
      "\n",
      "---------- 5 ----------\n",
      "modele  4  loss =  1253496.1562393175\n",
      "modele  5  loss =  1253049.5851110306\n",
      "\n",
      "\n",
      "---------- 6 ----------\n",
      "modele  5  loss =  1253049.5851110306\n",
      "modele  6  loss =  1252004.6089818198\n",
      "\n",
      "\n",
      "---------- 7 ----------\n",
      "modele  6  loss =  1252004.6089818198\n",
      "modele  7  loss =  1251805.573997929\n",
      "\n",
      "\n",
      "---------- 8 ----------\n",
      "modele  7  loss =  1251805.573997929\n",
      "modele  8  loss =  1251713.2446756482\n",
      "\n",
      "\n",
      "---------- 9 ----------\n",
      "modele  8  loss =  1251713.2446756482\n",
      "modele  9  loss =  1251449.8003764881\n",
      "\n",
      "\n",
      "---------- 10 ----------\n",
      "modele  9  loss =  1251449.8003764881\n",
      "modele  10  loss =  1251321.461433594\n",
      "\n",
      "\n",
      "---------- 11 ----------\n",
      "modele  10  loss =  1251321.461433594\n",
      "modele  11  loss =  1250669.2790697624\n",
      "\n",
      "\n",
      "---------- 12 ----------\n",
      "modele  11  loss =  1250669.2790697624\n",
      "modele  12  loss =  1250385.7265364984\n",
      "\n",
      "\n",
      "---------- 13 ----------\n",
      "modele  12  loss =  1250385.7265364984\n",
      "modele  13  loss =  1250205.1267300635\n",
      "\n",
      "\n",
      "---------- 14 ----------\n",
      "modele  13  loss =  1250205.1267300635\n",
      "modele  14  loss =  1250181.0467437475\n",
      "\n",
      "\n",
      "---------- 15 ----------\n",
      "modele  14  loss =  1250181.0467437475\n",
      "modele  15  loss =  1250159.4927679093\n",
      "\n",
      "\n",
      "---------- 16 ----------\n",
      "modele  15  loss =  1250159.4927679093\n",
      "modele  16  loss =  1250124.9642737783\n",
      "\n",
      "\n",
      "---------- 17 ----------\n",
      "modele  16  loss =  1250124.9642737783\n",
      "modele  17  loss =  1250035.274560457\n",
      "\n",
      "\n",
      "---------- 18 ----------\n",
      "modele  17  loss =  1250035.274560457\n",
      "modele  18  loss =  1249981.8071757064\n",
      "\n",
      "\n",
      "---------- 19 ----------\n",
      "modele  18  loss =  1249981.8071757064\n",
      "modele  19  loss =  1249951.890799067\n",
      "\n",
      "\n",
      "---------- 20 ----------\n",
      "modele  19  loss =  1249951.890799067\n",
      "modele  20  loss =  1249937.6973961645\n",
      "\n",
      "\n",
      "---------- 21 ----------\n",
      "modele  20  loss =  1249937.6973961645\n",
      "modele  21  loss =  1249931.1462372593\n",
      "\n",
      "\n",
      "---------- 22 ----------\n",
      "modele  21  loss =  1249931.1462372593\n",
      "modele  22  loss =  1249918.1436555765\n",
      "\n",
      "\n",
      "---------- 23 ----------\n",
      "modele  22  loss =  1249918.1436555765\n",
      "modele  23  loss =  1249908.6531963244\n",
      "\n",
      "\n",
      "---------- 24 ----------\n",
      "modele  23  loss =  1249908.6531963244\n",
      "modele  24  loss =  1249902.5671065617\n",
      "\n",
      "\n",
      "---------- 25 ----------\n",
      "modele  24  loss =  1249902.5671065617\n",
      "modele  25  loss =  1249495.8048152053\n",
      "\n",
      "\n",
      "---------- 26 ----------\n",
      "modele  25  loss =  1249495.8048152053\n",
      "modele  26  loss =  1249404.7415617264\n",
      "\n",
      "\n",
      "---------- 27 ----------\n",
      "modele  26  loss =  1249404.7415617264\n",
      "modele  27  loss =  1249370.2934888545\n",
      "\n",
      "\n",
      "---------- 28 ----------\n",
      "modele  27  loss =  1249370.2934888545\n",
      "modele  28  loss =  1249334.957630181\n",
      "\n",
      "\n",
      "---------- 29 ----------\n",
      "modele  28  loss =  1249334.957630181\n",
      "modele  29  loss =  1249299.2530359332\n",
      "\n",
      "\n",
      "---------- 30 ----------\n",
      "modele  29  loss =  1249299.2530359332\n",
      "modele  30  loss =  1249270.7816266152\n",
      "\n",
      "\n",
      "---------- 31 ----------\n",
      "modele  30  loss =  1249270.7816266152\n",
      "modele  31  loss =  1249245.7899521482\n",
      "\n",
      "\n",
      "---------- 32 ----------\n",
      "modele  31  loss =  1249245.7899521482\n",
      "modele  32  loss =  1249207.674580849\n",
      "\n",
      "\n",
      "---------- 33 ----------\n",
      "modele  32  loss =  1249207.674580849\n",
      "modele  33  loss =  1249143.7439382551\n",
      "\n",
      "\n",
      "---------- 34 ----------\n",
      "modele  33  loss =  1249143.7439382551\n",
      "modele  34  loss =  1249136.1390818171\n",
      "\n",
      "\n",
      "---------- 35 ----------\n",
      "modele  34  loss =  1249136.1390818171\n",
      "modele  35  loss =  1249124.2406072905\n",
      "\n",
      "\n",
      "---------- 36 ----------\n",
      "modele  35  loss =  1249124.2406072905\n",
      "modele  36  loss =  1249083.9109089146\n",
      "\n",
      "\n",
      "---------- 37 ----------\n",
      "modele  36  loss =  1249083.9109089146\n",
      "modele  37  loss =  1249040.7317448223\n",
      "\n",
      "\n",
      "---------- 38 ----------\n",
      "modele  37  loss =  1249040.7317448223\n",
      "modele  38  loss =  1249022.6875520113\n",
      "\n",
      "\n",
      "---------- 39 ----------\n",
      "modele  38  loss =  1249022.6875520113\n",
      "modele  39  loss =  1249011.0249661314\n",
      "\n",
      "\n",
      "---------- 40 ----------\n",
      "modele  39  loss =  1249011.0249661314\n",
      "modele  40  loss =  1249002.4581205253\n",
      "\n",
      "\n",
      "---------- 41 ----------\n",
      "modele  40  loss =  1249002.4581205253\n",
      "modele  41  loss =  1248973.8433423792\n",
      "\n",
      "\n",
      "---------- 42 ----------\n",
      "modele  41  loss =  1248973.8433423792\n",
      "modele  42  loss =  1248966.4172837879\n",
      "\n",
      "\n",
      "---------- 43 ----------\n",
      "modele  42  loss =  1248966.4172837879\n",
      "modele  43  loss =  1248941.2577924738\n",
      "\n",
      "\n",
      "---------- 44 ----------\n",
      "modele  43  loss =  1248941.2577924738\n",
      "modele  44  loss =  1248925.3054095134\n",
      "\n",
      "\n",
      "---------- 45 ----------\n",
      "modele  44  loss =  1248925.3054095134\n",
      "modele  45  loss =  1248909.3921459895\n",
      "\n",
      "\n",
      "---------- 46 ----------\n",
      "modele  45  loss =  1248909.3921459895\n",
      "modele  46  loss =  1248900.1932549952\n",
      "\n",
      "\n",
      "---------- 47 ----------\n",
      "modele  46  loss =  1248900.1932549952\n",
      "modele  47  loss =  1248897.6218224578\n",
      "\n",
      "\n",
      "---------- 48 ----------\n",
      "modele  47  loss =  1248897.6218224578\n",
      "modele  48  loss =  1248884.3662972532\n",
      "\n",
      "\n",
      "---------- 49 ----------\n",
      "modele  48  loss =  1248884.3662972532\n",
      "modele  49  loss =  1248880.8683211943\n",
      "\n",
      "\n",
      "---------- 50 ----------\n",
      "modele  49  loss =  1248880.8683211943\n",
      "modele  50  loss =  1248878.475384988\n",
      "\n",
      "\n",
      "---------- 51 ----------\n",
      "modele  50  loss =  1248878.475384988\n",
      "modele  51  loss =  1248867.8037055752\n",
      "\n",
      "\n",
      "---------- 52 ----------\n",
      "modele  51  loss =  1248867.8037055752\n",
      "modele  52  loss =  1248859.6803240203\n",
      "\n",
      "\n",
      "---------- 53 ----------\n",
      "modele  52  loss =  1248859.6803240203\n",
      "modele  53  loss =  1248856.2282314294\n",
      "\n",
      "\n",
      "---------- 54 ----------\n",
      "modele  53  loss =  1248856.2282314294\n",
      "modele  54  loss =  1248846.0682911149\n",
      "\n",
      "\n",
      "---------- 55 ----------\n",
      "modele  54  loss =  1248846.0682911149\n",
      "modele  55  loss =  1248835.8260703126\n",
      "\n",
      "\n",
      "---------- 56 ----------\n",
      "modele  55  loss =  1248835.8260703126\n",
      "modele  56  loss =  1248831.9066877735\n",
      "\n",
      "\n",
      "---------- 57 ----------\n",
      "modele  56  loss =  1248831.9066877735\n",
      "modele  57  loss =  1248826.4506176948\n",
      "\n",
      "\n",
      "---------- 58 ----------\n",
      "modele  57  loss =  1248826.4506176948\n",
      "modele  58  loss =  1248818.4667431694\n",
      "\n",
      "\n",
      "---------- 59 ----------\n",
      "modele  58  loss =  1248818.4667431694\n",
      "modele  59  loss =  1248814.4330142587\n",
      "\n",
      "\n",
      "---------- 60 ----------\n",
      "modele  59  loss =  1248814.4330142587\n",
      "modele  60  loss =  1248808.210565346\n",
      "\n",
      "\n",
      "---------- 61 ----------\n",
      "modele  60  loss =  1248808.210565346\n",
      "modele  61  loss =  1248802.9087244563\n",
      "\n",
      "\n",
      "---------- 62 ----------\n",
      "modele  61  loss =  1248802.9087244563\n",
      "modele  62  loss =  1248801.1507476957\n",
      "\n",
      "\n",
      "---------- 63 ----------\n",
      "modele  62  loss =  1248801.1507476957\n",
      "modele  63  loss =  1248646.0524640696\n",
      "\n",
      "\n",
      "---------- 64 ----------\n",
      "modele  63  loss =  1248646.0524640696\n",
      "modele  64  loss =  1248628.795770579\n",
      "\n",
      "\n",
      "---------- 65 ----------\n",
      "modele  64  loss =  1248628.795770579\n",
      "modele  65  loss =  1248618.855745688\n",
      "\n",
      "\n",
      "---------- 66 ----------\n",
      "modele  65  loss =  1248618.855745688\n",
      "modele  66  loss =  1248537.5693966046\n",
      "\n",
      "\n",
      "---------- 67 ----------\n",
      "modele  66  loss =  1248537.5693966046\n",
      "modele  67  loss =  1248526.7821490406\n",
      "\n",
      "\n",
      "---------- 68 ----------\n",
      "modele  67  loss =  1248526.7821490406\n",
      "modele  68  loss =  1248519.236413669\n",
      "\n",
      "\n",
      "---------- 69 ----------\n",
      "modele  68  loss =  1248519.236413669\n",
      "modele  69  loss =  1248512.7172966981\n",
      "\n",
      "\n",
      "---------- 70 ----------\n",
      "modele  69  loss =  1248512.7172966981\n",
      "modele  70  loss =  1248502.1816558552\n",
      "\n",
      "\n",
      "---------- 71 ----------\n",
      "modele  70  loss =  1248502.1816558552\n",
      "modele  71  loss =  1248501.3556995927\n",
      "\n",
      "\n",
      "---------- 72 ----------\n",
      "modele  71  loss =  1248501.3556995927\n",
      "modele  72  loss =  1248500.6348097776\n",
      "\n",
      "\n",
      "---------- 73 ----------\n",
      "modele  72  loss =  1248500.6348097776\n",
      "modele  73  loss =  1248499.2741227276\n",
      "\n",
      "\n",
      "---------- 74 ----------\n",
      "modele  73  loss =  1248499.2741227276\n",
      "modele  74  loss =  1248498.1225856526\n",
      "\n",
      "\n",
      "---------- 75 ----------\n",
      "modele  74  loss =  1248498.1225856526\n",
      "modele  75  loss =  1248496.2286233846\n",
      "\n",
      "\n",
      "---------- 76 ----------\n",
      "modele  75  loss =  1248496.2286233846\n",
      "modele  76  loss =  1248492.583414485\n",
      "\n",
      "\n",
      "---------- 77 ----------\n",
      "modele  76  loss =  1248492.583414485\n",
      "modele  77  loss =  1248489.658717\n",
      "\n",
      "\n",
      "---------- 78 ----------\n",
      "modele  77  loss =  1248489.658717\n",
      "modele  78  loss =  1248486.9553510535\n",
      "\n",
      "\n",
      "---------- 79 ----------\n",
      "modele  78  loss =  1248486.9553510535\n",
      "modele  79  loss =  1248475.3897140282\n",
      "\n",
      "\n",
      "---------- 80 ----------\n",
      "modele  79  loss =  1248475.3897140282\n",
      "modele  80  loss =  1248473.3734130654\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 81 ----------\n",
      "modele  80  loss =  1248473.3734130654\n",
      "modele  81  loss =  1248465.4113292596\n",
      "\n",
      "\n",
      "---------- 82 ----------\n",
      "modele  81  loss =  1248465.4113292596\n",
      "modele  82  loss =  1248463.5818237\n",
      "\n",
      "\n",
      "---------- 83 ----------\n",
      "modele  82  loss =  1248463.5818237\n",
      "modele  83  loss =  1248458.8466318324\n",
      "\n",
      "\n",
      "---------- 84 ----------\n",
      "modele  83  loss =  1248458.8466318324\n",
      "modele  84  loss =  1248455.6001303727\n",
      "\n",
      "\n",
      "---------- 85 ----------\n",
      "modele  84  loss =  1248455.6001303727\n",
      "modele  85  loss =  1248454.3297409439\n",
      "\n",
      "\n",
      "---------- 86 ----------\n",
      "modele  85  loss =  1248454.3297409439\n",
      "modele  86  loss =  1248449.515885652\n",
      "\n",
      "\n",
      "---------- 87 ----------\n",
      "modele  86  loss =  1248449.515885652\n",
      "modele  87  loss =  1248447.1067931806\n",
      "\n",
      "\n",
      "---------- 88 ----------\n",
      "modele  87  loss =  1248447.1067931806\n",
      "modele  88  loss =  1248445.5435644104\n",
      "\n",
      "\n",
      "---------- 89 ----------\n",
      "modele  88  loss =  1248445.5435644104\n",
      "modele  89  loss =  1248443.2297882498\n",
      "\n",
      "\n",
      "---------- 90 ----------\n",
      "modele  89  loss =  1248443.2297882498\n",
      "modele  90  loss =  1248441.4805874943\n",
      "\n",
      "\n",
      "---------- 91 ----------\n",
      "modele  90  loss =  1248441.4805874943\n",
      "modele  91  loss =  1248438.8391085735\n",
      "\n",
      "\n",
      "---------- 92 ----------\n",
      "modele  91  loss =  1248438.8391085735\n",
      "modele  92  loss =  1248433.2661553696\n",
      "\n",
      "\n",
      "---------- 93 ----------\n",
      "modele  92  loss =  1248433.2661553696\n",
      "modele  93  loss =  1248432.8436896394\n",
      "\n",
      "\n",
      "---------- 94 ----------\n",
      "modele  93  loss =  1248432.8436896394\n",
      "modele  94  loss =  1248431.4191012331\n",
      "\n",
      "\n",
      "---------- 95 ----------\n",
      "modele  94  loss =  1248431.4191012331\n",
      "modele  95  loss =  1248423.9957705385\n",
      "\n",
      "\n",
      "---------- 96 ----------\n",
      "modele  95  loss =  1248423.9957705385\n",
      "modele  96  loss =  1248422.2564439322\n",
      "\n",
      "\n",
      "---------- 97 ----------\n",
      "modele  96  loss =  1248422.2564439322\n",
      "modele  97  loss =  1248416.904268721\n",
      "\n",
      "\n",
      "---------- 98 ----------\n",
      "modele  97  loss =  1248416.904268721\n",
      "modele  98  loss =  1248415.7034533466\n",
      "\n",
      "\n",
      "---------- 99 ----------\n",
      "modele  98  loss =  1248415.7034533466\n",
      "modele  99  loss =  1248414.2071685859\n",
      "\n",
      "\n",
      "---------- 100 ----------\n",
      "modele  99  loss =  1248414.2071685859\n",
      "modele  100  loss =  1248411.0562122576\n",
      "\n",
      "\n",
      "---------- 101 ----------\n",
      "modele  100  loss =  1248411.0562122576\n",
      "modele  101  loss =  1248406.590707578\n",
      "\n",
      "\n",
      "---------- 102 ----------\n",
      "modele  101  loss =  1248406.590707578\n",
      "modele  102  loss =  1248405.1263564026\n",
      "\n",
      "\n",
      "---------- 103 ----------\n",
      "modele  102  loss =  1248405.1263564026\n",
      "modele  103  loss =  1248404.204879594\n",
      "\n",
      "\n",
      "---------- 104 ----------\n",
      "modele  103  loss =  1248404.204879594\n",
      "modele  104  loss =  1248403.2687403678\n",
      "\n",
      "\n",
      "---------- 105 ----------\n",
      "modele  104  loss =  1248403.2687403678\n",
      "modele  105  loss =  1248401.452678184\n",
      "\n",
      "\n",
      "---------- 106 ----------\n",
      "modele  105  loss =  1248401.452678184\n",
      "modele  106  loss =  1248399.1708760378\n",
      "\n",
      "\n",
      "---------- 107 ----------\n",
      "modele  106  loss =  1248399.1708760378\n",
      "modele  107  loss =  1248398.3513495144\n",
      "\n",
      "\n",
      "---------- 108 ----------\n",
      "modele  107  loss =  1248398.3513495144\n",
      "modele  108  loss =  1248397.643986991\n",
      "\n",
      "\n",
      "---------- 109 ----------\n",
      "modele  108  loss =  1248397.643986991\n",
      "modele  109  loss =  1248394.0976630487\n",
      "\n",
      "\n",
      "---------- 110 ----------\n",
      "modele  109  loss =  1248394.0976630487\n",
      "modele  110  loss =  1248393.7721538788\n",
      "\n",
      "\n",
      "---------- 111 ----------\n",
      "modele  110  loss =  1248393.7721538788\n",
      "modele  111  loss =  1248343.8113593215\n",
      "\n",
      "\n",
      "---------- 112 ----------\n",
      "modele  111  loss =  1248343.8113593215\n",
      "modele  112  loss =  1248334.9649877464\n",
      "\n",
      "\n",
      "---------- 113 ----------\n",
      "modele  112  loss =  1248334.9649877464\n",
      "modele  113  loss =  1248334.431134702\n",
      "\n",
      "\n",
      "---------- 114 ----------\n",
      "modele  113  loss =  1248334.431134702\n",
      "modele  114  loss =  1248333.6178757027\n",
      "\n",
      "\n",
      "---------- 115 ----------\n",
      "modele  114  loss =  1248333.6178757027\n",
      "modele  115  loss =  1248332.8920463114\n",
      "\n",
      "\n",
      "---------- 116 ----------\n",
      "modele  115  loss =  1248332.8920463114\n",
      "modele  116  loss =  1248332.2287849477\n",
      "\n",
      "\n",
      "---------- 117 ----------\n",
      "modele  116  loss =  1248332.2287849477\n",
      "modele  117  loss =  1248330.1751068346\n",
      "\n",
      "\n",
      "---------- 118 ----------\n",
      "modele  117  loss =  1248330.1751068346\n",
      "modele  118  loss =  1248326.71012901\n",
      "\n",
      "\n",
      "---------- 119 ----------\n",
      "modele  118  loss =  1248326.71012901\n",
      "modele  119  loss =  1248324.5916571931\n",
      "\n",
      "\n",
      "---------- 120 ----------\n",
      "modele  119  loss =  1248324.5916571931\n",
      "modele  120  loss =  1248322.3423605235\n",
      "\n",
      "\n",
      "---------- 121 ----------\n",
      "modele  120  loss =  1248322.3423605235\n",
      "modele  121  loss =  1248321.8858125065\n",
      "\n",
      "\n",
      "---------- 122 ----------\n",
      "modele  121  loss =  1248321.8858125065\n",
      "modele  122  loss =  1248319.376971777\n",
      "\n",
      "\n",
      "---------- 123 ----------\n",
      "modele  122  loss =  1248319.376971777\n",
      "modele  123  loss =  1248318.6224899022\n",
      "\n",
      "\n",
      "---------- 124 ----------\n",
      "modele  123  loss =  1248318.6224899022\n",
      "modele  124  loss =  1248318.5091201123\n",
      "\n",
      "\n",
      "---------- 125 ----------\n",
      "modele  124  loss =  1248318.5091201123\n",
      "modele  125  loss =  1248318.3896847852\n",
      "\n",
      "\n",
      "---------- 126 ----------\n",
      "modele  125  loss =  1248318.3896847852\n",
      "modele  126  loss =  1248317.81929276\n",
      "\n",
      "\n",
      "---------- 127 ----------\n",
      "modele  126  loss =  1248317.81929276\n",
      "modele  127  loss =  1248317.1758832713\n",
      "\n",
      "\n",
      "---------- 128 ----------\n",
      "modele  127  loss =  1248317.1758832713\n",
      "modele  128  loss =  1248313.447813646\n",
      "\n",
      "\n",
      "---------- 129 ----------\n",
      "modele  128  loss =  1248313.447813646\n",
      "modele  129  loss =  1248309.114612192\n",
      "\n",
      "\n",
      "---------- 130 ----------\n",
      "modele  129  loss =  1248309.114612192\n",
      "modele  130  loss =  1248308.116936924\n",
      "\n",
      "\n",
      "---------- 131 ----------\n",
      "modele  130  loss =  1248308.116936924\n",
      "modele  131  loss =  1248305.8727686955\n",
      "\n",
      "\n",
      "---------- 132 ----------\n",
      "modele  131  loss =  1248305.8727686955\n",
      "modele  132  loss =  1248304.9515090815\n",
      "\n",
      "\n",
      "---------- 133 ----------\n",
      "modele  132  loss =  1248304.9515090815\n",
      "modele  133  loss =  1248303.5270391302\n",
      "\n",
      "\n",
      "---------- 134 ----------\n",
      "modele  133  loss =  1248303.5270391302\n",
      "modele  134  loss =  1248303.0472951701\n",
      "\n",
      "\n",
      "---------- 135 ----------\n",
      "modele  134  loss =  1248303.0472951701\n",
      "modele  135  loss =  1248302.5924569825\n",
      "\n",
      "\n",
      "---------- 136 ----------\n",
      "modele  135  loss =  1248302.5924569825\n",
      "modele  136  loss =  1248301.5904023512\n",
      "\n",
      "\n",
      "---------- 137 ----------\n",
      "modele  136  loss =  1248301.5904023512\n",
      "modele  137  loss =  1248301.1779132795\n",
      "\n",
      "\n",
      "---------- 138 ----------\n",
      "modele  137  loss =  1248301.1779132795\n",
      "modele  138  loss =  1248300.8441736728\n",
      "\n",
      "\n",
      "---------- 139 ----------\n",
      "modele  138  loss =  1248300.8441736728\n",
      "modele  139  loss =  1248300.3133533578\n",
      "\n",
      "\n",
      "---------- 140 ----------\n",
      "modele  139  loss =  1248300.3133533578\n",
      "modele  140  loss =  1248287.6838753563\n",
      "\n",
      "\n",
      "---------- 141 ----------\n",
      "modele  140  loss =  1248287.6838753563\n",
      "modele  141  loss =  1248278.8366656366\n",
      "\n",
      "\n",
      "---------- 142 ----------\n",
      "modele  141  loss =  1248278.8366656366\n",
      "modele  142  loss =  1248278.3497277105\n",
      "\n",
      "\n",
      "---------- 143 ----------\n",
      "modele  142  loss =  1248278.3497277105\n",
      "modele  143  loss =  1248278.0939357888\n",
      "\n",
      "\n",
      "---------- 144 ----------\n",
      "modele  143  loss =  1248278.0939357888\n",
      "modele  144  loss =  1248277.6780453245\n",
      "\n",
      "\n",
      "---------- 145 ----------\n",
      "modele  144  loss =  1248277.6780453245\n",
      "modele  145  loss =  1248276.0521635262\n",
      "\n",
      "\n",
      "---------- 146 ----------\n",
      "modele  145  loss =  1248276.0521635262\n",
      "modele  146  loss =  1248274.1711678477\n",
      "\n",
      "\n",
      "---------- 147 ----------\n",
      "modele  146  loss =  1248274.1711678477\n",
      "modele  147  loss =  1248273.3250035595\n",
      "\n",
      "\n",
      "---------- 148 ----------\n",
      "modele  147  loss =  1248273.3250035595\n",
      "modele  148  loss =  1248272.6429611612\n",
      "\n",
      "\n",
      "---------- 149 ----------\n",
      "modele  148  loss =  1248272.6429611612\n",
      "modele  149  loss =  1248271.9505180481\n",
      "\n",
      "\n",
      "---------- 150 ----------\n",
      "modele  149  loss =  1248271.9505180481\n",
      "modele  150  loss =  1248271.7886468705\n",
      "\n",
      "\n",
      "---------- 151 ----------\n",
      "modele  150  loss =  1248271.7886468705\n",
      "modele  151  loss =  1248271.0590331692\n",
      "\n",
      "\n",
      "---------- 152 ----------\n",
      "modele  151  loss =  1248271.0590331692\n",
      "modele  152  loss =  1248270.4968191292\n",
      "\n",
      "\n",
      "---------- 153 ----------\n",
      "modele  152  loss =  1248270.4968191292\n",
      "modele  153  loss =  1248270.046926874\n",
      "\n",
      "\n",
      "---------- 154 ----------\n",
      "modele  153  loss =  1248270.046926874\n",
      "modele  154  loss =  1248269.2223937362\n",
      "\n",
      "\n",
      "---------- 155 ----------\n",
      "modele  154  loss =  1248269.2223937362\n",
      "modele  155  loss =  1248268.882292896\n",
      "\n",
      "\n",
      "---------- 156 ----------\n",
      "modele  155  loss =  1248268.882292896\n",
      "modele  156  loss =  1248268.0392191838\n",
      "\n",
      "\n",
      "---------- 157 ----------\n",
      "modele  156  loss =  1248268.0392191838\n",
      "modele  157  loss =  1248267.2544882367\n",
      "\n",
      "\n",
      "---------- 158 ----------\n",
      "modele  157  loss =  1248267.2544882367\n",
      "modele  158  loss =  1248266.455930397\n",
      "\n",
      "\n",
      "---------- 159 ----------\n",
      "modele  158  loss =  1248266.455930397\n",
      "modele  159  loss =  1248265.6901063928\n",
      "\n",
      "\n",
      "---------- 160 ----------\n",
      "modele  159  loss =  1248265.6901063928\n",
      "modele  160  loss =  1248264.8322473327\n",
      "\n",
      "\n",
      "---------- 161 ----------\n",
      "modele  160  loss =  1248264.8322473327\n",
      "modele  161  loss =  1248264.1676235567\n",
      "\n",
      "\n",
      "---------- 162 ----------\n",
      "modele  161  loss =  1248264.1676235567\n",
      "modele  162  loss =  1248263.561713184\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 163 ----------\n",
      "modele  162  loss =  1248263.561713184\n",
      "modele  163  loss =  1248262.9786437186\n",
      "\n",
      "\n",
      "---------- 164 ----------\n",
      "modele  163  loss =  1248262.9786437186\n",
      "modele  164  loss =  1248262.4783239223\n",
      "\n",
      "\n",
      "---------- 165 ----------\n",
      "modele  164  loss =  1248262.4783239223\n",
      "modele  165  loss =  1248262.0011282102\n",
      "\n",
      "\n",
      "---------- 166 ----------\n",
      "modele  165  loss =  1248262.0011282102\n",
      "modele  166  loss =  1248261.8309033015\n",
      "\n",
      "\n",
      "---------- 167 ----------\n",
      "modele  166  loss =  1248261.8309033015\n",
      "modele  167  loss =  1248261.6530810678\n",
      "\n",
      "\n",
      "---------- 168 ----------\n",
      "modele  167  loss =  1248261.6530810678\n",
      "modele  168  loss =  1248261.4047816056\n",
      "\n",
      "\n",
      "---------- 169 ----------\n",
      "modele  168  loss =  1248261.4047816056\n",
      "modele  169  loss =  1248260.6282775863\n",
      "\n",
      "\n",
      "---------- 170 ----------\n",
      "modele  169  loss =  1248260.6282775863\n",
      "modele  170  loss =  1248260.2280173057\n",
      "\n",
      "\n",
      "---------- 171 ----------\n",
      "modele  170  loss =  1248260.2280173057\n",
      "modele  171  loss =  1248260.153677235\n",
      "\n",
      "\n",
      "---------- 172 ----------\n",
      "modele  171  loss =  1248260.153677235\n",
      "modele  172  loss =  1248257.437091765\n",
      "\n",
      "\n",
      "---------- 173 ----------\n",
      "modele  172  loss =  1248257.437091765\n",
      "modele  173  loss =  1248257.0853510632\n",
      "\n",
      "\n",
      "---------- 174 ----------\n",
      "modele  173  loss =  1248257.0853510632\n",
      "modele  174  loss =  1248255.5929437499\n",
      "\n",
      "\n",
      "---------- 175 ----------\n",
      "modele  174  loss =  1248255.5929437499\n",
      "modele  175  loss =  1248255.1624786772\n",
      "\n",
      "\n",
      "---------- 176 ----------\n",
      "modele  175  loss =  1248255.1624786772\n",
      "modele  176  loss =  1248254.1120118736\n",
      "\n",
      "\n",
      "---------- 177 ----------\n",
      "modele  176  loss =  1248254.1120118736\n",
      "modele  177  loss =  1248253.71589769\n",
      "\n",
      "\n",
      "---------- 178 ----------\n",
      "modele  177  loss =  1248253.71589769\n",
      "modele  178  loss =  1248252.7159031553\n",
      "\n",
      "\n",
      "---------- 179 ----------\n",
      "modele  178  loss =  1248252.7159031553\n",
      "modele  179  loss =  1248252.443974635\n",
      "\n",
      "\n",
      "---------- 180 ----------\n",
      "modele  179  loss =  1248252.443974635\n",
      "modele  180  loss =  1248251.9883952101\n",
      "\n",
      "\n",
      "---------- 181 ----------\n",
      "modele  180  loss =  1248251.9883952101\n",
      "modele  181  loss =  1248251.5215511767\n",
      "\n",
      "\n",
      "---------- 182 ----------\n",
      "modele  181  loss =  1248251.5215511767\n",
      "modele  182  loss =  1248251.3412699653\n",
      "\n",
      "\n",
      "---------- 183 ----------\n",
      "modele  182  loss =  1248251.3412699653\n",
      "modele  183  loss =  1248251.2245713733\n",
      "\n",
      "\n",
      "---------- 184 ----------\n",
      "modele  183  loss =  1248251.2245713733\n",
      "modele  184  loss =  1248250.9232853716\n",
      "\n",
      "\n",
      "---------- 185 ----------\n",
      "modele  184  loss =  1248250.9232853716\n",
      "modele  185  loss =  1248250.707474138\n",
      "\n",
      "\n",
      "---------- 186 ----------\n",
      "modele  185  loss =  1248250.707474138\n",
      "modele  186  loss =  1248249.5764688125\n",
      "\n",
      "\n",
      "---------- 187 ----------\n",
      "modele  186  loss =  1248249.5764688125\n",
      "modele  187  loss =  1248248.1483474288\n",
      "\n",
      "\n",
      "---------- 188 ----------\n",
      "modele  187  loss =  1248248.1483474288\n",
      "modele  188  loss =  1248247.11262292\n",
      "\n",
      "\n",
      "---------- 189 ----------\n",
      "modele  188  loss =  1248247.11262292\n",
      "modele  189  loss =  1248247.042219042\n",
      "\n",
      "\n",
      "---------- 190 ----------\n",
      "modele  189  loss =  1248247.042219042\n",
      "modele  190  loss =  1248246.9267769696\n",
      "\n",
      "\n",
      "---------- 191 ----------\n",
      "modele  190  loss =  1248246.9267769696\n",
      "modele  191  loss =  1248246.5801320332\n",
      "\n",
      "\n",
      "---------- 192 ----------\n",
      "modele  191  loss =  1248246.5801320332\n",
      "modele  192  loss =  1248246.192006855\n",
      "\n",
      "\n",
      "---------- 193 ----------\n",
      "modele  192  loss =  1248246.192006855\n",
      "modele  193  loss =  1248244.0307920862\n",
      "\n",
      "\n",
      "---------- 194 ----------\n",
      "modele  193  loss =  1248244.0307920862\n",
      "modele  194  loss =  1248243.5857428361\n",
      "\n",
      "\n",
      "---------- 195 ----------\n",
      "modele  194  loss =  1248243.5857428361\n",
      "modele  195  loss =  1248243.1513341423\n",
      "\n",
      "\n",
      "---------- 196 ----------\n",
      "modele  195  loss =  1248243.1513341423\n",
      "modele  196  loss =  1248242.630895363\n",
      "\n",
      "\n",
      "---------- 197 ----------\n",
      "modele  196  loss =  1248242.630895363\n",
      "modele  197  loss =  1248242.3692712788\n",
      "\n",
      "\n",
      "---------- 198 ----------\n",
      "modele  197  loss =  1248242.3692712788\n",
      "modele  198  loss =  1248242.03073004\n",
      "\n",
      "\n",
      "---------- 199 ----------\n",
      "modele  198  loss =  1248242.03073004\n",
      "modele  199  loss =  1248241.912643634\n",
      "\n",
      "\n",
      "---------- 200 ----------\n",
      "modele  199  loss =  1248241.912643634\n",
      "modele  200  loss =  1248240.7979000965\n",
      "\n",
      "\n",
      "---------- 201 ----------\n",
      "modele  200  loss =  1248240.7979000965\n",
      "modele  201  loss =  1248239.7535379105\n",
      "\n",
      "\n",
      "---------- 202 ----------\n",
      "modele  201  loss =  1248239.7535379105\n",
      "modele  202  loss =  1248239.2765154787\n",
      "\n",
      "\n",
      "---------- 203 ----------\n",
      "modele  202  loss =  1248239.2765154787\n",
      "modele  203  loss =  1248238.6514284364\n",
      "\n",
      "\n",
      "---------- 204 ----------\n",
      "modele  203  loss =  1248238.6514284364\n",
      "modele  204  loss =  1248237.490567775\n",
      "\n",
      "\n",
      "---------- 205 ----------\n",
      "modele  204  loss =  1248237.490567775\n",
      "modele  205  loss =  1248237.1131180786\n",
      "\n",
      "\n",
      "---------- 206 ----------\n",
      "modele  205  loss =  1248237.1131180786\n",
      "modele  206  loss =  1248236.47091585\n",
      "\n",
      "\n",
      "---------- 207 ----------\n",
      "modele  206  loss =  1248236.47091585\n",
      "modele  207  loss =  1248236.1456486431\n",
      "\n",
      "\n",
      "---------- 208 ----------\n",
      "modele  207  loss =  1248236.1456486431\n",
      "modele  208  loss =  1248236.0425857885\n",
      "\n",
      "\n",
      "---------- 209 ----------\n",
      "modele  208  loss =  1248236.0425857885\n",
      "modele  209  loss =  1248235.72152829\n",
      "\n",
      "\n",
      "---------- 210 ----------\n",
      "modele  209  loss =  1248235.72152829\n",
      "modele  210  loss =  1248234.1468752483\n",
      "\n",
      "\n",
      "---------- 211 ----------\n",
      "modele  210  loss =  1248234.1468752483\n",
      "modele  211  loss =  1248233.3839129265\n",
      "\n",
      "\n",
      "---------- 212 ----------\n",
      "modele  211  loss =  1248233.3839129265\n",
      "modele  212  loss =  1248233.1797802192\n",
      "\n",
      "\n",
      "---------- 213 ----------\n",
      "modele  212  loss =  1248233.1797802192\n",
      "modele  213  loss =  1248232.6628252736\n",
      "\n",
      "\n",
      "---------- 214 ----------\n",
      "modele  213  loss =  1248232.6628252736\n",
      "modele  214  loss =  1248231.9522526073\n",
      "\n",
      "\n",
      "---------- 215 ----------\n",
      "modele  214  loss =  1248231.9522526073\n",
      "modele  215  loss =  1248231.6835354797\n",
      "\n",
      "\n",
      "---------- 216 ----------\n",
      "modele  215  loss =  1248231.6835354797\n",
      "modele  216  loss =  1248231.3380367868\n",
      "\n",
      "\n",
      "---------- 217 ----------\n",
      "modele  216  loss =  1248231.3380367868\n",
      "modele  217  loss =  1248230.9281027303\n",
      "\n",
      "\n",
      "---------- 218 ----------\n",
      "modele  217  loss =  1248230.9281027303\n",
      "modele  218  loss =  1248230.7859063644\n",
      "\n",
      "\n",
      "---------- 219 ----------\n",
      "modele  218  loss =  1248230.7859063644\n",
      "modele  219  loss =  1248230.6263084924\n",
      "\n",
      "\n",
      "---------- 220 ----------\n",
      "modele  219  loss =  1248230.6263084924\n",
      "modele  220  loss =  1248230.1291388664\n",
      "\n",
      "\n",
      "---------- 221 ----------\n",
      "modele  220  loss =  1248230.1291388664\n",
      "modele  221  loss =  1248229.4218054558\n",
      "\n",
      "\n",
      "---------- 222 ----------\n",
      "modele  221  loss =  1248229.4218054558\n",
      "modele  222  loss =  1248228.6988535197\n",
      "\n",
      "\n",
      "---------- 223 ----------\n",
      "modele  222  loss =  1248228.6988535197\n",
      "modele  223  loss =  1248227.9925771363\n",
      "\n",
      "\n",
      "---------- 224 ----------\n",
      "modele  223  loss =  1248227.9925771363\n",
      "modele  224  loss =  1248227.6855849396\n",
      "\n",
      "\n",
      "---------- 225 ----------\n",
      "modele  224  loss =  1248227.6855849396\n",
      "modele  225  loss =  1248227.5324432377\n",
      "\n",
      "\n",
      "---------- 226 ----------\n",
      "modele  225  loss =  1248227.5324432377\n",
      "modele  226  loss =  1248227.0270297541\n",
      "\n",
      "\n",
      "---------- 227 ----------\n",
      "modele  226  loss =  1248227.0270297541\n",
      "modele  227  loss =  1248226.5752896192\n",
      "\n",
      "\n",
      "---------- 228 ----------\n",
      "modele  227  loss =  1248226.5752896192\n",
      "modele  228  loss =  1248226.364603337\n",
      "\n",
      "\n",
      "---------- 229 ----------\n",
      "modele  228  loss =  1248226.364603337\n",
      "modele  229  loss =  1248226.1760914668\n",
      "\n",
      "\n",
      "---------- 230 ----------\n",
      "modele  229  loss =  1248226.1760914668\n",
      "modele  230  loss =  1248223.3895424737\n",
      "\n",
      "\n",
      "---------- 231 ----------\n",
      "modele  230  loss =  1248223.3895424737\n",
      "modele  231  loss =  1248222.0332319497\n",
      "\n",
      "\n",
      "---------- 232 ----------\n",
      "modele  231  loss =  1248222.0332319497\n",
      "modele  232  loss =  1248221.7405786808\n",
      "\n",
      "\n",
      "---------- 233 ----------\n",
      "modele  232  loss =  1248221.7405786808\n",
      "modele  233  loss =  1248221.4237792937\n",
      "\n",
      "\n",
      "---------- 234 ----------\n",
      "modele  233  loss =  1248221.4237792937\n",
      "modele  234  loss =  1248220.6508655513\n",
      "\n",
      "\n",
      "---------- 235 ----------\n",
      "modele  234  loss =  1248220.6508655513\n",
      "modele  235  loss =  1248220.036638936\n",
      "\n",
      "\n",
      "---------- 236 ----------\n",
      "modele  235  loss =  1248220.036638936\n",
      "modele  236  loss =  1248219.6328543415\n",
      "\n",
      "\n",
      "---------- 237 ----------\n",
      "modele  236  loss =  1248219.6328543415\n",
      "modele  237  loss =  1248218.5098349804\n",
      "\n",
      "\n",
      "---------- 238 ----------\n",
      "modele  237  loss =  1248218.5098349804\n",
      "modele  238  loss =  1248216.5593861998\n",
      "\n",
      "\n",
      "---------- 239 ----------\n",
      "modele  238  loss =  1248216.5593861998\n",
      "modele  239  loss =  1248209.0885811392\n",
      "\n",
      "\n",
      "---------- 240 ----------\n",
      "modele  239  loss =  1248209.0885811392\n",
      "modele  240  loss =  1248204.5913673663\n",
      "\n",
      "\n",
      "---------- 241 ----------\n",
      "modele  240  loss =  1248204.5913673663\n",
      "modele  241  loss =  1248204.234773337\n",
      "\n",
      "\n",
      "---------- 242 ----------\n",
      "modele  241  loss =  1248204.234773337\n",
      "modele  242  loss =  1248203.5139742943\n",
      "\n",
      "\n",
      "---------- 243 ----------\n",
      "modele  242  loss =  1248203.5139742943\n",
      "modele  243  loss =  1248187.3875285247\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 244 ----------\n",
      "modele  243  loss =  1248187.3875285247\n",
      "modele  244  loss =  1248180.65909359\n",
      "\n",
      "\n",
      "---------- 245 ----------\n",
      "modele  244  loss =  1248180.65909359\n",
      "modele  245  loss =  1248179.4973850427\n",
      "\n",
      "\n",
      "---------- 246 ----------\n",
      "modele  245  loss =  1248179.4973850427\n",
      "modele  246  loss =  1248176.7801411084\n",
      "\n",
      "\n",
      "---------- 247 ----------\n",
      "modele  246  loss =  1248176.7801411084\n",
      "modele  247  loss =  1248175.5681739692\n",
      "\n",
      "\n",
      "---------- 248 ----------\n",
      "modele  247  loss =  1248175.5681739692\n",
      "modele  248  loss =  1248175.2812117743\n",
      "\n",
      "\n",
      "---------- 249 ----------\n",
      "modele  248  loss =  1248175.2812117743\n",
      "modele  249  loss =  1248173.93249933\n",
      "\n",
      "\n",
      "---------- 250 ----------\n",
      "modele  249  loss =  1248173.93249933\n",
      "modele  250  loss =  1248173.1363882518\n",
      "\n",
      "\n",
      "---------- 251 ----------\n",
      "modele  250  loss =  1248173.1363882518\n",
      "modele  251  loss =  1248173.0585299116\n",
      "\n",
      "\n",
      "---------- 252 ----------\n",
      "modele  251  loss =  1248173.0585299116\n",
      "modele  252  loss =  1248163.338888038\n",
      "\n",
      "\n",
      "---------- 253 ----------\n",
      "modele  252  loss =  1248163.338888038\n",
      "modele  253  loss =  1248163.2774717996\n",
      "\n",
      "\n",
      "---------- 254 ----------\n",
      "modele  253  loss =  1248163.2774717996\n",
      "modele  254  loss =  1248162.7882019838\n",
      "\n",
      "\n",
      "---------- 255 ----------\n",
      "modele  254  loss =  1248162.7882019838\n",
      "modele  255  loss =  1248162.394187619\n",
      "\n",
      "\n",
      "---------- 256 ----------\n",
      "modele  255  loss =  1248162.394187619\n",
      "modele  256  loss =  1248161.9693469265\n",
      "\n",
      "\n",
      "---------- 257 ----------\n",
      "modele  256  loss =  1248161.9693469265\n",
      "modele  257  loss =  1248161.5873074615\n",
      "\n",
      "\n",
      "---------- 258 ----------\n",
      "modele  257  loss =  1248161.5873074615\n",
      "modele  258  loss =  1248160.9743423015\n",
      "\n",
      "\n",
      "---------- 259 ----------\n",
      "modele  258  loss =  1248160.9743423015\n",
      "modele  259  loss =  1248160.6106815776\n",
      "\n",
      "\n",
      "---------- 260 ----------\n",
      "modele  259  loss =  1248160.6106815776\n",
      "modele  260  loss =  1248160.497138239\n",
      "\n",
      "\n",
      "---------- 261 ----------\n",
      "modele  260  loss =  1248160.497138239\n",
      "modele  261  loss =  1248160.3091996824\n",
      "\n",
      "\n",
      "---------- 262 ----------\n",
      "modele  261  loss =  1248160.3091996824\n",
      "modele  262  loss =  1248160.2715024122\n",
      "\n",
      "\n",
      "---------- 263 ----------\n",
      "modele  262  loss =  1248160.2715024122\n",
      "modele  263  loss =  1248160.2032304595\n",
      "\n",
      "\n",
      "---------- 264 ----------\n",
      "modele  263  loss =  1248160.2032304595\n",
      "modele  264  loss =  1248159.9429230287\n",
      "\n",
      "\n",
      "---------- 265 ----------\n",
      "modele  264  loss =  1248159.9429230287\n",
      "modele  265  loss =  1248159.7177819526\n",
      "\n",
      "\n",
      "---------- 266 ----------\n",
      "modele  265  loss =  1248159.7177819526\n",
      "modele  266  loss =  1248158.1500056814\n",
      "\n",
      "\n",
      "---------- 267 ----------\n",
      "modele  266  loss =  1248158.1500056814\n",
      "modele  267  loss =  1248156.7826924077\n",
      "\n",
      "\n",
      "---------- 268 ----------\n",
      "modele  267  loss =  1248156.7826924077\n",
      "modele  268  loss =  1248155.8676434532\n",
      "\n",
      "\n",
      "---------- 269 ----------\n",
      "modele  268  loss =  1248155.8676434532\n",
      "modele  269  loss =  1248151.6056503223\n",
      "\n",
      "\n",
      "---------- 270 ----------\n",
      "modele  269  loss =  1248151.6056503223\n",
      "modele  270  loss =  1248150.8910129345\n",
      "\n",
      "\n",
      "---------- 271 ----------\n",
      "modele  270  loss =  1248150.8910129345\n",
      "modele  271  loss =  1248150.2394422013\n",
      "\n",
      "\n",
      "---------- 272 ----------\n",
      "modele  271  loss =  1248150.2394422013\n",
      "modele  272  loss =  1248147.640068756\n",
      "\n",
      "\n",
      "---------- 273 ----------\n",
      "modele  272  loss =  1248147.640068756\n",
      "modele  273  loss =  1248135.7623879488\n",
      "\n",
      "\n",
      "---------- 274 ----------\n",
      "modele  273  loss =  1248135.7623879488\n",
      "modele  274  loss =  1248131.2815471806\n",
      "\n",
      "\n",
      "---------- 275 ----------\n",
      "modele  274  loss =  1248131.2815471806\n",
      "modele  275  loss =  1248129.2962063327\n",
      "\n",
      "\n",
      "---------- 276 ----------\n",
      "modele  275  loss =  1248129.2962063327\n",
      "modele  276  loss =  1248126.564465533\n",
      "\n",
      "\n",
      "---------- 277 ----------\n",
      "modele  276  loss =  1248126.564465533\n",
      "modele  277  loss =  1248126.126493399\n",
      "\n",
      "\n",
      "---------- 278 ----------\n",
      "modele  277  loss =  1248126.126493399\n",
      "modele  278  loss =  1248121.7133897129\n",
      "\n",
      "\n",
      "---------- 279 ----------\n",
      "modele  278  loss =  1248121.7133897129\n",
      "modele  279  loss =  1248119.6157920922\n",
      "\n",
      "\n",
      "---------- 280 ----------\n",
      "modele  279  loss =  1248119.6157920922\n",
      "modele  280  loss =  1248119.335273214\n",
      "\n",
      "\n",
      "---------- 281 ----------\n",
      "modele  280  loss =  1248119.335273214\n",
      "modele  281  loss =  1248119.1763332374\n",
      "\n",
      "\n",
      "---------- 282 ----------\n",
      "modele  281  loss =  1248119.1763332374\n",
      "modele  282  loss =  1248117.4079303478\n",
      "\n",
      "\n",
      "---------- 283 ----------\n",
      "modele  282  loss =  1248117.4079303478\n",
      "modele  283  loss =  1248116.5519296029\n",
      "\n",
      "\n",
      "---------- 284 ----------\n",
      "modele  283  loss =  1248116.5519296029\n",
      "modele  284  loss =  1248115.377191705\n",
      "\n",
      "\n",
      "---------- 285 ----------\n",
      "modele  284  loss =  1248115.377191705\n",
      "modele  285  loss =  1248114.9273103224\n",
      "\n",
      "\n",
      "---------- 286 ----------\n",
      "modele  285  loss =  1248114.9273103224\n",
      "modele  286  loss =  1248114.5695549883\n",
      "\n",
      "\n",
      "---------- 287 ----------\n",
      "modele  286  loss =  1248114.5695549883\n",
      "modele  287  loss =  1248114.156837564\n",
      "\n",
      "\n",
      "---------- 288 ----------\n",
      "modele  287  loss =  1248114.156837564\n",
      "modele  288  loss =  1248113.5233854288\n",
      "\n",
      "\n",
      "---------- 289 ----------\n",
      "modele  288  loss =  1248113.5233854288\n",
      "modele  289  loss =  1248113.050244789\n",
      "\n",
      "\n",
      "---------- 290 ----------\n",
      "modele  289  loss =  1248113.050244789\n",
      "modele  290  loss =  1248112.878814398\n",
      "\n",
      "\n",
      "---------- 291 ----------\n",
      "modele  290  loss =  1248112.878814398\n",
      "modele  291  loss =  1248112.5191778885\n",
      "\n",
      "\n",
      "---------- 292 ----------\n",
      "modele  291  loss =  1248112.5191778885\n",
      "modele  292  loss =  1248111.8214228202\n",
      "\n",
      "\n",
      "---------- 293 ----------\n",
      "modele  292  loss =  1248111.8214228202\n",
      "modele  293  loss =  1248111.657093872\n",
      "\n",
      "\n",
      "---------- 294 ----------\n",
      "modele  293  loss =  1248111.657093872\n",
      "modele  294  loss =  1248109.9436499309\n",
      "\n",
      "\n",
      "---------- 295 ----------\n",
      "modele  294  loss =  1248109.9436499309\n",
      "modele  295  loss =  1248109.4913902886\n",
      "\n",
      "\n",
      "---------- 296 ----------\n",
      "modele  295  loss =  1248109.4913902886\n",
      "modele  296  loss =  1248109.2715311844\n",
      "\n",
      "\n",
      "---------- 297 ----------\n",
      "modele  296  loss =  1248109.2715311844\n",
      "modele  297  loss =  1248109.1386614852\n",
      "\n",
      "\n",
      "---------- 298 ----------\n",
      "modele  297  loss =  1248109.1386614852\n",
      "modele  298  loss =  1248108.514984799\n",
      "\n",
      "\n",
      "---------- 299 ----------\n",
      "modele  298  loss =  1248108.514984799\n",
      "modele  299  loss =  1248108.181850926\n",
      "\n",
      "\n",
      "---------- 300 ----------\n",
      "modele  299  loss =  1248108.181850926\n",
      "modele  300  loss =  1248108.0158152361\n",
      "\n",
      "\n",
      "---------- 301 ----------\n",
      "modele  300  loss =  1248108.0158152361\n",
      "modele  301  loss =  1248107.9660960566\n",
      "\n",
      "\n",
      "---------- 302 ----------\n",
      "modele  301  loss =  1248107.9660960566\n",
      "modele  302  loss =  1248107.8357060808\n",
      "\n",
      "\n",
      "---------- 303 ----------\n",
      "modele  302  loss =  1248107.8357060808\n",
      "modele  303  loss =  1248107.598094246\n",
      "\n",
      "\n",
      "---------- 304 ----------\n",
      "modele  303  loss =  1248107.598094246\n",
      "modele  304  loss =  1248107.437653579\n",
      "\n",
      "\n",
      "---------- 305 ----------\n",
      "modele  304  loss =  1248107.437653579\n",
      "modele  305  loss =  1248106.9861129322\n",
      "\n",
      "\n",
      "---------- 306 ----------\n",
      "modele  305  loss =  1248106.9861129322\n",
      "modele  306  loss =  1248106.7588582742\n",
      "\n",
      "\n",
      "---------- 307 ----------\n",
      "modele  306  loss =  1248106.7588582742\n",
      "modele  307  loss =  1248106.7282480015\n",
      "\n",
      "\n",
      "---------- 308 ----------\n",
      "modele  307  loss =  1248106.7282480015\n",
      "modele  308  loss =  1248106.5680155852\n",
      "\n",
      "\n",
      "---------- 309 ----------\n",
      "modele  308  loss =  1248106.5680155852\n",
      "modele  309  loss =  1248106.4169505222\n",
      "\n",
      "\n",
      "---------- 310 ----------\n",
      "modele  309  loss =  1248106.4169505222\n",
      "modele  310  loss =  1248106.2625842355\n",
      "\n",
      "\n",
      "---------- 311 ----------\n",
      "modele  310  loss =  1248106.2625842355\n",
      "modele  311  loss =  1248106.1520577415\n",
      "\n",
      "\n",
      "---------- 312 ----------\n",
      "modele  311  loss =  1248106.1520577415\n",
      "modele  312  loss =  1248105.994976628\n",
      "\n",
      "\n",
      "---------- 313 ----------\n",
      "modele  312  loss =  1248105.994976628\n",
      "modele  313  loss =  1248105.9001161382\n",
      "\n",
      "\n",
      "---------- 314 ----------\n",
      "modele  313  loss =  1248105.9001161382\n",
      "modele  314  loss =  1248105.7652217252\n",
      "\n",
      "\n",
      "---------- 315 ----------\n",
      "modele  314  loss =  1248105.7652217252\n",
      "modele  315  loss =  1248104.778133024\n",
      "\n",
      "\n",
      "---------- 316 ----------\n",
      "modele  315  loss =  1248104.778133024\n",
      "modele  316  loss =  1248102.4474115174\n",
      "\n",
      "\n",
      "---------- 317 ----------\n",
      "modele  316  loss =  1248102.4474115174\n",
      "modele  317  loss =  1248102.3307730593\n",
      "\n",
      "\n",
      "---------- 318 ----------\n",
      "modele  317  loss =  1248102.3307730593\n",
      "modele  318  loss =  1248101.477380853\n",
      "\n",
      "\n",
      "---------- 319 ----------\n",
      "modele  318  loss =  1248101.477380853\n",
      "modele  319  loss =  1248101.133576205\n",
      "\n",
      "\n",
      "---------- 320 ----------\n",
      "modele  319  loss =  1248101.133576205\n",
      "modele  320  loss =  1248100.8444681799\n",
      "\n",
      "\n",
      "---------- 321 ----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modele  320  loss =  1248100.8444681799\n",
      "modele  321  loss =  1248100.2354143849\n",
      "\n",
      "\n",
      "---------- 322 ----------\n",
      "modele  321  loss =  1248100.2354143849\n",
      "modele  322  loss =  1248099.5478384735\n",
      "\n",
      "\n",
      "---------- 323 ----------\n",
      "modele  322  loss =  1248099.5478384735\n",
      "modele  323  loss =  1248098.946375203\n",
      "\n",
      "\n",
      "---------- 324 ----------\n",
      "modele  323  loss =  1248098.946375203\n",
      "modele  324  loss =  1248098.5286784873\n",
      "\n",
      "\n",
      "---------- 325 ----------\n",
      "modele  324  loss =  1248098.5286784873\n",
      "modele  325  loss =  1248098.0155512644\n",
      "\n",
      "\n",
      "---------- 326 ----------\n",
      "modele  325  loss =  1248098.0155512644\n",
      "modele  326  loss =  1248097.920425175\n",
      "\n",
      "\n",
      "---------- 327 ----------\n",
      "modele  326  loss =  1248097.920425175\n",
      "modele  327  loss =  1248097.8569373186\n",
      "\n",
      "\n",
      "---------- 328 ----------\n",
      "modele  327  loss =  1248097.8569373186\n",
      "modele  328  loss =  1248097.6322100377\n",
      "\n",
      "\n",
      "---------- 329 ----------\n",
      "modele  328  loss =  1248097.6322100377\n",
      "modele  329  loss =  1248097.4772961459\n",
      "\n",
      "\n",
      "---------- 330 ----------\n",
      "modele  329  loss =  1248097.4772961459\n",
      "modele  330  loss =  1248097.3111198007\n",
      "\n",
      "\n",
      "---------- 331 ----------\n",
      "modele  330  loss =  1248097.3111198007\n",
      "modele  331  loss =  1248097.1287332117\n",
      "\n",
      "\n",
      "---------- 332 ----------\n",
      "modele  331  loss =  1248097.1287332117\n",
      "modele  332  loss =  1248096.9968799646\n",
      "\n",
      "\n",
      "---------- 333 ----------\n",
      "modele  332  loss =  1248096.9968799646\n",
      "modele  333  loss =  1248096.8116343278\n",
      "\n",
      "\n",
      "---------- 334 ----------\n",
      "modele  333  loss =  1248096.8116343278\n",
      "modele  334  loss =  1248096.7584795193\n",
      "\n",
      "\n",
      "---------- 335 ----------\n",
      "modele  334  loss =  1248096.7584795193\n",
      "modele  335  loss =  1248096.7073624062\n",
      "\n",
      "\n",
      "---------- 336 ----------\n",
      "modele  335  loss =  1248096.7073624062\n",
      "modele  336  loss =  1248096.5812127283\n",
      "\n",
      "\n",
      "---------- 337 ----------\n",
      "modele  336  loss =  1248096.5812127283\n",
      "modele  337  loss =  1248096.524087699\n",
      "\n",
      "\n",
      "---------- 338 ----------\n",
      "modele  337  loss =  1248096.524087699\n",
      "modele  338  loss =  1248096.1939462062\n",
      "\n",
      "\n",
      "---------- 339 ----------\n",
      "modele  338  loss =  1248096.1939462062\n",
      "modele  339  loss =  1248095.5104836605\n",
      "\n",
      "\n",
      "---------- 340 ----------\n",
      "modele  339  loss =  1248095.5104836605\n",
      "modele  340  loss =  1248095.4395507162\n",
      "\n",
      "\n",
      "---------- 341 ----------\n",
      "modele  340  loss =  1248095.4395507162\n",
      "modele  341  loss =  1248095.2235178489\n",
      "\n",
      "\n",
      "---------- 342 ----------\n",
      "modele  341  loss =  1248095.2235178489\n",
      "modele  342  loss =  1248095.0292833366\n",
      "\n",
      "\n",
      "---------- 343 ----------\n",
      "modele  342  loss =  1248095.0292833366\n",
      "modele  343  loss =  1248094.7351530555\n",
      "\n",
      "\n",
      "---------- 344 ----------\n",
      "modele  343  loss =  1248094.7351530555\n",
      "modele  344  loss =  1248093.9317005726\n",
      "\n",
      "\n",
      "---------- 345 ----------\n",
      "modele  344  loss =  1248093.9317005726\n",
      "modele  345  loss =  1248093.8485018497\n",
      "\n",
      "\n",
      "---------- 346 ----------\n",
      "modele  345  loss =  1248093.8485018497\n",
      "modele  346  loss =  1248093.5118763559\n",
      "\n",
      "\n",
      "---------- 347 ----------\n",
      "modele  346  loss =  1248093.5118763559\n",
      "modele  347  loss =  1248093.157790479\n",
      "\n",
      "\n",
      "---------- 348 ----------\n",
      "modele  347  loss =  1248093.157790479\n",
      "modele  348  loss =  1248093.1209702168\n",
      "\n",
      "\n",
      "---------- 349 ----------\n",
      "modele  348  loss =  1248093.1209702168\n",
      "modele  349  loss =  1248093.0686705362\n",
      "\n",
      "\n",
      "---------- 350 ----------\n",
      "modele  349  loss =  1248093.0686705362\n",
      "modele  350  loss =  1248092.8487060324\n",
      "\n",
      "\n",
      "---------- 351 ----------\n",
      "modele  350  loss =  1248092.8487060324\n",
      "modele  351  loss =  1248092.6659032088\n",
      "\n",
      "\n",
      "---------- 352 ----------\n",
      "modele  351  loss =  1248092.6659032088\n",
      "modele  352  loss =  1248092.171993753\n",
      "\n",
      "\n",
      "---------- 353 ----------\n",
      "modele  352  loss =  1248092.171993753\n",
      "modele  353  loss =  1248092.1188849257\n",
      "\n",
      "\n",
      "---------- 354 ----------\n",
      "modele  353  loss =  1248092.1188849257\n",
      "modele  354  loss =  1248091.6891074083\n",
      "\n",
      "\n",
      "---------- 355 ----------\n",
      "modele  354  loss =  1248091.6891074083\n",
      "modele  355  loss =  1248091.4858324076\n",
      "\n",
      "\n",
      "---------- 356 ----------\n",
      "modele  355  loss =  1248091.4858324076\n",
      "modele  356  loss =  1248091.301053417\n",
      "\n",
      "\n",
      "---------- 357 ----------\n",
      "modele  356  loss =  1248091.301053417\n",
      "modele  357  loss =  1248091.0633404772\n",
      "\n",
      "\n",
      "---------- 358 ----------\n",
      "modele  357  loss =  1248091.0633404772\n",
      "modele  358  loss =  1248090.8508600774\n",
      "\n",
      "\n",
      "---------- 359 ----------\n",
      "modele  358  loss =  1248090.8508600774\n",
      "modele  359  loss =  1248090.6042421465\n",
      "\n",
      "\n",
      "---------- 360 ----------\n",
      "modele  359  loss =  1248090.6042421465\n",
      "modele  360  loss =  1248090.3641123334\n",
      "\n",
      "\n",
      "---------- 361 ----------\n",
      "modele  360  loss =  1248090.3641123334\n",
      "modele  361  loss =  1248089.9169876513\n",
      "\n",
      "\n",
      "---------- 362 ----------\n",
      "modele  361  loss =  1248089.9169876513\n",
      "modele  362  loss =  1248089.159802851\n",
      "\n",
      "\n",
      "---------- 363 ----------\n",
      "modele  362  loss =  1248089.159802851\n",
      "modele  363  loss =  1248088.901622332\n",
      "\n",
      "\n",
      "---------- 364 ----------\n",
      "modele  363  loss =  1248088.901622332\n",
      "modele  364  loss =  1248088.4665905666\n",
      "\n",
      "\n",
      "---------- 365 ----------\n",
      "modele  364  loss =  1248088.4665905666\n",
      "modele  365  loss =  1248088.313505392\n",
      "\n",
      "\n",
      "---------- 366 ----------\n",
      "modele  365  loss =  1248088.313505392\n",
      "modele  366  loss =  1248087.9553396613\n",
      "\n",
      "\n",
      "---------- 367 ----------\n",
      "modele  366  loss =  1248087.9553396613\n",
      "modele  367  loss =  1248087.7921519298\n",
      "\n",
      "\n",
      "---------- 368 ----------\n",
      "modele  367  loss =  1248087.7921519298\n",
      "modele  368  loss =  1248087.5048399814\n",
      "\n",
      "\n",
      "---------- 369 ----------\n",
      "modele  368  loss =  1248087.5048399814\n",
      "modele  369  loss =  1248087.3183023827\n",
      "\n",
      "\n",
      "---------- 370 ----------\n",
      "modele  369  loss =  1248087.3183023827\n",
      "modele  370  loss =  1248087.0800241146\n",
      "\n",
      "\n",
      "---------- 371 ----------\n",
      "modele  370  loss =  1248087.0800241146\n",
      "modele  371  loss =  1248086.865632734\n",
      "\n",
      "\n",
      "---------- 372 ----------\n",
      "modele  371  loss =  1248086.865632734\n",
      "modele  372  loss =  1248086.6184004922\n",
      "\n",
      "\n",
      "---------- 373 ----------\n",
      "modele  372  loss =  1248086.6184004922\n",
      "modele  373  loss =  1248086.378522982\n",
      "\n",
      "\n",
      "---------- 374 ----------\n",
      "modele  373  loss =  1248086.378522982\n",
      "modele  374  loss =  1248085.9299674244\n",
      "\n",
      "\n",
      "---------- 375 ----------\n",
      "modele  374  loss =  1248085.9299674244\n",
      "modele  375  loss =  1248085.165099486\n",
      "\n",
      "\n",
      "---------- 376 ----------\n",
      "modele  375  loss =  1248085.165099486\n",
      "modele  376  loss =  1248084.911832333\n",
      "\n",
      "\n",
      "---------- 377 ----------\n",
      "modele  376  loss =  1248084.911832333\n",
      "modele  377  loss =  1248084.47895118\n",
      "\n",
      "\n",
      "---------- 378 ----------\n",
      "modele  377  loss =  1248084.47895118\n",
      "modele  378  loss =  1248084.329712435\n",
      "\n",
      "\n",
      "---------- 379 ----------\n",
      "modele  378  loss =  1248084.329712435\n",
      "modele  379  loss =  1248083.9745616755\n",
      "\n",
      "\n",
      "---------- 380 ----------\n",
      "modele  379  loss =  1248083.9745616755\n",
      "modele  380  loss =  1248083.8103551427\n",
      "\n",
      "\n",
      "---------- 381 ----------\n",
      "modele  380  loss =  1248083.8103551427\n",
      "modele  381  loss =  1248083.5257993839\n",
      "\n",
      "\n",
      "---------- 382 ----------\n",
      "modele  381  loss =  1248083.5257993839\n",
      "modele  382  loss =  1248083.3380194078\n",
      "\n",
      "\n",
      "---------- 383 ----------\n",
      "modele  382  loss =  1248083.3380194078\n",
      "modele  383  loss =  1248083.0993417609\n",
      "\n",
      "\n",
      "---------- 384 ----------\n",
      "modele  383  loss =  1248083.0993417609\n",
      "modele  384  loss =  1248082.894407974\n",
      "\n",
      "\n",
      "---------- 385 ----------\n",
      "modele  384  loss =  1248082.894407974\n",
      "modele  385  loss =  1248082.4285729136\n",
      "\n",
      "\n",
      "---------- 386 ----------\n",
      "modele  385  loss =  1248082.4285729136\n",
      "modele  386  loss =  1248081.7734821094\n",
      "\n",
      "\n",
      "---------- 387 ----------\n",
      "modele  386  loss =  1248081.7734821094\n",
      "modele  387  loss =  1248081.4949724101\n",
      "\n",
      "\n",
      "---------- 388 ----------\n",
      "modele  387  loss =  1248081.4949724101\n",
      "modele  388  loss =  1248081.2585743472\n",
      "\n",
      "\n",
      "---------- 389 ----------\n",
      "modele  388  loss =  1248081.2585743472\n",
      "modele  389  loss =  1248081.1631815908\n",
      "\n",
      "\n",
      "---------- 390 ----------\n",
      "modele  389  loss =  1248081.1631815908\n",
      "modele  390  loss =  1248080.9129469646\n",
      "\n",
      "\n",
      "---------- 391 ----------\n",
      "modele  390  loss =  1248080.9129469646\n",
      "modele  391  loss =  1248080.355502979\n",
      "\n",
      "\n",
      "---------- 392 ----------\n",
      "modele  391  loss =  1248080.355502979\n",
      "modele  392  loss =  1248080.0625998245\n",
      "\n",
      "\n",
      "---------- 393 ----------\n",
      "modele  392  loss =  1248080.0625998245\n",
      "modele  393  loss =  1248079.9911793363\n",
      "\n",
      "\n",
      "---------- 394 ----------\n",
      "modele  393  loss =  1248079.9911793363\n",
      "modele  394  loss =  1248079.8412451826\n",
      "\n",
      "\n",
      "---------- 395 ----------\n",
      "modele  394  loss =  1248079.8412451826\n",
      "modele  395  loss =  1248079.6695772244\n",
      "\n",
      "\n",
      "---------- 396 ----------\n",
      "modele  395  loss =  1248079.6695772244\n",
      "modele  396  loss =  1248079.4866027005\n",
      "\n",
      "\n",
      "---------- 397 ----------\n",
      "modele  396  loss =  1248079.4866027005\n",
      "modele  397  loss =  1248079.3694268386\n",
      "\n",
      "\n",
      "---------- 398 ----------\n",
      "modele  397  loss =  1248079.3694268386\n",
      "modele  398  loss =  1248079.196822619\n",
      "\n",
      "\n",
      "---------- 399 ----------\n",
      "modele  398  loss =  1248079.196822619\n",
      "modele  399  loss =  1248079.1671176893\n",
      "\n",
      "\n",
      "---------- 400 ----------\n",
      "modele  399  loss =  1248079.1671176893\n",
      "modele  400  loss =  1248079.1014083964\n",
      "\n",
      "\n",
      "---------- 401 ----------\n",
      "modele  400  loss =  1248079.1014083964\n",
      "modele  401  loss =  1248079.0645432891\n",
      "\n",
      "\n",
      "---------- 402 ----------\n",
      "modele  401  loss =  1248079.0645432891\n",
      "modele  402  loss =  1248079.0201887167\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 403 ----------\n",
      "modele  402  loss =  1248079.0201887167\n",
      "modele  403  loss =  1248078.9842168102\n",
      "\n",
      "\n",
      "---------- 404 ----------\n",
      "modele  403  loss =  1248078.9842168102\n",
      "modele  404  loss =  1248078.9426632863\n",
      "\n",
      "\n",
      "---------- 405 ----------\n",
      "modele  404  loss =  1248078.9426632863\n",
      "modele  405  loss =  1248078.89548085\n",
      "\n",
      "\n",
      "---------- 406 ----------\n",
      "modele  405  loss =  1248078.89548085\n",
      "modele  406  loss =  1248078.8510857746\n",
      "\n",
      "\n",
      "---------- 407 ----------\n",
      "modele  406  loss =  1248078.8510857746\n",
      "modele  407  loss =  1248078.7906608135\n",
      "\n",
      "\n",
      "---------- 408 ----------\n",
      "modele  407  loss =  1248078.7906608135\n",
      "modele  408  loss =  1248078.7429091334\n",
      "\n",
      "\n",
      "---------- 409 ----------\n",
      "modele  408  loss =  1248078.7429091334\n",
      "modele  409  loss =  1248078.67447941\n",
      "\n",
      "\n",
      "---------- 410 ----------\n",
      "modele  409  loss =  1248078.67447941\n",
      "modele  410  loss =  1248078.5127640024\n",
      "\n",
      "\n",
      "---------- 411 ----------\n",
      "modele  410  loss =  1248078.5127640024\n",
      "modele  411  loss =  1248078.4186803845\n",
      "\n",
      "\n",
      "---------- 412 ----------\n",
      "modele  411  loss =  1248078.4186803845\n",
      "modele  412  loss =  1248078.348145014\n",
      "\n",
      "\n",
      "---------- 413 ----------\n",
      "modele  412  loss =  1248078.348145014\n",
      "modele  413  loss =  1248078.146934236\n",
      "\n",
      "\n",
      "---------- 414 ----------\n",
      "modele  413  loss =  1248078.146934236\n",
      "modele  414  loss =  1248078.0788685905\n",
      "\n",
      "\n",
      "---------- 415 ----------\n",
      "modele  414  loss =  1248078.0788685905\n",
      "modele  415  loss =  1248077.8680881904\n",
      "\n",
      "\n",
      "---------- 416 ----------\n",
      "modele  415  loss =  1248077.8680881904\n",
      "modele  416  loss =  1248077.7472419438\n",
      "\n",
      "\n",
      "---------- 417 ----------\n",
      "modele  416  loss =  1248077.7472419438\n",
      "modele  417  loss =  1248077.498297101\n",
      "\n",
      "\n",
      "---------- 418 ----------\n",
      "modele  417  loss =  1248077.498297101\n",
      "modele  418  loss =  1248077.3039265028\n",
      "\n",
      "\n",
      "---------- 419 ----------\n",
      "modele  418  loss =  1248077.3039265028\n",
      "modele  419  loss =  1248077.0020002571\n",
      "\n",
      "\n",
      "---------- 420 ----------\n",
      "modele  419  loss =  1248077.0020002571\n",
      "modele  420  loss =  1248076.740298352\n",
      "\n",
      "\n",
      "---------- 421 ----------\n",
      "modele  420  loss =  1248076.740298352\n",
      "modele  421  loss =  1248076.520714389\n",
      "\n",
      "\n",
      "---------- 422 ----------\n",
      "modele  421  loss =  1248076.520714389\n",
      "modele  422  loss =  1248076.3865499753\n",
      "\n",
      "\n",
      "---------- 423 ----------\n",
      "modele  422  loss =  1248076.3865499753\n",
      "modele  423  loss =  1248076.1171522597\n",
      "\n",
      "\n",
      "---------- 424 ----------\n",
      "modele  423  loss =  1248076.1171522597\n",
      "modele  424  loss =  1248075.8532679395\n",
      "\n",
      "\n",
      "---------- 425 ----------\n",
      "modele  424  loss =  1248075.8532679395\n",
      "modele  425  loss =  1248075.679638418\n",
      "\n",
      "\n",
      "---------- 426 ----------\n",
      "modele  425  loss =  1248075.679638418\n",
      "modele  426  loss =  1248075.4170372728\n",
      "\n",
      "\n",
      "---------- 427 ----------\n",
      "modele  426  loss =  1248075.4170372728\n",
      "modele  427  loss =  1248075.2013438328\n",
      "\n",
      "\n",
      "---------- 428 ----------\n",
      "modele  427  loss =  1248075.2013438328\n",
      "modele  428  loss =  1248074.8407903519\n",
      "\n",
      "\n",
      "---------- 429 ----------\n",
      "modele  428  loss =  1248074.8407903519\n",
      "modele  429  loss =  1248074.6273236838\n",
      "\n",
      "\n",
      "---------- 430 ----------\n",
      "modele  429  loss =  1248074.6273236838\n",
      "modele  430  loss =  1248074.4956768258\n",
      "\n",
      "\n",
      "---------- 431 ----------\n",
      "modele  430  loss =  1248074.4956768258\n",
      "modele  431  loss =  1248074.389048099\n",
      "\n",
      "\n",
      "---------- 432 ----------\n",
      "modele  431  loss =  1248074.389048099\n",
      "modele  432  loss =  1248074.2389803857\n",
      "\n",
      "\n",
      "---------- 433 ----------\n",
      "modele  432  loss =  1248074.2389803857\n",
      "modele  433  loss =  1248074.152271105\n",
      "\n",
      "\n",
      "---------- 434 ----------\n",
      "modele  433  loss =  1248074.152271105\n",
      "modele  434  loss =  1248073.9776684318\n",
      "\n",
      "\n",
      "---------- 435 ----------\n",
      "modele  434  loss =  1248073.9776684318\n",
      "modele  435  loss =  1248073.6979140074\n",
      "\n",
      "\n",
      "---------- 436 ----------\n",
      "modele  435  loss =  1248073.6979140074\n",
      "modele  436  loss =  1248073.635536477\n",
      "\n",
      "\n",
      "---------- 437 ----------\n",
      "modele  436  loss =  1248073.635536477\n",
      "modele  437  loss =  1248073.0177447428\n",
      "\n",
      "\n",
      "---------- 438 ----------\n",
      "modele  437  loss =  1248073.0177447428\n",
      "modele  438  loss =  1248071.9066995403\n",
      "\n",
      "\n",
      "---------- 439 ----------\n",
      "modele  438  loss =  1248071.9066995403\n",
      "modele  439  loss =  1248071.7436397793\n",
      "\n",
      "\n",
      "---------- 440 ----------\n",
      "modele  439  loss =  1248071.7436397793\n",
      "modele  440  loss =  1248071.1655960584\n",
      "\n",
      "\n",
      "---------- 441 ----------\n",
      "modele  440  loss =  1248071.1655960584\n",
      "modele  441  loss =  1248070.8494951967\n",
      "\n",
      "\n",
      "---------- 442 ----------\n",
      "modele  441  loss =  1248070.8494951967\n",
      "modele  442  loss =  1248070.6549878789\n",
      "\n",
      "\n",
      "---------- 443 ----------\n",
      "modele  442  loss =  1248070.6549878789\n",
      "modele  443  loss =  1248070.0617420345\n",
      "\n",
      "\n",
      "---------- 444 ----------\n",
      "modele  443  loss =  1248070.0617420345\n",
      "modele  444  loss =  1248069.9008121903\n",
      "\n",
      "\n",
      "---------- 445 ----------\n",
      "modele  444  loss =  1248069.9008121903\n",
      "modele  445  loss =  1248069.7142459848\n",
      "\n",
      "\n",
      "---------- 446 ----------\n",
      "modele  445  loss =  1248069.7142459848\n",
      "modele  446  loss =  1248069.2385245503\n",
      "\n",
      "\n",
      "---------- 447 ----------\n",
      "modele  446  loss =  1248069.2385245503\n",
      "modele  447  loss =  1248069.001686918\n",
      "\n",
      "\n",
      "---------- 448 ----------\n",
      "modele  447  loss =  1248069.001686918\n",
      "modele  448  loss =  1248068.6165257986\n",
      "\n",
      "\n",
      "---------- 449 ----------\n",
      "modele  448  loss =  1248068.6165257986\n",
      "modele  449  loss =  1248068.2863655882\n",
      "\n",
      "\n",
      "---------- 450 ----------\n",
      "modele  449  loss =  1248068.2863655882\n",
      "modele  450  loss =  1248068.0044186579\n",
      "\n",
      "\n",
      "---------- 451 ----------\n",
      "modele  450  loss =  1248068.0044186579\n",
      "modele  451  loss =  1248067.8147123319\n",
      "\n",
      "\n",
      "---------- 452 ----------\n",
      "modele  451  loss =  1248067.8147123319\n",
      "modele  452  loss =  1248067.183792466\n",
      "\n",
      "\n",
      "---------- 453 ----------\n",
      "modele  452  loss =  1248067.183792466\n",
      "modele  453  loss =  1248066.716772215\n",
      "\n",
      "\n",
      "---------- 454 ----------\n",
      "modele  453  loss =  1248066.716772215\n",
      "modele  454  loss =  1248066.5071987626\n",
      "\n",
      "\n",
      "---------- 455 ----------\n",
      "modele  454  loss =  1248066.5071987626\n",
      "modele  455  loss =  1248066.4087741124\n",
      "\n",
      "\n",
      "---------- 456 ----------\n",
      "modele  455  loss =  1248066.4087741124\n",
      "modele  456  loss =  1248065.7908302548\n",
      "\n",
      "\n",
      "---------- 457 ----------\n",
      "modele  456  loss =  1248065.7908302548\n",
      "modele  457  loss =  1248065.0466731023\n",
      "\n",
      "\n",
      "---------- 458 ----------\n",
      "modele  457  loss =  1248065.0466731023\n",
      "modele  458  loss =  1248064.8760218087\n",
      "\n",
      "\n",
      "---------- 459 ----------\n",
      "modele  458  loss =  1248064.8760218087\n",
      "modele  459  loss =  1248064.558027625\n",
      "\n",
      "\n",
      "---------- 460 ----------\n",
      "modele  459  loss =  1248064.558027625\n",
      "modele  460  loss =  1248064.2539229805\n",
      "\n",
      "\n",
      "---------- 461 ----------\n",
      "modele  460  loss =  1248064.2539229805\n",
      "modele  461  loss =  1248064.0937874443\n",
      "\n",
      "\n",
      "---------- 462 ----------\n",
      "modele  461  loss =  1248064.0937874443\n",
      "modele  462  loss =  1248063.3573183776\n",
      "\n",
      "\n",
      "---------- 463 ----------\n",
      "modele  462  loss =  1248063.3573183776\n",
      "modele  463  loss =  1248062.997768757\n",
      "\n",
      "\n",
      "---------- 464 ----------\n",
      "modele  463  loss =  1248062.997768757\n",
      "modele  464  loss =  1248062.7400560337\n",
      "\n",
      "\n",
      "---------- 465 ----------\n",
      "modele  464  loss =  1248062.7400560337\n",
      "modele  465  loss =  1248062.3187204506\n",
      "\n",
      "\n",
      "---------- 466 ----------\n",
      "modele  465  loss =  1248062.3187204506\n",
      "modele  466  loss =  1248062.1575512355\n",
      "\n",
      "\n",
      "---------- 467 ----------\n",
      "modele  466  loss =  1248062.1575512355\n",
      "modele  467  loss =  1248061.3851746907\n",
      "\n",
      "\n",
      "---------- 468 ----------\n",
      "modele  467  loss =  1248061.3851746907\n",
      "modele  468  loss =  1248061.0436295401\n",
      "\n",
      "\n",
      "---------- 469 ----------\n",
      "modele  468  loss =  1248061.0436295401\n",
      "modele  469  loss =  1248060.641119625\n",
      "\n",
      "\n",
      "---------- 470 ----------\n",
      "modele  469  loss =  1248060.641119625\n",
      "modele  470  loss =  1248060.2410676174\n",
      "\n",
      "\n",
      "---------- 471 ----------\n",
      "modele  470  loss =  1248060.2410676174\n",
      "modele  471  loss =  1248060.0518700033\n",
      "\n",
      "\n",
      "---------- 472 ----------\n",
      "modele  471  loss =  1248060.0518700033\n",
      "modele  472  loss =  1248059.2606688435\n",
      "\n",
      "\n",
      "---------- 473 ----------\n",
      "modele  472  loss =  1248059.2606688435\n",
      "modele  473  loss =  1248058.9248463728\n",
      "\n",
      "\n",
      "---------- 474 ----------\n",
      "modele  473  loss =  1248058.9248463728\n",
      "modele  474  loss =  1248058.6569765387\n",
      "\n",
      "\n",
      "---------- 475 ----------\n",
      "modele  474  loss =  1248058.6569765387\n",
      "modele  475  loss =  1248058.2571249958\n",
      "\n",
      "\n",
      "---------- 476 ----------\n",
      "modele  475  loss =  1248058.2571249958\n",
      "modele  476  loss =  1248058.070607009\n",
      "\n",
      "\n",
      "---------- 477 ----------\n",
      "modele  476  loss =  1248058.070607009\n",
      "modele  477  loss =  1248057.2813430103\n",
      "\n",
      "\n",
      "---------- 478 ----------\n",
      "modele  477  loss =  1248057.2813430103\n",
      "modele  478  loss =  1248056.9471045225\n",
      "\n",
      "\n",
      "---------- 479 ----------\n",
      "modele  478  loss =  1248056.9471045225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modele  479  loss =  1248056.6814746722\n",
      "\n",
      "\n",
      "---------- 480 ----------\n",
      "modele  479  loss =  1248056.6814746722\n",
      "modele  480  loss =  1248056.2818104427\n",
      "\n",
      "\n",
      "---------- 481 ----------\n",
      "modele  480  loss =  1248056.2818104427\n",
      "modele  481  loss =  1248056.0977962897\n",
      "\n",
      "\n",
      "---------- 482 ----------\n",
      "modele  481  loss =  1248056.0977962897\n",
      "modele  482  loss =  1248055.310342367\n",
      "\n",
      "\n",
      "---------- 483 ----------\n",
      "modele  482  loss =  1248055.310342367\n",
      "modele  483  loss =  1248055.022423949\n",
      "\n",
      "\n",
      "---------- 484 ----------\n",
      "modele  483  loss =  1248055.022423949\n",
      "modele  484  loss =  1248054.8239382477\n",
      "\n",
      "\n",
      "---------- 485 ----------\n",
      "modele  484  loss =  1248054.8239382477\n",
      "modele  485  loss =  1248054.1292830547\n",
      "\n",
      "\n",
      "---------- 486 ----------\n",
      "modele  485  loss =  1248054.1292830547\n",
      "modele  486  loss =  1248053.9352702168\n",
      "\n",
      "\n",
      "---------- 487 ----------\n",
      "modele  486  loss =  1248053.9352702168\n",
      "modele  487  loss =  1248053.6784828021\n",
      "\n",
      "\n",
      "---------- 488 ----------\n",
      "modele  487  loss =  1248053.6784828021\n",
      "modele  488  loss =  1248053.629437529\n",
      "\n",
      "\n",
      "---------- 489 ----------\n",
      "modele  488  loss =  1248053.629437529\n",
      "modele  489  loss =  1248053.5400532389\n",
      "\n",
      "\n",
      "---------- 490 ----------\n",
      "modele  489  loss =  1248053.5400532389\n",
      "modele  490  loss =  1248053.0802971981\n",
      "\n",
      "\n",
      "---------- 491 ----------\n",
      "modele  490  loss =  1248053.0802971981\n",
      "modele  491  loss =  1248052.7313519996\n",
      "\n",
      "\n",
      "---------- 492 ----------\n",
      "modele  491  loss =  1248052.7313519996\n",
      "modele  492  loss =  1248052.3842568782\n",
      "\n",
      "\n",
      "---------- 493 ----------\n",
      "modele  492  loss =  1248052.3842568782\n",
      "modele  493  loss =  1248052.3048324296\n",
      "\n",
      "\n",
      "---------- 494 ----------\n",
      "modele  493  loss =  1248052.3048324296\n",
      "modele  494  loss =  1248052.1726780618\n",
      "\n",
      "\n",
      "---------- 495 ----------\n",
      "modele  494  loss =  1248052.1726780618\n",
      "modele  495  loss =  1248051.6380942375\n",
      "\n",
      "\n",
      "---------- 496 ----------\n",
      "modele  495  loss =  1248051.6380942375\n",
      "modele  496  loss =  1248051.3054968638\n",
      "\n",
      "\n",
      "---------- 497 ----------\n",
      "modele  496  loss =  1248051.3054968638\n",
      "modele  497  loss =  1248051.1953732932\n",
      "\n",
      "\n",
      "---------- 498 ----------\n",
      "modele  497  loss =  1248051.1953732932\n",
      "modele  498  loss =  1248050.713316426\n",
      "\n",
      "\n",
      "---------- 499 ----------\n",
      "modele  498  loss =  1248050.713316426\n",
      "modele  499  loss =  1248050.4078242336\n",
      "\n",
      "\n",
      "---------- 500 ----------\n",
      "modele  499  loss =  1248050.4078242336\n",
      "modele  500  loss =  1248050.3213446545\n",
      "\n",
      "\n",
      "---------- 501 ----------\n",
      "modele  500  loss =  1248050.3213446545\n",
      "modele  501  loss =  1248049.8928870116\n",
      "\n",
      "\n",
      "---------- 502 ----------\n",
      "modele  501  loss =  1248049.8928870116\n",
      "modele  502  loss =  1248049.6379594442\n",
      "\n",
      "\n",
      "---------- 503 ----------\n",
      "modele  502  loss =  1248049.6379594442\n",
      "modele  503  loss =  1248049.4866369918\n",
      "\n",
      "\n",
      "---------- 504 ----------\n",
      "modele  503  loss =  1248049.4866369918\n",
      "modele  504  loss =  1248049.2869188942\n",
      "\n",
      "\n",
      "---------- 505 ----------\n",
      "modele  504  loss =  1248049.2869188942\n",
      "modele  505  loss =  1248048.8533558685\n",
      "\n",
      "\n",
      "---------- 506 ----------\n",
      "modele  505  loss =  1248048.8533558685\n",
      "modele  506  loss =  1248048.8322445455\n",
      "\n",
      "\n",
      "---------- 507 ----------\n",
      "modele  506  loss =  1248048.8322445455\n",
      "modele  507  loss =  1248048.4800770525\n",
      "\n",
      "\n",
      "---------- 508 ----------\n",
      "modele  507  loss =  1248048.4800770525\n",
      "modele  508  loss =  1248048.3428307143\n",
      "\n",
      "\n",
      "---------- 509 ----------\n",
      "modele  508  loss =  1248048.3428307143\n",
      "modele  509  loss =  1248047.9233188343\n",
      "\n",
      "\n",
      "---------- 510 ----------\n",
      "modele  509  loss =  1248047.9233188343\n",
      "modele  510  loss =  1248047.250846693\n",
      "\n",
      "\n",
      "---------- 511 ----------\n",
      "modele  510  loss =  1248047.250846693\n",
      "modele  511  loss =  1248046.8951343512\n",
      "\n",
      "\n",
      "---------- 512 ----------\n",
      "modele  511  loss =  1248046.8951343512\n",
      "modele  512  loss =  1248046.6480974462\n",
      "\n",
      "\n",
      "---------- 513 ----------\n",
      "modele  512  loss =  1248046.6480974462\n",
      "modele  513  loss =  1248046.5304510868\n",
      "\n",
      "\n",
      "---------- 514 ----------\n",
      "modele  513  loss =  1248046.5304510868\n",
      "modele  514  loss =  1248046.3192883285\n",
      "\n",
      "\n",
      "---------- 515 ----------\n",
      "modele  514  loss =  1248046.3192883285\n",
      "modele  515  loss =  1248045.8585322362\n",
      "\n",
      "\n",
      "---------- 516 ----------\n",
      "modele  515  loss =  1248045.8585322362\n",
      "modele  516  loss =  1248045.2249875073\n",
      "\n",
      "\n",
      "---------- 517 ----------\n",
      "modele  516  loss =  1248045.2249875073\n",
      "modele  517  loss =  1248044.9673815903\n",
      "\n",
      "\n",
      "---------- 518 ----------\n",
      "modele  517  loss =  1248044.9673815903\n",
      "modele  518  loss =  1248044.3337280173\n",
      "\n",
      "\n",
      "---------- 519 ----------\n",
      "modele  518  loss =  1248044.3337280173\n",
      "modele  519  loss =  1248044.148391377\n",
      "\n",
      "\n",
      "---------- 520 ----------\n",
      "modele  519  loss =  1248044.148391377\n",
      "modele  520  loss =  1248044.109229326\n",
      "\n",
      "\n",
      "---------- 521 ----------\n",
      "modele  520  loss =  1248044.109229326\n",
      "modele  521  loss =  1248044.049746648\n",
      "\n",
      "\n",
      "---------- 522 ----------\n",
      "modele  521  loss =  1248044.049746648\n",
      "modele  522  loss =  1248043.8645277913\n",
      "\n",
      "\n",
      "---------- 523 ----------\n",
      "modele  522  loss =  1248043.8645277913\n",
      "modele  523  loss =  1248043.7770200057\n",
      "\n",
      "\n",
      "---------- 524 ----------\n",
      "modele  523  loss =  1248043.7770200057\n",
      "modele  524  loss =  1248043.690620837\n",
      "\n",
      "\n",
      "---------- 525 ----------\n",
      "modele  524  loss =  1248043.690620837\n",
      "modele  525  loss =  1248043.6031781107\n",
      "\n",
      "\n",
      "---------- 526 ----------\n",
      "modele  525  loss =  1248043.6031781107\n",
      "modele  526  loss =  1248043.5263826565\n",
      "\n",
      "\n",
      "---------- 527 ----------\n",
      "modele  526  loss =  1248043.5263826565\n",
      "modele  527  loss =  1248043.4259229791\n",
      "\n",
      "\n",
      "---------- 528 ----------\n",
      "modele  527  loss =  1248043.4259229791\n",
      "modele  528  loss =  1248043.3396335314\n",
      "\n",
      "\n",
      "---------- 529 ----------\n",
      "modele  528  loss =  1248043.3396335314\n",
      "modele  529  loss =  1248043.0124929494\n",
      "\n",
      "\n",
      "---------- 530 ----------\n",
      "modele  529  loss =  1248043.0124929494\n",
      "modele  530  loss =  1248042.8286996426\n",
      "\n",
      "\n",
      "---------- 531 ----------\n",
      "modele  530  loss =  1248042.8286996426\n",
      "modele  531  loss =  1248042.6862619445\n",
      "\n",
      "\n",
      "---------- 532 ----------\n",
      "modele  531  loss =  1248042.6862619445\n",
      "modele  532  loss =  1248042.5210767707\n",
      "\n",
      "\n",
      "---------- 533 ----------\n",
      "modele  532  loss =  1248042.5210767707\n",
      "modele  533  loss =  1248041.9856043237\n",
      "\n",
      "\n",
      "---------- 534 ----------\n",
      "modele  533  loss =  1248041.9856043237\n",
      "modele  534  loss =  1248041.942860366\n",
      "\n",
      "\n",
      "---------- 535 ----------\n",
      "modele  534  loss =  1248041.942860366\n",
      "modele  535  loss =  1248041.5003468962\n",
      "\n",
      "\n",
      "---------- 536 ----------\n",
      "modele  535  loss =  1248041.5003468962\n",
      "modele  536  loss =  1248041.391760589\n",
      "\n",
      "\n",
      "---------- 537 ----------\n",
      "modele  536  loss =  1248041.391760589\n",
      "modele  537  loss =  1248041.1901768313\n",
      "\n",
      "\n",
      "---------- 538 ----------\n",
      "modele  537  loss =  1248041.1901768313\n",
      "modele  538  loss =  1248040.7386221637\n",
      "\n",
      "\n",
      "---------- 539 ----------\n",
      "modele  538  loss =  1248040.7386221637\n",
      "modele  539  loss =  1248037.9839932555\n",
      "\n",
      "\n",
      "---------- 540 ----------\n",
      "modele  539  loss =  1248037.9839932555\n",
      "modele  540  loss =  1248031.8460951366\n",
      "\n",
      "\n",
      "---------- 541 ----------\n",
      "modele  540  loss =  1248031.8460951366\n",
      "modele  541  loss =  1248028.8463161895\n",
      "\n",
      "\n",
      "---------- 542 ----------\n",
      "modele  541  loss =  1248028.8463161895\n",
      "modele  542  loss =  1248027.3427966018\n",
      "\n",
      "\n",
      "---------- 543 ----------\n",
      "modele  542  loss =  1248027.3427966018\n",
      "modele  543  loss =  1248023.1214675691\n",
      "\n",
      "\n",
      "---------- 544 ----------\n",
      "modele  543  loss =  1248023.1214675691\n",
      "modele  544  loss =  1248017.974925296\n",
      "\n",
      "\n",
      "---------- 545 ----------\n",
      "modele  544  loss =  1248017.974925296\n",
      "modele  545  loss =  1248015.8218390036\n",
      "\n",
      "\n",
      "---------- 546 ----------\n",
      "modele  545  loss =  1248015.8218390036\n",
      "modele  546  loss =  1248013.5261119932\n",
      "\n",
      "\n",
      "---------- 547 ----------\n",
      "modele  546  loss =  1248013.5261119932\n",
      "modele  547  loss =  1248009.5652122837\n",
      "\n",
      "\n",
      "---------- 548 ----------\n",
      "modele  547  loss =  1248009.5652122837\n",
      "modele  548  loss =  1248006.4459437532\n",
      "\n",
      "\n",
      "---------- 549 ----------\n",
      "modele  548  loss =  1248006.4459437532\n",
      "modele  549  loss =  1248004.8610802507\n",
      "\n",
      "\n",
      "---------- 550 ----------\n",
      "modele  549  loss =  1248004.8610802507\n",
      "modele  550  loss =  1248001.8510698895\n",
      "\n",
      "\n",
      "---------- 551 ----------\n",
      "modele  550  loss =  1248001.8510698895\n",
      "modele  551  loss =  1247999.0676046528\n",
      "\n",
      "\n",
      "---------- 552 ----------\n",
      "modele  551  loss =  1247999.0676046528\n",
      "modele  552  loss =  1247998.970292873\n",
      "\n",
      "\n",
      "---------- 553 ----------\n",
      "modele  552  loss =  1247998.970292873\n",
      "modele  553  loss =  1247998.9429223265\n",
      "\n",
      "\n",
      "---------- 554 ----------\n",
      "modele  553  loss =  1247998.9429223265\n",
      "modele  554  loss =  1247998.6441401723\n",
      "\n",
      "\n",
      "---------- 555 ----------\n",
      "modele  554  loss =  1247998.6441401723\n",
      "modele  555  loss =  1247998.4990310892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------- 556 ----------\n",
      "modele  555  loss =  1247998.4990310892\n",
      "modele  556  loss =  1247991.37332535\n",
      "\n",
      "\n",
      "---------- 557 ----------\n",
      "modele  556  loss =  1247991.37332535\n",
      "modele  557  loss =  1247990.40392278\n",
      "\n",
      "\n",
      "---------- 558 ----------\n",
      "modele  557  loss =  1247990.40392278\n",
      "modele  558  loss =  1247907.7470176017\n",
      "\n",
      "\n",
      "---------- 559 ----------\n",
      "modele  558  loss =  1247907.7470176017\n",
      "modele  559  loss =  1247905.291461189\n",
      "\n",
      "\n",
      "---------- 560 ----------\n",
      "modele  559  loss =  1247905.291461189\n",
      "modele  560  loss =  1247904.3648278774\n",
      "\n",
      "\n",
      "---------- 561 ----------\n",
      "modele  560  loss =  1247904.3648278774\n",
      "modele  561  loss =  1247879.5971529435\n",
      "\n",
      "\n",
      "---------- 562 ----------\n",
      "modele  561  loss =  1247879.5971529435\n",
      "modele  562  loss =  1247875.9631682534\n",
      "\n",
      "\n",
      "---------- 563 ----------\n",
      "modele  562  loss =  1247875.9631682534\n",
      "modele  563  loss =  1247875.0140142934\n",
      "\n",
      "\n",
      "---------- 564 ----------\n",
      "modele  563  loss =  1247875.0140142934\n",
      "modele  564  loss =  1247873.1460348885\n",
      "\n",
      "\n",
      "---------- 565 ----------\n",
      "modele  564  loss =  1247873.1460348885\n",
      "modele  565  loss =  1247872.9928811588\n",
      "\n",
      "\n",
      "---------- 566 ----------\n",
      "modele  565  loss =  1247872.9928811588\n",
      "modele  566  loss =  1247872.391184934\n",
      "\n",
      "\n",
      "---------- 567 ----------\n",
      "modele  566  loss =  1247872.391184934\n",
      "modele  567  loss =  1247869.7846465784\n",
      "\n",
      "\n",
      "---------- 568 ----------\n",
      "modele  567  loss =  1247869.7846465784\n",
      "modele  568  loss =  1247869.5686073888\n",
      "\n",
      "\n",
      "---------- 569 ----------\n",
      "modele  568  loss =  1247869.5686073888\n",
      "modele  569  loss =  1247869.0311920752\n",
      "\n",
      "\n",
      "---------- 570 ----------\n",
      "modele  569  loss =  1247869.0311920752\n",
      "modele  570  loss =  1247868.6728266685\n",
      "\n",
      "\n",
      "---------- 571 ----------\n",
      "modele  570  loss =  1247868.6728266685\n",
      "modele  571  loss =  1247865.513810804\n",
      "\n",
      "\n",
      "---------- 572 ----------\n",
      "modele  571  loss =  1247865.513810804\n",
      "modele  572  loss =  1247864.065021154\n",
      "\n",
      "\n",
      "---------- 573 ----------\n",
      "modele  572  loss =  1247864.065021154\n",
      "modele  573  loss =  1247863.5721752765\n",
      "\n",
      "\n",
      "---------- 574 ----------\n",
      "modele  573  loss =  1247863.5721752765\n",
      "modele  574  loss =  1247862.9222259193\n",
      "\n",
      "\n",
      "---------- 575 ----------\n",
      "modele  574  loss =  1247862.9222259193\n",
      "modele  575  loss =  1247862.7697253486\n",
      "\n",
      "\n",
      "---------- 576 ----------\n",
      "modele  575  loss =  1247862.7697253486\n",
      "modele  576  loss =  1247862.4742747182\n",
      "\n",
      "\n",
      "---------- 577 ----------\n",
      "modele  576  loss =  1247862.4742747182\n",
      "modele  577  loss =  1247862.3431249063\n",
      "\n",
      "\n",
      "---------- 578 ----------\n",
      "modele  577  loss =  1247862.3431249063\n",
      "modele  578  loss =  1247862.298822666\n",
      "\n",
      "\n",
      "---------- 579 ----------\n",
      "modele  578  loss =  1247862.298822666\n",
      "modele  579  loss =  1247861.6684487544\n",
      "\n",
      "\n",
      "---------- 580 ----------\n",
      "modele  579  loss =  1247861.6684487544\n",
      "modele  580  loss =  1247861.612501834\n",
      "\n",
      "\n",
      "---------- 581 ----------\n",
      "modele  580  loss =  1247861.612501834\n",
      "modele  581  loss =  1247861.46598947\n",
      "\n",
      "\n",
      "---------- 582 ----------\n",
      "modele  581  loss =  1247861.46598947\n",
      "modele  582  loss =  1247861.3473685558\n",
      "\n",
      "\n",
      "---------- 583 ----------\n",
      "modele  582  loss =  1247861.3473685558\n",
      "modele  583  loss =  1247861.1343058718\n",
      "\n",
      "\n",
      "---------- 584 ----------\n",
      "modele  583  loss =  1247861.1343058718\n",
      "modele  584  loss =  1247861.0144816528\n",
      "\n",
      "\n",
      "---------- 585 ----------\n",
      "modele  584  loss =  1247861.0144816528\n",
      "modele  585  loss =  1247860.9128167653\n",
      "\n",
      "\n",
      "---------- 586 ----------\n",
      "modele  585  loss =  1247860.9128167653\n",
      "modele  586  loss =  1247860.879814425\n",
      "\n",
      "\n",
      "---------- 587 ----------\n",
      "modele  586  loss =  1247860.879814425\n",
      "modele  587  loss =  1247860.8630572073\n",
      "\n",
      "\n",
      "---------- 588 ----------\n",
      "modele  587  loss =  1247860.8630572073\n",
      "modele  588  loss =  1247860.8204517395\n",
      "\n",
      "\n",
      "---------- 589 ----------\n",
      "modele  588  loss =  1247860.8204517395\n",
      "modele  589  loss =  1247860.7249732846\n",
      "\n",
      "\n",
      "---------- 590 ----------\n",
      "modele  589  loss =  1247860.7249732846\n",
      "modele  590  loss =  1247860.671241821\n",
      "\n",
      "\n",
      "---------- 591 ----------\n",
      "modele  590  loss =  1247860.671241821\n",
      "modele  591  loss =  1247860.6519478192\n",
      "\n",
      "\n",
      "---------- 592 ----------\n",
      "modele  591  loss =  1247860.6519478192\n",
      "modele  592  loss =  1247860.5460308788\n",
      "\n",
      "\n",
      "---------- 593 ----------\n",
      "modele  592  loss =  1247860.5460308788\n",
      "modele  593  loss =  1247860.4030029036\n",
      "\n",
      "\n",
      "---------- 594 ----------\n",
      "modele  593  loss =  1247860.4030029036\n",
      "modele  594  loss =  1247860.3562893174\n",
      "\n",
      "\n",
      "---------- 595 ----------\n",
      "modele  594  loss =  1247860.3562893174\n",
      "modele  595  loss =  1247860.2856880343\n",
      "\n",
      "\n",
      "---------- 596 ----------\n",
      "modele  595  loss =  1247860.2856880343\n",
      "modele  596  loss =  1247860.2518493417\n",
      "\n",
      "\n",
      "---------- 597 ----------\n",
      "modele  596  loss =  1247860.2518493417\n",
      "modele  597  loss =  1247860.2095547118\n",
      "\n",
      "\n",
      "---------- 598 ----------\n",
      "modele  597  loss =  1247860.2095547118\n",
      "modele  598  loss =  1247860.1840713832\n",
      "\n",
      "\n",
      "---------- 599 ----------\n",
      "modele  598  loss =  1247860.1840713832\n",
      "modele  599  loss =  1247860.1601467994\n",
      "\n",
      "\n",
      "---------- 600 ----------\n",
      "modele  599  loss =  1247860.1601467994\n",
      "modele  600  loss =  1247860.1425953014\n",
      "\n",
      "\n",
      "---------- 601 ----------\n",
      "modele  600  loss =  1247860.1425953014\n",
      "modele  601  loss =  1247860.0951460935\n",
      "\n",
      "\n",
      "---------- 602 ----------\n",
      "modele  601  loss =  1247860.0951460935\n",
      "modele  602  loss =  1247860.094267734\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "20bbd329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: {'left': {1.7218375836660709: -17.267302102157373,\n",
       "   8.353606886424044: -2.3939840573292104,\n",
       "   2.2158226091468216: 0.3884633657159433,\n",
       "   -8.030252824232566: -25.685528901384497,\n",
       "   0.026751779032707097: 0.9603285708995613,\n",
       "   0.9512629831062073: 0.29341253142065155,\n",
       "   -4.928821439869109: -1.5934835344929774,\n",
       "   0.21656279561412092: 0.33280824609839993,\n",
       "   0.5727972041648501: 0.13662266086006516,\n",
       "   -4.868208585761645: -0.8703428772951746,\n",
       "   0.2463179933653439: 0.2045973553946552,\n",
       "   -4.8846252453952275: -0.5017993358641284,\n",
       "   0.44132564654186124: 0.15941741880309873,\n",
       "   -4.862342997412269: -0.4096586597204403,\n",
       "   0.386228917923822: 0.12153331533945344,\n",
       "   -4.856175434179692: -0.2820794241645673,\n",
       "   0.18966493231023895: 0.09649095348581173,\n",
       "   -4.856172723766156: -0.2411685145730843,\n",
       "   0.4445929005844285: 0.07625332115310583,\n",
       "   0.45748698421944783: 0.07649889064922663,\n",
       "   -4.856178037400608: -0.19584551284871687,\n",
       "   2.092526131690187: 0.061708598943405454,\n",
       "   2.221344428070724: 0.026033040602779986,\n",
       "   -4.847912108984784: -0.20465825465784446,\n",
       "   0.4517927240503853: 0.08028082578392248,\n",
       "   -4.847909186875644: -0.25979654479070635,\n",
       "   2.100580342097941: 0.06306436680750646,\n",
       "   2.250322005625994: 0.04125110711532474,\n",
       "   2.2494174555807978: 0.015549081298950801,\n",
       "   -4.784285535317131: -0.2791891522396413,\n",
       "   0.45748510721846775: 0.09771986495649757,\n",
       "   -2.2333840958700413: -0.08402127616742376,\n",
       "   2.225637191689141: 0.03476441367897843,\n",
       "   2.248119190450364: 0.024420847443113656,\n",
       "   -4.784285417105553: -0.22188820156038444,\n",
       "   2.2256383764316303: 0.025596913550640556,\n",
       "   2.2481189155568226: 0.03745036702760991,\n",
       "   -6.99856061701075: 0.3606218932504667,\n",
       "   -4.801597757156485: -0.19705221891222907,\n",
       "   0.17825102259134545: 0.060396739133392864,\n",
       "   -2.233380957263322: -0.02426934767274547,\n",
       "   2.3047670307449373: 0.0131166596876566,\n",
       "   2.3426282286451534: 0.027731701137208752,\n",
       "   -2.2296426266098273: -0.08403622535042644,\n",
       "   0.2440271559996007: 0.020414293688629414},\n",
       "  'right': {-6.681697704462227: 8.069048126107708,\n",
       "   1.343220392261862: 12.138241221539095,\n",
       "   9.219738412281535: 109.38089677669838,\n",
       "   -4.702162003178103: -0.3321897968164112,\n",
       "   2.2515524886766345: 0.40658427287746135,\n",
       "   6.424232214982311: -0.7980300839229858,\n",
       "   2.3911473928882905: 0.2697674611536789,\n",
       "   6.424232751010421: -0.5053825380362329,\n",
       "   2.3857500411369594: 0.32164091942156936,\n",
       "   6.424231240074732: -0.8337798470514526,\n",
       "   2.3857518773766486: 0.30406792511297914,\n",
       "   6.4242345479865: -0.5913295499447573,\n",
       "   2.3079407420173763: 0.18635123877322787,\n",
       "   6.42423148484516: -0.339040641556112,\n",
       "   2.4656912046063026: 0.10896205370205998,\n",
       "   6.242274216516749: -0.3667129653277896,\n",
       "   2.4743277608611876: 0.11132628747193035,\n",
       "   2.4743232492003133: 0.03831600452158401,\n",
       "   2.4656907077885113: 0.012618389556824294,\n",
       "   6.242275438758525: -0.08603353495491715,\n",
       "   2.4743317583632574: 0.02221532598749739,\n",
       "   -2.1252311890528692: -0.010853489128256944,\n",
       "   -2.1252331255289163: -0.019223404464277775,\n",
       "   2.4890718058121513: 0.013820716975622902,\n",
       "   2.536331447994507: 0.013801779905869239,\n",
       "   2.4656908901078296: 0.020500046763309715,\n",
       "   -2.1252355975941404: -0.008963674685595679,\n",
       "   2.4743290938240152: 0.029766264873136338,\n",
       "   2.4618571686170436: 0.015433339267826012,\n",
       "   2.4743319019766346: 0.04299945082710295,\n",
       "   2.474325023893462: 0.02450767455087102,\n",
       "   6.242275660241768: -0.27254655362309704,\n",
       "   2.4743287757255357: 0.12140084808284655,\n",
       "   2.474328915836537: 0.009904908948337619,\n",
       "   6.351587437390119: -0.5808194286949487,\n",
       "   2.4743290822527277: 0.09946223271102013,\n",
       "   2.5203797113355995: 0.004487073595139111,\n",
       "   2.520380523755909: 0.007988788691560835,\n",
       "   -2.1313661595870523: -0.03178758644266598,\n",
       "   2.491856500963212: 0.03352365036097912,\n",
       "   2.4760906318946487: 0.008134423225406012,\n",
       "   2.5203812053178303: 0.029605407730242284,\n",
       "   -2.1997751545505393: -0.021736619718622882,\n",
       "   2.476086227124594: 0.014005800167956255,\n",
       "   2.4760933520753885: 0.016374538759213974,\n",
       "   2.4656952590757: 0.02173471805184584,\n",
       "   -2.1252327214432505: -0.006658404523452591,\n",
       "   2.391145332411282: 0.010264781165164523,\n",
       "   -2.1973157285451195: -0.0054313967114626325,\n",
       "   2.4656894513767256: 0.0045609297908429935,\n",
       "   2.4743288475814964: 0.10038820657227128,\n",
       "   2.4656914461322903: 0.008050991264823218}},\n",
       " 0: {'right': {-1.1900824363086082: -0.6016662930584371,\n",
       "   -7.573625556124357: 0.131869887558779,\n",
       "   5.0862161987641725: -2.785901617736105,\n",
       "   -1.5618068377786627: -0.14292922825865143,\n",
       "   -1.4781327522874919: -0.09535161411426052,\n",
       "   -1.671904392704106: -0.06512643780645336,\n",
       "   -1.707717397541306: -0.053169708436252995,\n",
       "   -1.6721743175445458: -0.044774107406588604,\n",
       "   -1.7077187838945338: -0.030218545129914905,\n",
       "   1.0508353955368392: 0.03353108081306699,\n",
       "   -1.7077155820573051: -0.015323368093993364,\n",
       "   8.728027146560924: -6.714275513389084,\n",
       "   5.972462037004953: 0.6269418604033167,\n",
       "   -1.6719096381097225: -0.020954948262926737,\n",
       "   8.618687738216531: -2.278002172388042,\n",
       "   5.972463135107366: 0.6435712898822539,\n",
       "   -1.6233084109161628: -0.011523892712932358,\n",
       "   -1.6228349880418254: -0.012607933634514012,\n",
       "   1.7612672713914614: 0.019353183312081468,\n",
       "   -1.7399558952765433: -0.015609012298184295,\n",
       "   -1.756153267318951: -0.01452797103024809,\n",
       "   1.7885130164578689: 0.023645184773858862,\n",
       "   -1.7561544494038392: -0.015127241080164113,\n",
       "   6.123115310680059: 0.0777873972956116,\n",
       "   -1.7561520140279518: -0.007307212090846298,\n",
       "   1.900971108233064: 0.013687053524806921,\n",
       "   -1.7561551991346542: -0.0036182587770045766,\n",
       "   1.8613790624508533: 0.004925835868023331,\n",
       "   -1.7168413978473902: -0.0035383603883597983,\n",
       "   1.7885484988317932: 0.03447995135872145,\n",
       "   -1.7525210329207974: -0.011452585977024369,\n",
       "   1.788548803873649: 0.058397263436154895,\n",
       "   -1.7168424786817509: -0.03582111749294874,\n",
       "   -1.716844424184381: -0.006483274117065776,\n",
       "   1.7717238805338569: 0.03020806204074518,\n",
       "   1.0494261839574692: 0.008076034113410242,\n",
       "   -1.7678743258042027: -0.005655359552241617,\n",
       "   1.0470214358205179: 0.023284550827992183,\n",
       "   -1.7168444799924583: -0.006742867313863018,\n",
       "   -1.7519987821108456: -0.005243953776578039,\n",
       "   -1.7399523255674938: -0.0038946959489165023},\n",
       "  'left': {-1.313094999052274: -0.6573665978877409,\n",
       "   -1.7399562993452578: -0.3138770030311601,\n",
       "   -8.375487340530775: 24.95404362489815,\n",
       "   -3.3823236731259176: -0.8332786994178183,\n",
       "   6.170490131379045: 0.08905469440902206,\n",
       "   -3.4620292848307566: -0.6384058737369055,\n",
       "   6.13516419615301: 0.08934773079975628,\n",
       "   -3.4506005830473456: -0.5686392254614842,\n",
       "   6.108438737382512: 0.0799924794908983,\n",
       "   -5.636172291369974: -1.8865912223943282,\n",
       "   -1.8309884185069178: -0.14164888635735923,\n",
       "   5.315576316453865: 0.029604553157619017,\n",
       "   -8.290405568213274: 18.74965348991327,\n",
       "   -1.9711786566208211: -0.21361646429805914,\n",
       "   -6.032159628719784: -3.7185735244480598,\n",
       "   4.876133257611929: 0.0552558774135916,\n",
       "   -2.7186502176900547: -0.05413117872318608,\n",
       "   4.891867410374764: 0.014055023638492987,\n",
       "   -1.8404176613200782: -0.06149263272040056,\n",
       "   -1.8399751500618822: -0.060903140426626694,\n",
       "   -1.8361863973112273: -0.06197530203118631,\n",
       "   -1.834426169347998: -0.07196129259672666,\n",
       "   1.7885497244695714: 0.025308171600519774,\n",
       "   -1.8344262460657053: -0.04544066915360372,\n",
       "   1.7885482894948108: 0.016262570454728287,\n",
       "   -4.66962844769164: 0.13913676319241552,\n",
       "   -1.8361883833932715: -0.07235184798983234,\n",
       "   1.7885472959018238: 0.017288235969098367,\n",
       "   -1.8361880685427532: -0.04998495282671739,\n",
       "   1.7885161330393164: 0.01896250936652235,\n",
       "   -1.8344294067494562: -0.03942035207318425,\n",
       "   1.788514696408297: 0.03751962308654322,\n",
       "   -1.832562598313241: -0.09971619397581227,\n",
       "   1.7875390967818712: 0.015106599073209302,\n",
       "   1.7612690160211122: 0.012863342572421362,\n",
       "   -1.8344259631291833: -0.02975600053810215,\n",
       "   -4.669629026479324: 0.19014484597892228,\n",
       "   -1.8344264817790144: -0.05092716562062474,\n",
       "   1.7855312274649013: 0.07967235738177136,\n",
       "   -1.8344258423192923: -0.07069116608365415,\n",
       "   -4.669629346841886: 0.1767809597958037,\n",
       "   -1.8325623412633518: -0.12280656090456438,\n",
       "   1.7578757908612326: 0.018799321038997015,\n",
       "   -1.83442661970087: -0.04139599465277643,\n",
       "   -1.8404175808134136: -0.055355298321121765,\n",
       "   1.7564582318486954: 0.020987671882021358,\n",
       "   1.7179580891761725: 0.009993921695592762,\n",
       "   -4.669629970938051: 0.1069092677466307,\n",
       "   -1.8344284618515878: -0.06089485106467551,\n",
       "   -4.658097139796195: 0.12320984549497242,\n",
       "   -1.8309860044586548: -0.03709741255327188,\n",
       "   -4.651396967627594: 0.07569836209594281,\n",
       "   -1.8344254054209632: -0.014596899126153456,\n",
       "   1.746445396199496: 0.007743410781751661,\n",
       "   -1.8404178036297716: -0.019164936082970048,\n",
       "   1.7148588908353435: 0.005056661069925094,\n",
       "   1.717960734412886: 0.0036543959325360973,\n",
       "   -1.8361834332872384: -0.04151658680194786,\n",
       "   1.756454640432117: 0.024153123734667707,\n",
       "   -1.8361859726764103: -0.009711394655862508,\n",
       "   -1.8404186678747474: -0.008986566287647655,\n",
       "   -1.8404180974101987: -0.18230889015784923,\n",
       "   1.7120019656333565: 0.007001222106461421,\n",
       "   -4.651398248597349: 0.04289560576080191,\n",
       "   -1.8344282744642753: -0.024781552066245455,\n",
       "   1.7148612074920966: 0.011524935015088106,\n",
       "   1.0443599033520279: 0.08143177814084444,\n",
       "   -4.6696324518395365: 0.10052000570602161,\n",
       "   -1.836185357258403: -0.05459122463034985,\n",
       "   1.7179599638705043: 0.04108594074789049,\n",
       "   -4.651397169848744: 0.05125991193106304,\n",
       "   -1.8344265171654657: -0.026467210997322566,\n",
       "   1.7179589832883022: 0.0036172563132063676,\n",
       "   -1.8404205128915676: -0.016898470163322663,\n",
       "   1.0460627375188425: 0.01018350149289432,\n",
       "   -1.8404384309908042: -0.020050209227869535,\n",
       "   1.714858489487274: 0.006861424426294607,\n",
       "   -1.8344242302553688: -0.051714722593195095,\n",
       "   1.7119986992622382: 0.016375165463436103,\n",
       "   1.711998235313135: 0.016608140073359135,\n",
       "   -4.658097581468245: 0.02904711821823,\n",
       "   -1.8344248319479515: -0.01724737576708171,\n",
       "   -1.834427196379747: -0.01631390165507179,\n",
       "   -1.8309913655607126: -0.027188066478294907,\n",
       "   1.7179578320320221: 0.005689180258216031,\n",
       "   1.7179602824618803: 0.008633479790290952,\n",
       "   -4.651398407020586: 0.1434776677768515,\n",
       "   -1.8309875129396336: -0.060281869357221754,\n",
       "   1.7179579933050588: 0.022245738862710966,\n",
       "   -1.834424906931282: -0.02023425780150307,\n",
       "   1.7148611811979593: 0.012144017932875352,\n",
       "   -1.8344255186967606: -0.06734844812482521,\n",
       "   1.708703854638107: 0.07046162310221925,\n",
       "   -1.8309918043756566: -0.09319970088790888,\n",
       "   -4.551303514626432: 0.0887355197539093,\n",
       "   -1.834424947447759: -0.11913068614864217,\n",
       "   1.0322809040345111: 0.021068398590811707,\n",
       "   1.7046950574363926: 0.02738324826289836,\n",
       "   1.001116680501102: 0.0010198206484838262,\n",
       "   -1.8291004775026196: -0.011274636418999406,\n",
       "   -1.7743596860660895: -0.010231087802300441,\n",
       "   -4.601110821765286: 0.05677435930320136,\n",
       "   -1.8290956797007512: -0.0346855292390799,\n",
       "   1.0322786508298907: 0.004270287138536995,\n",
       "   1.0392824002426122: 0.004914595466459212,\n",
       "   -1.8361899798112433: -0.01083453523542539,\n",
       "   1.7087077967595892: 0.0030801033903609973,\n",
       "   1.0392801117010761: 0.003299578396761243,\n",
       "   -1.8290979448604363: -0.012117915879560898,\n",
       "   1.031586480735915: 0.00795622379404294,\n",
       "   -4.551301953494726: 0.028131237557517914,\n",
       "   1.0315853989082264: 0.005600570265369334,\n",
       "   -4.623122962165055: 0.017716205261186023,\n",
       "   -1.8290968568614812: -0.00610496991847751,\n",
       "   -7.817511896066792: 9.634199953806696,\n",
       "   -1.8404205416387818: -0.05876686363611552,\n",
       "   -6.173936856617993: -2.1046403551016626,\n",
       "   1.032282046508952: 0.04785017953497337,\n",
       "   -4.651395349820012: 0.22114562780979627,\n",
       "   -1.8309871968171292: -0.004389787059652974,\n",
       "   -1.7743573819313663: -0.005579415484024134,\n",
       "   -1.8290993040019616: -0.0013626862948441826}},\n",
       " 3: {'left': {-2.188198299579499: 0.2242746020154447,\n",
       "   -8.87236069908404: -2.7031153750016284,\n",
       "   -3.725562507550994: 0.13573555342792074,\n",
       "   -8.301646842561263: -0.37103055577110333,\n",
       "   -5.8701979534107345: -0.027972856475814465,\n",
       "   6.677579724942263: 0.015562232113820894,\n",
       "   0.4622631675343639: -0.03067945059642808,\n",
       "   -4.191610186800128: 0.030897422605961775,\n",
       "   -3.7305356136542818: 0.014268563931657428,\n",
       "   -9.328274092193292: -0.3550618717360291,\n",
       "   0.458551818534271: -0.017193147928571247,\n",
       "   0.4622582117095351: -0.011500312731532935,\n",
       "   -4.191607676838204: 0.006983554928705134,\n",
       "   -9.328270697753124: -0.538145591944097,\n",
       "   -4.191610076289488: 0.02427377628477228,\n",
       "   0.4585507728861521: -0.015950815420135826,\n",
       "   -4.191607079265306: 0.04816437490737368,\n",
       "   -5.870194988681917: -0.054690825626707455,\n",
       "   -8.531359407252669: 0.09253564179034095,\n",
       "   -4.19161003780785: 0.01164266162418567,\n",
       "   -6.863460232342948: -0.057327348850864775,\n",
       "   -3.7460912422285086: 0.05473049699028131,\n",
       "   -6.854051310208271: -0.08441183124215816,\n",
       "   0.45854964758124067: -0.008023642997666532,\n",
       "   -2.5329262295417845: -0.012771687512335742,\n",
       "   -8.129070369516803: 0.04486045119484507,\n",
       "   0.4585507665849506: -0.012566841491126971,\n",
       "   0.46226048606509046: -0.005751180571991653,\n",
       "   -3.7460894690001214: 0.023735451512920974,\n",
       "   0.46226093404376833: -0.01204946565976122,\n",
       "   -4.195099816463053: 0.021503809584156493,\n",
       "   -5.864786204967266: -0.05520250049038156,\n",
       "   0.48628891645051514: -0.014421553749841488,\n",
       "   -7.561541713542852: 0.0949468639106471,\n",
       "   0.5105806039716233: -0.006268227881344047,\n",
       "   -7.561541679389207: 0.0332679527870409,\n",
       "   0.509454138999151: -0.00857219825626121,\n",
       "   -4.1950976105971005: 0.009525709735932262,\n",
       "   0.5084440864381572: -0.008950145231106694,\n",
       "   -8.129067564016292: 0.02762123811721724,\n",
       "   -7.80550777493289: 0.05869701851294707,\n",
       "   0.46226172755944817: -0.010814426388818318,\n",
       "   0.5094499592547888: -0.004798775016476881,\n",
       "   -8.129066790801115: 0.18321825822153925,\n",
       "   0.5230344914634816: -0.006631180285509355,\n",
       "   0.5230319643925443: -0.015310639814849158,\n",
       "   -1.6563289956825875: 0.0073824366899755125,\n",
       "   -3.7344366492001777: 0.012683984816024431,\n",
       "   -1.655404575768626: 0.006938462696970338,\n",
       "   -4.18707653681438: 0.04478922802444279,\n",
       "   -5.867823719173883: -0.056011680160633806,\n",
       "   -3.724994073449636: 0.035960849292830735,\n",
       "   0.5230338522368516: -0.030485872489783933,\n",
       "   -1.8157411444082396: 0.0032009691864289484,\n",
       "   -5.867931855676016: -0.014454960404667278,\n",
       "   -3.7249972051364937: 0.005953161550779641,\n",
       "   -1.6548348347070436: 0.002100858399818252},\n",
       "  'right': {9.339538494211656: 10.969356286507882,\n",
       "   1.4027554061481782: -0.09975324172028682,\n",
       "   0.9064283313264527: -0.022155103759896256,\n",
       "   0.8651683677346669: -0.03146373475686584,\n",
       "   6.713279452694096: 0.3115961293489427,\n",
       "   0.6664381057784844: -0.08521687124983994,\n",
       "   6.69706547945596: 0.36517026389170154,\n",
       "   0.6490372690803454: -0.02929247359169016,\n",
       "   0.612356030348789: -0.02446292537763482,\n",
       "   8.497014581796849: -1.3355474691266833,\n",
       "   6.6970697802650125: 0.30045302365877674,\n",
       "   0.6448440837022223: -0.009921989183265124,\n",
       "   0.533177955191133: -0.04044113798933702,\n",
       "   9.262639608577343: 4.777516615201999,\n",
       "   0.6438911147435061: -0.011818743920803605,\n",
       "   8.50905607670687: -1.8402831807393845,\n",
       "   6.721481087103331: 0.2900113944229157,\n",
       "   0.6451639270845032: -0.011264560830620742,\n",
       "   0.6451652301101674: -0.0025590557391611656,\n",
       "   -3.725560088354549: 0.0013438856393050513,\n",
       "   -3.724998657852095: 0.003782060723897446,\n",
       "   -3.724995664856399: 0.0032845121532788673,\n",
       "   0.6098721478236045: -0.0111029211961519,\n",
       "   6.72059765918703: 0.1971083711889975,\n",
       "   0.8651672322156075: -0.04365840606367558,\n",
       "   -1.6548343645676193: 0.012275224105799816,\n",
       "   0.609870272826668: -0.0067010122329480585,\n",
       "   6.720596768028805: 0.10571318209722644,\n",
       "   9.262641212995282: 1.6331912104298911,\n",
       "   8.497018335454168: -1.3948142374398051,\n",
       "   6.720597835089397: 0.2845526397338418,\n",
       "   9.23654092445945: 4.7060432247285116,\n",
       "   8.492440821242223: -3.1054434416186285,\n",
       "   7.3370911782354185: 0.5569996232286889,\n",
       "   7.338977092192363: 0.2443919370635915,\n",
       "   0.5329149810533662: -0.021474803997295896,\n",
       "   0.5329153721541863: -0.015251927031810993,\n",
       "   6.721483285663759: 0.015034957398121978,\n",
       "   0.6098709002732271: -0.001358589355928581}},\n",
       " 1: {'left': {7.314766210757614: 0.03984758793949644,\n",
       "   -2.3060377812991772: -0.14233545150333077,\n",
       "   -2.034078963497823: -0.11409340536913115,\n",
       "   7.444584206268041: 0.018985853961635525,\n",
       "   4.869844526312488: -0.02110714224692824,\n",
       "   -5.294762111651818: 0.08193016191732742,\n",
       "   -5.294759215819978: 0.06155152003529605,\n",
       "   -2.0858392690951497: -0.03158770147268688,\n",
       "   -5.294759316982359: 0.04133148226219144,\n",
       "   3.87324868506329: 0.006885283737090631,\n",
       "   -5.273743073543499: 0.04384727741935356,\n",
       "   -5.280273330648804: 0.027632945985579846,\n",
       "   -5.280269153486488: 0.09480295967595517,\n",
       "   -2.0856799071214303: -0.04055431614188953,\n",
       "   -2.085680479479139: -0.04030449042899529,\n",
       "   -8.139761135565005: -2.2130893385298247,\n",
       "   -0.38200898770610964: 0.09931482364860415,\n",
       "   -5.426187072695751: 0.09119245775256991,\n",
       "   -5.421130293943046: 0.07173974464442658,\n",
       "   -5.426189463393185: 0.027557931880818332,\n",
       "   -2.034077343008754: -0.019968634376709093,\n",
       "   -5.328885774537061: 0.049734678907269875,\n",
       "   -2.0421305149661526: -0.05846223751435246,\n",
       "   -8.044435830219948: -0.8629957968826641,\n",
       "   2.014021730361722: 0.02311007435894675,\n",
       "   -0.38088726547269974: 0.010570444735989289,\n",
       "   -5.4261895018555695: 0.096238041059462,\n",
       "   -2.030558900813762: -0.034929657976970015,\n",
       "   -5.42112932824734: 0.11012479354419001,\n",
       "   -2.0856790361550765: -0.022643886461369635,\n",
       "   -5.421131899860506: 0.04493443458422959,\n",
       "   -2.0865250105968904: -0.04919945811954059,\n",
       "   -2.085842072668467: -0.002442510715004122,\n",
       "   -5.2947592527899205: 0.04443351887438332,\n",
       "   -2.1318613997314686: -0.008497033655558074,\n",
       "   -5.2918815088372595: 0.03836314791505099,\n",
       "   -5.294757898200561: 0.03252154048422304,\n",
       "   -2.0421325497432607: -0.027755470877172513,\n",
       "   -5.294759644768669: 0.06811802631707763,\n",
       "   -2.1318595888157055: -0.025146210798856348,\n",
       "   -5.282947796751467: 0.09973386291571898,\n",
       "   -2.1374981965962716: -0.02759036917789267,\n",
       "   -2.1374964374484957: -0.017967328026462583,\n",
       "   -5.28705553773639: 0.06841701687639563,\n",
       "   -2.034076919731567: -0.01918842265566314,\n",
       "   -2.042128416856923: -0.021849411661476754,\n",
       "   -5.245964445415892: 0.02576699660303254,\n",
       "   -5.245936151112686: 0.09159610934820986,\n",
       "   -2.04213164153313: -0.01802057403844661,\n",
       "   -5.24593587331552: 0.0678878102347072,\n",
       "   -2.0421309484120447: -0.031294982835872606,\n",
       "   -2.0865230805568036: -0.02675265875903666,\n",
       "   -2.086525383153341: -0.026119772915462997,\n",
       "   -2.0856789009551133: -0.005649741998642271,\n",
       "   -5.245587755092721: 0.06080111261751032,\n",
       "   -7.877574739551277: -0.7891447807170657,\n",
       "   -0.38201274028782484: 0.03521404436617554,\n",
       "   -5.222394839067557: 0.017010359544637983,\n",
       "   -0.3780174082719661: 0.006720731351234303,\n",
       "   -5.245594171710184: 0.003301516270807956,\n",
       "   -2.0858387031715364: -0.0016662177076212496,\n",
       "   -5.245968360751441: 0.02381088664236193,\n",
       "   -2.0421326570293403: -0.007524162822180503},\n",
       "  'right': {8.150755210308697: 1.673544126991196,\n",
       "   2.369282142618687: -0.16074478549206975,\n",
       "   8.075050493886339: 0.8336959983144249,\n",
       "   2.6567901480948612: -0.11163325714303604,\n",
       "   2.553632777604048: -0.049609302048202276,\n",
       "   8.06674867573116: 0.5933518975743451,\n",
       "   5.024958548105684: -0.10923174124559891,\n",
       "   2.550336094104053: -0.07253032476234841,\n",
       "   8.06647253652032: 0.6756057930736961,\n",
       "   5.024961229805566: -0.13061912077887736,\n",
       "   2.5627060518407956: -0.03317773071403562,\n",
       "   7.844310702869679: 0.7044485262421659,\n",
       "   5.026212027275429: -0.20238106072109435,\n",
       "   -5.220678985563757: 0.009472008823077567,\n",
       "   7.841750145484236: 0.2773708300280803,\n",
       "   5.792921384018643: -0.04862388731214301,\n",
       "   7.469730393587037: 0.08267182483792747,\n",
       "   5.755290905839302: -0.04133749747442123,\n",
       "   -5.239086976029854: 0.004311266489890289,\n",
       "   -0.23411073619281741: 0.006081141638055872,\n",
       "   -5.239087158638043: 0.009246690072211398,\n",
       "   7.44870683468272: 0.12521050226014382,\n",
       "   5.046790950794005: -0.0569554782389112,\n",
       "   -2.0168253679304153: -0.001863400835384387,\n",
       "   8.464672754518968: -0.145867583406436,\n",
       "   -0.22364659435134812: 0.010160655400673133,\n",
       "   7.448703926420512: 0.0837891037441844,\n",
       "   -2.01195468790877: -0.004258174023110447,\n",
       "   3.8735923714870757: 0.011639684194569063,\n",
       "   2.026923763238332: 0.003904858549033362,\n",
       "   7.44870391033691: 0.050538294571795914,\n",
       "   3.8735392792532215: 0.0029501913067371804,\n",
       "   -5.245935692260471: 0.003887228267033355,\n",
       "   -2.0168257852368834: -0.005396002704777083,\n",
       "   -2.0119524212515736: -0.0046148656106592655,\n",
       "   3.8735932021630233: 0.009362273345831314,\n",
       "   -2.030562733711468: -0.005839005790294734,\n",
       "   3.8735427607272594: 0.007265530152299377,\n",
       "   2.0269213977583735: 0.01888019719691712}}}"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "bfb669bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -95.97725762, -264.72753731,  328.12143504, ...,  176.92456657,\n",
       "       -196.84779879,    5.20749632])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "78b9e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict[0]['left'][7]=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "07c02ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'left': {1.4643818499046604: -0.0045625758933129, 7: 3},\n",
       "  'right': {1.6198870422568603: -0.01903738486855065}}}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f490af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
